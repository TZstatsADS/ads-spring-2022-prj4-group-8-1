{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KveoZlUNJijZ"
   },
   "source": [
    "# Project 4\n",
    "\n",
    "Implementating, evaluating and comparing on  algorithms: Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment (DM and DM-sen) and Handling Conditional Discrimination (LM and LPS)\n",
    "\n",
    "Team members: \n",
    "\n",
    "- Sarah Kurihara\n",
    "\n",
    "- Varchasvi Vedula\n",
    "\n",
    "- Wenhui Fang\n",
    "\n",
    "- Krista Zhang\n",
    "\n",
    "- Sharon Meng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR5_702OVJAQ"
   },
   "source": [
    "# 1.Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_cZws18Qpyp"
   },
   "source": [
    "## 1.1 Import essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VV6K-AxuVIbA"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import time,sys\n",
    "import zipfile #When running in jupyter notebook, delete this code\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.colab import drive #When running in jupyter notebook, delete this code\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "import scipy.stats as ss\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m94DWsuCzcp"
   },
   "source": [
    "## 1.2 Load Database and Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WdN6QSFk9pAR"
   },
   "outputs": [],
   "source": [
    "#When running in jupyter notebook, delete this code chunk\n",
    "#!mkdir data\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJ2bTdt5T3EM",
    "outputId": "c7ce4179-01cd-471a-beb2-28139cdf0066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sex  age race  juv_fel_count  decile_score  juv_misd_count  juv_other_count  \\\n",
      "1   1   34    0              0             3               0                0   \n",
      "2   1   24    0              0             4               0                1   \n",
      "6   1   41    1              0             6               0                0   \n",
      "8   0   39    1              0             1               0                0   \n",
      "9   1   21    1              0             3               0                0   \n",
      "\n",
      "   priors_count  c_days_from_compas c_charge_degree  is_violent_recid  event  \\\n",
      "1             0            0.693147               0                 1      1   \n",
      "2             4            0.693147               0                 0      0   \n",
      "6            14            0.693147               0                 0      1   \n",
      "8             0            0.693147               1                 0      0   \n",
      "9             1            5.733341               0                 1      1   \n",
      "\n",
      "   two_year_recid       los   custody     lasts  \n",
      "1               1  5.488938  5.484797  5.017280  \n",
      "2               1  3.295837  0.000000  4.158883  \n",
      "6               1  5.023881  6.070738  3.583519  \n",
      "8               0  4.262680  4.290459  6.614726  \n",
      "9               1  3.178054  3.218876  6.061457  \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "#data = pd.read_csv('drive/MyDrive/5243Project4/compas-scores-two-years.csv')#When running in jupyter notebook, delete this code\n",
    "data = pd.read_csv('./data/compas-scores-two-years.csv') #When running in jupyter notebook, use this code\n",
    "#print(data.shape)\n",
    "# filter out groups other than African-American and Caucasian and set them as 0-1\n",
    "data = data[(data['race']=='African-American') | (data['race']=='Caucasian')]\n",
    "data['race'].loc[data['race']=='Caucasian'] = 1\n",
    "data['race'].loc[data['race']=='African-American'] = 0\n",
    "#print(data.shape)\n",
    "\n",
    "nan = (data.isnull().sum()/len(data))\n",
    "nan = nan[nan > 0.15].sort_values()\n",
    "nan_var = list(nan.index)\n",
    "data = data.drop(columns=nan_var)\n",
    "\n",
    "data['c_jail_in'] = pd.to_datetime(data['c_jail_in'])\n",
    "data['c_jail_out'] = pd.to_datetime(data['c_jail_out'])\n",
    "data['los'] = np.log((data['c_jail_out']-data['c_jail_in']).astype('timedelta64[h]')+1)#use log hours\n",
    "data['in_custody'] = pd.to_datetime(data['in_custody'])\n",
    "data['out_custody'] = pd.to_datetime(data['out_custody'])\n",
    "data['custody'] = np.log((data['out_custody']-data['in_custody']).astype('timedelta64[h]')+1)\n",
    "data['lasts'] = np.log(data['end']-data['start']+1)\n",
    "data['c_days_from_compas'] = np.log(data['c_days_from_compas']+1)\n",
    "\n",
    "#filter out useless variables including high correlation and string type\n",
    "useless_var = ['id','name','first','last','compas_screening_date','dob','age_cat','days_b_screening_arrest',\n",
    "               'c_jail_in','c_jail_out','c_case_number','c_charge_desc','is_recid',\n",
    "               'type_of_assessment','screening_date','v_type_of_assessment',\n",
    "               'v_screening_date','in_custody','out_custody','score_text','v_score_text',\n",
    "               'decile_score.1','v_decile_score','priors_count.1','start','end']\n",
    "data = data.drop(columns=useless_var)\n",
    "data = data[data['los']!=float('-inf')]\n",
    "data = data[data['custody']!=float('-inf')]\n",
    "data = data[data['lasts']!=float('-inf')]\n",
    "\n",
    "#one hot encoding on several features:sex,c_charge_degree\n",
    "data['sex'].loc[data['sex']=='Male']= 1\n",
    "data['sex'].loc[data['sex']=='Female']= 0\n",
    "data['c_charge_degree'].loc[data['c_charge_degree']=='M']= 1\n",
    "data['c_charge_degree'].loc[data['c_charge_degree']=='F']= 0\n",
    "#data.to_csv('./data/compas_preproc.csv',index=False,header=True)\n",
    "del nan_var, useless_var\n",
    "\n",
    "#data = data[['age','race','sex','decile_score','priors_count','los','c_charge_degree','two_year_recid']]\n",
    "data = data.dropna()#6150*23->5730*16\n",
    "#print(data.shape)\n",
    "print(data.head(5))\n",
    "\n",
    "X = data.drop(columns='two_year_recid')\n",
    "features = list(X.columns)\n",
    "\n",
    "X.index = range(data.shape[0])\n",
    "#As age, priors_count, los are continuous variables, we can scale them\n",
    "X_cont = X[['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts']]\n",
    "X_cate = X[['sex', 'race', 'c_charge_degree', 'is_violent_recid', 'event']]\n",
    "X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts'])\n",
    "#X_cont = X[['age', 'decile_score', 'priors_count', 'los']]\n",
    "#X_cate = X[['sex', 'race', 'c_charge_degree']]\n",
    "#X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'decile_score', 'priors_count', 'los'])\n",
    "\n",
    "X_df = pd.concat([X_cate,X_cont],axis=1)\n",
    "#X['decile_score'] = X['decile_score']/10\n",
    "y_df = data[\"two_year_recid\"]\n",
    "# convert class label 0 to -1 so as to add sign in distance\n",
    "#y[y==0] = -1\n",
    "features = list(X.columns)\n",
    "\n",
    "X = np.asarray(X_df).astype('float32')\n",
    "y = np.asarray(y_df).astype('float32')\n",
    "\n",
    "del X_cate,X_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnCfEMb_QtA1"
   },
   "source": [
    "## 1.3 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YTcWq5U9-K6",
    "outputId": "948c8697-46f9-4181-b209-b2d1882d3c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################ \n",
      "                       Split up Train-Validation-Test sets for A4  \n",
      " ################################################################################ \n",
      "\n",
      " X_train size:  (4130, 15) ,      y_train size:  (4130,) \n",
      " X_validation size:  (800, 15) ,  y_validation size:  (800,) \n",
      " X_test size:  (800, 15) ,        y_test size:  (800,)\n",
      " X_train_AA size:  (2472, 15) ,   X_train_C size:  (1658, 15) \n",
      " X_test_AA size:  (478, 15) ,     X_test_C size:  (322, 15) \n",
      " Ratio: 1.490952955367913 1.484472049689441\n",
      "\n",
      " ################################################################################\n",
      "\n",
      " ################################################################################ \n",
      "                       Split up Train-Validation-Test sets for A6 \n",
      " ################################################################################ \n",
      "\n",
      " X_train size:  (4130, 15) ,      y_train size:  (4130,) \n",
      " X_validation size:  (800, 15) ,  y_validation size:  (800,) \n",
      " X_test size:  (800, 15) ,        y_test size:  (800,)\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#Use 5:1:1 as the ratio of train:val:test\n",
    "# Fro A4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=800, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=800,random_state=3)\n",
    "\n",
    "train_size = len(y_train)\n",
    "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
    "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
    "test_size = len(y_test)\n",
    "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
    "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Validation-Test sets for A4 \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train size: \", X_train.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_validation size: \", X_val.shape, \",  y_validation size: \", y_val.shape, '\\n',\n",
    "      \"X_test size: \", X_test.shape, ',        y_test size: ',y_test.shape)\n",
    "print(\" X_train_AA size: \", X_train[train_idx_AA].shape, \",   X_train_C size: \", X_train[train_idx_C].shape, '\\n',\n",
    "      \"X_test_AA size: \", X_test[test_idx_AA].shape, ',     X_test_C size: ',X_test[test_idx_C].shape, '\\n',\n",
    "      \"Ratio:\", X_train[train_idx_AA].shape[0]/X_train[train_idx_C].shape[0],X_test[test_idx_AA].shape[0]/X_test[test_idx_C].shape[0])\n",
    "print('\\n',\"#\"*80)\n",
    "####################################################################################################\n",
    "# For A6\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=800, random_state=3)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df,y_train_df,test_size=800,random_state=3)\n",
    "\n",
    "X_train_df.race = abs(X_train_df.race-1)\n",
    "X_test_df.race = abs(X_test_df.race-1)\n",
    "X_val_df.race = abs(X_val_df.race-1)\n",
    "X_train_df.c_charge_degree = abs(X_train_df.c_charge_degree-1)\n",
    "X_test_df.c_charge_degree = abs(X_test_df.c_charge_degree-1)\n",
    "X_val_df.c_charge_degree = abs(X_val_df.c_charge_degree-1)\n",
    "train = pd.concat([X_train_df.reset_index(drop=True), y_train_df.reset_index(drop=True)], axis = 1)\n",
    "test = pd.concat([X_test_df.reset_index(drop=True), y_test_df.reset_index(drop=True)], axis = 1)\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Validation-Test sets for A6\",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train size: \", X_train_df.shape, \",      y_train size: \", y_train_df.shape, '\\n',\n",
    "      \"X_validation size: \", X_val_df.shape, \",  y_validation size: \", y_val_df.shape, '\\n',\n",
    "      \"X_test size: \", X_test_df.shape, ',        y_test size: ',y_test_df.shape)\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUTl8rVGT7gf"
   },
   "source": [
    "# 2. Handling Conditional Discrimination (LM and LPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xYtVXe2TWDM4"
   },
   "outputs": [],
   "source": [
    "# X_ALL = all predictors, S = sensitive attributes, E = explanatory attribute, Y = response\n",
    "S = \"race\"\n",
    "E = \"c_charge_degree\"\n",
    "Y = \"two_year_recid\"\n",
    "\n",
    "X_ALL = ['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', \n",
    "         'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts',\n",
    "         'sex', 'race', 'c_charge_degree', 'is_violent_recid', 'event']\n",
    "\n",
    "X_train_6 = train[X_ALL]\n",
    "y_train_6 = train[Y]\n",
    "\n",
    "X_test_6 = test[X_ALL]\n",
    "y_test_6 = test[Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5LY4JOmWXny"
   },
   "source": [
    "## 2.1 Baseline Model\n",
    "\n",
    "We use a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MEARY_yWesL",
    "outputId": "d17edcc2-0063-4fcd-f744-182401e1dba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_6, y_train_6)\n",
    "clf.score(X_test_6, y_test_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i99Qthk8Wit_"
   },
   "source": [
    "## 2.2 A6 Algorithms\n",
    "\n",
    "A6 uses the following setup:\n",
    "\n",
    "- There is a sensitive attribute S. We want to avoid discrimination between different values of S. In our case, S is race, which takes values in {African American (1), Caucasian (0)}\n",
    "- There is an explanatory variable E, which is correlated with S. Hence, simply removing S won't fix bias. In our case, we assume it is c_charge_degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjUlnFe0WsSj"
   },
   "source": [
    "### 2.2.1 Create functions used in pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Te0QoBzGWuEn"
   },
   "outputs": [],
   "source": [
    "# Creates a list of partitions: 1 for each unique value of e\n",
    "\n",
    "# X is the full dataset (in our case, train)\n",
    "# e is the \n",
    "def PARTITION(X):\n",
    "    partitions = list()\n",
    "    \n",
    "    for e_i in np.unique(X[E]):\n",
    "        partitions.append(X[X[E]==e_i])\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y8qZDiYtWviN"
   },
   "outputs": [],
   "source": [
    "# Delta function returns the number of observations (i.e. people) who are incorrectly classified \n",
    "# based on theoretical probabilities of reciding, calculated as the average rate of reciding\n",
    "# for each explanatory varaible (in our case, type of crime comittied, c_charge_degree)\n",
    "\n",
    "def DELTA(X, X_ei, s_i):\n",
    "    \n",
    "    # Gi is the number of observations for each race\n",
    "    # Don't we need to pass S as a parameter for this function?\n",
    "    Gi = sum(X_ei[S] == s_i)\n",
    "    \n",
    "    # X_ei_si is the dataset that contains the observations for each race\n",
    "    X_ei_si = X_ei[X_ei[S] == s_i]\n",
    "    \n",
    "    # P_denom is the number of people in group \n",
    "    # P_num is number of observations who recid\n",
    "    P_denom = X_ei_si.shape[0]\n",
    "    P_num = sum(X_ei_si[Y] == 1)\n",
    "    \n",
    "    # P is the probability of reciding for one race\n",
    "    # It is calculated by taking number of people who recid in each group \n",
    "    # dividied by total number of people in that group\n",
    "    P = P_num/P_denom\n",
    "    \n",
    "    # All other observations (for the other group)\n",
    "    X_ei_not_si = X_ei[X_ei[S] != s_i]\n",
    "    \n",
    "    # The probability of reciding for the other group (same calculation as above)\n",
    "    Ps_2 = sum(X_ei_not_si[Y] == 1)/X_ei_not_si.shape[0]\n",
    "    \n",
    "    # Ps is P*, which is the theoretical true probability of reciding\n",
    "    # Calculated by the average \n",
    "    Ps = (P+Ps_2)/2\n",
    "    \n",
    "    # Calcualte the number of incorrectly classified people\n",
    "    d = int(round(Gi * abs(P - Ps)))\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpeDin9JWzvq"
   },
   "source": [
    "### 2.2.2 Local Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zppRq5RWW2-v",
    "outputId": "f6aa0a9e-dacf-4659-bca5-a7a393bf5cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELTA(African American) =  46 African Americans changed from 1 to 0\n",
      "DELTA(Caucasian) =  39 Caucasians changed from 0 to 1\n",
      "DELTA(African American) =  90 African Americans changed from 1 to 0\n",
      "DELTA(Caucasian) =  54 Caucasians changed from 0 to 1\n"
     ]
    }
   ],
   "source": [
    "relabeled_X_ei = list()\n",
    "\n",
    "for X_ei in PARTITION(train):\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    \n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    delta_afam = DELTA(train, X_ei, 1)\n",
    "    afam_predicted_1_index = [afam_index[v] for v in np.squeeze(np.where(ranker_model.predict(afam[X_ALL]) == 1))]\n",
    "    afam_predicted_1_index_Y1 = [i for (i,v) in zip(afam_predicted_1_index, X_ei.iloc[afam_predicted_1_index][Y]) if v==1]\n",
    "    afam_predicted_1 = X_ei.iloc[afam_predicted_1_index_Y1]\n",
    "    \n",
    "    afam_ranks = (ss.rankdata(ranker_model.decision_function(afam_predicted_1[X_ALL]))-1).astype(int)\n",
    "    afam_tochange = [i for (i, v) in zip(list(range(len(afam_ranks))), afam_ranks < delta_afam) if v]\n",
    "    afam_tochange_idx = [afam_predicted_1_index_Y1[v] for v in afam_tochange]\n",
    "    \n",
    "    cauca_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    cauca = X_ei[X_ei[S] == 0].copy()\n",
    "    delta_cauca = DELTA(train, X_ei, 0)\n",
    "    cauca_predicted_0_index = [cauca_index[v] for v in np.squeeze(np.where(ranker_model.predict(cauca[X_ALL]) == 0))]\n",
    "    cauca_predicted_0_index_Y0 = [i for (i,v) in zip(cauca_predicted_0_index, X_ei.iloc[cauca_predicted_0_index][Y]) if v==0]\n",
    "    cauca_predicted_0 = X_ei.iloc[cauca_predicted_0_index_Y0]\n",
    "    \n",
    "    cauca_ranks = (ss.rankdata(-ranker_model.decision_function(cauca_predicted_0[X_ALL]))-1).astype(int)\n",
    "    cauca_tochange = [i for (i, v) in zip(list(range(len(cauca_ranks))), cauca_ranks < delta_cauca) if v]\n",
    "    cauca_tochange_idx = [cauca_predicted_0_index_Y0[v] for v in cauca_tochange]\n",
    "    \n",
    "    for i in afam_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 0\n",
    "    for i in cauca_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 1\n",
    "    \n",
    "    relabeled_X_ei.append(X_ei_copy)\n",
    "    \n",
    "    print(\"DELTA(African American) = \", delta_afam, \"African Americans changed from 1 to 0\")\n",
    "    print(\"DELTA(Caucasian) = \", delta_cauca, \"Caucasians changed from 0 to 1\")\n",
    "    \n",
    "local_massaging = pd.concat(relabeled_X_ei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y9gbjkxIYCl0"
   },
   "outputs": [],
   "source": [
    "lm_X_train = local_massaging[X_ALL]\n",
    "lm_Y_train = local_massaging[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ij_weuRYESi",
    "outputId": "c71cca0f-67ff-4526-bc4c-21cf043d7358"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "clf.score(X_test_6[X_ALL], y_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7X_mtSMCYFNG",
    "outputId": "8bb2d551-fa23-48ab-ba5a-0de48a0db684"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number changed values should be sum of all DELTAs shown above\n",
    "res = [1 for i, j in zip(train.sort_index()[\"two_year_recid\"], pd.DataFrame(lm_Y_train).sort_index()[\"two_year_recid\"]) if i != j]\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhWjuXQbYGlN"
   },
   "source": [
    "### 2.2.3 Local Preferential Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tblu2osXYH5-"
   },
   "source": [
    "In this algorithm, we take in a dataset (train) and return a modified dataset of same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWfjPnNWYMXs",
    "outputId": "1b9dd82e-0d94-48d6-b0d0-935ff505adab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start partition\n",
      "X_ei shape: (1386, 16)\n",
      "Half Delta(AA): 23\n",
      "Half Delta(Cauc): 19\n",
      "Total AAs: 750\n",
      "Total Cs: 636\n",
      "afam dataset shape: (750, 16)\n",
      "c dataset shape: (636, 16)\n",
      "N: 23\n",
      "rows in cleaned_recid before: (355, 17)\n",
      "rows in cleaned_recid after: (332, 17)\n",
      "N: 23\n",
      "rows in cleaned_no_recid before: (395, 17)\n",
      "rows in cleaned_no_recid after: (418, 17)\n",
      "size of final A: (750, 17)\n",
      "M: 19\n",
      "rows in cleaned_recid before: (415, 17)\n",
      "rows in cleaned_recid after: (396, 17)\n",
      "M: 19\n",
      "rows in cleaned_no_recid before: (221, 17)\n",
      "rows in cleaned_no_recid after: (240, 17)\n",
      "size of final C: (636, 17)\n",
      "end partition\n",
      "size of train: (4130, 16)\n",
      "size of recomp: (1386, 16)\n",
      "start partition\n",
      "X_ei shape: (2744, 16)\n",
      "Half Delta(AA): 45\n",
      "Half Delta(Cauc): 27\n",
      "Total AAs: 1722\n",
      "Total Cs: 1022\n",
      "afam dataset shape: (1722, 16)\n",
      "c dataset shape: (1022, 16)\n",
      "N: 45\n",
      "rows in cleaned_recid before: (983, 17)\n",
      "rows in cleaned_recid after: (938, 17)\n",
      "N: 45\n",
      "rows in cleaned_no_recid before: (739, 17)\n",
      "rows in cleaned_no_recid after: (784, 17)\n",
      "size of final A: (1722, 17)\n",
      "M: 27\n",
      "rows in cleaned_recid before: (561, 17)\n",
      "rows in cleaned_recid after: (534, 17)\n",
      "M: 27\n",
      "rows in cleaned_no_recid before: (461, 17)\n",
      "rows in cleaned_no_recid after: (488, 17)\n",
      "size of final C: (1022, 17)\n",
      "end partition\n",
      "size of train: (4130, 16)\n",
      "size of recomp: (4130, 16)\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "recomp_train = pd.DataFrame()\n",
    "\n",
    "# for each partition (explanatory variable)\n",
    "for X_ei in PARTITION(train):\n",
    "    \n",
    "    print(\"start partition\")\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    print(\"X_ei shape:\", X_ei_copy.shape)\n",
    "    \n",
    "    # learn a ranker Hi : Xi -> Yi\n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    # Calculate half delta (AA: S_i = 1, AA: S_i = 0)\n",
    "    half_delta_afam = DELTA(train, X_ei, 1) // 2\n",
    "    half_delta_cauc = DELTA(train, X_ei, 0) // 2\n",
    "    print(\"Half Delta(AA):\", half_delta_afam)\n",
    "    print(\"Half Delta(Cauc):\", half_delta_cauc)\n",
    "    \n",
    "    # store indicies\n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    c_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    print(\"Total AAs:\", len(afam_index))\n",
    "    print(\"Total Cs:\", len(c_index))\n",
    "    \n",
    "    # get subset of data to work with\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    c = X_ei[X_ei[S] == 0].copy()\n",
    "    print(\"afam dataset shape:\", afam.shape)\n",
    "    print(\"c dataset shape:\", c.shape)\n",
    "    \n",
    "    # rank AA\n",
    "    afam.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(afam[X_ALL]), columns = ['rank'])\n",
    "    afam_with_rank = pd.concat([afam, rank], axis=1)\n",
    "    \n",
    "    # rank C\n",
    "    c.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(c[X_ALL]), columns = ['rank'])\n",
    "    c_with_rank = pd.concat([c, rank], axis=1)\n",
    "    \n",
    "    # sort values, reset indices\n",
    "    afam_with_rank = afam_with_rank.sort_values(['rank'])\n",
    "    afam_with_rank.reset_index(drop = True, inplace = True)\n",
    "    c_with_rank = c_with_rank.sort_values(['rank'])\n",
    "    c_with_rank.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ######## Modify AA data - find rows to delete/duplicate; decision boundary is 0 #####\n",
    "    recid = sum(afam_with_rank['rank'] > 0)\n",
    "    no_recid = sum(afam_with_rank['rank'] < 0)    \n",
    "    total = len(afam_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    # compas = compas[compas['days_b_screening_arrest'] >= -30]\n",
    "    cleaned_recid = afam_with_rank[afam_with_rank['rank'] > 0]\n",
    "    cleaned_no_recid = afam_with_rank[afam_with_rank['rank'] < 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    N = half_delta_afam\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:N], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:N])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_AA = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final A:\", total_AA.shape)\n",
    "    \n",
    "    ########## Modify C data ############\n",
    "    # Find rows to delete/duplicate; decision boundary is 0; opposite code as above\n",
    "    recid = sum(c_with_rank['rank'] < 0)\n",
    "    no_recid = sum(c_with_rank['rank'] > 0)    \n",
    "    total = len(c_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    cleaned_recid = c_with_rank[c_with_rank['rank'] < 0]\n",
    "    cleaned_no_recid = c_with_rank[c_with_rank['rank'] > 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    M = half_delta_cauc\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:M], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:M])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_C = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final C:\", total_C.shape)\n",
    "    print(\"end partition\")\n",
    "    \n",
    "    # combine both datasets\n",
    "    recomp_train = recomp_train.append(total_AA)\n",
    "    recomp_train = recomp_train.append(total_C)\n",
    "    recomp_train = recomp_train.drop('rank', axis=1)\n",
    "    \n",
    "    print(\"size of train:\", train.shape)\n",
    "    print(\"size of recomp:\", recomp_train.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzeyUYabYfVQ"
   },
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH4BB2d7Ym-f"
   },
   "source": [
    "Notation: P_c stands for probability based on the classifier's predictions.\n",
    "\n",
    "### Metrics Used:\n",
    "\n",
    "#### Parity or D_all\n",
    "\n",
    "Parity is defined as the difference is positive prediction rates in the two race groups. Paper 6 also calls this D_all, which stands for all discrimination. Fairness calls for Parity being close to 0.\n",
    "\n",
    "<br>\n",
    "<center>Parity = |P_c(recid = 1 | race = African American) - P_c(recid = 1 | race = Caucasian)</center>\n",
    "    \n",
    "#### Calibration\n",
    "\n",
    "Calibration is defined as the difference in accuracies between the two race groups. Fairness calls for Calibration being close to 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Calibration = |P_c(recid predicted correctly | race = African American) - P_c(recid predicted correctly | race = Caucasian)</center>\n",
    "\n",
    "#### Equality of Odds\n",
    "\n",
    "Equality of odds is achieved when the difference in positive prediction rates is equal for the two race groups. Fairness calls for the following value to be close to 0 for both y in {0,1}.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>D_Odds = P_c(recid.hat = 1 | race = African American, recid = y) - P_c(recid.hat = 1 | race = Caucasian, recid = y)</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YCtrJxRdYfAJ"
   },
   "outputs": [],
   "source": [
    "# X must include the sensitive feature\n",
    "def PARITY(X, Y_PRED):\n",
    "    s = X[S]\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    num_afam = sum(Y_PRED[X[S] == 1])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    num_cauca = sum(Y_PRED[X[S] == 0])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid = 1 | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid = 1 | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    parity = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Parity =\", parity)\n",
    "    \n",
    "    return(parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q2iH6nM9Y5eE"
   },
   "outputs": [],
   "source": [
    "# X must include S\n",
    "def CALIBRATION(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    Y_TRUE_afam = Y_TRUE[X[S] == 1]\n",
    "    num_afam = sum([1 for (i, v) in zip(Y_TRUE_afam, Y_PRED[X[S]==1]) if i == v])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    Y_TRUE_cauca = Y_TRUE[X[S] == 0]\n",
    "    num_cauca = sum([1 for (i, v) in zip(Y_TRUE_cauca, Y_PRED[X[S]==0]) if i == v])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid predicted correctly | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid predicted correctly | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    calibration = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Calibration =\", calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Pt0iQdPuY6Xw"
   },
   "outputs": [],
   "source": [
    "def EQUALITY_OF_ODDS(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    # S = afam, Y = 0\n",
    "    X_afam_0 = X[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    Y_PRED_afam_0 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    num_afam_0 = sum([1 for i in Y_PRED_afam_0 if i == 1])\n",
    "    denom_afam_0 = X_afam_0.shape[0]\n",
    "    P_afam_0 = num_afam_0/denom_afam_0\n",
    "    \n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 1])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 0\n",
    "    X_cauca_0 = X[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    Y_PRED_cauca_0 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    num_cauca_0 = sum([1 for i in Y_PRED_cauca_0 if i == 1])\n",
    "    denom_cauca_0 = X_cauca_0.shape[0]\n",
    "    P_cauca_0 = num_cauca_0/denom_cauca_0\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 1])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"For recid = 0:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 0) = \", P_afam_0)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 0) = \", P_cauca_0)\n",
    "    print(\"Difference in odds of true recid = 0 is  = D_FPR =\", abs(P_afam_0 - P_cauca_0))\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    print(\"Difference in odds of true recid = 1 is = D_TPR =\", abs(P_afam_1 - P_cauca_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "B4-awEJ2Y958"
   },
   "outputs": [],
   "source": [
    "def D_FNR(X, Y_TRUE, Y_PRED):\n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 0])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 0])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"Difference in False Negative Rates\")\n",
    "\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 0 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 0 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    \n",
    "    print(\"D_FNR =\", abs(P_afam_1 - P_cauca_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-nVpTZZZDxi"
   },
   "source": [
    "### 2.3.1 Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk0-uqK-ZLD5",
    "outputId": "b856aa86-7ceb-4a02-dba9-eca4970c7d32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_6, y_train_6)\n",
    "baseline_pred = clf.predict(X_test_6[X_ALL])\n",
    "clf.score(X_test_6, y_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxnIwyY6ZNlz",
    "outputId": "90866a3a-66db-42cc-9215-da7245d25bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       417\n",
      "           1       0.93      0.93      0.93       383\n",
      "\n",
      "    accuracy                           0.93       800\n",
      "   macro avg       0.93      0.93      0.93       800\n",
      "weighted avg       0.93      0.93      0.93       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_6, clf.predict(X_test_6[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4Ajuy0sZfcc",
    "outputId": "94ece690-e58f-4cb6-f420-1993a1d9f566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.5376569037656904\n",
      "P_c(recid = 1 | race = Caucasian) = 0.391304347826087\n",
      "Parity = 0.14635255593960345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14635255593960345"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_test_6, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x-X0JHkZhAK",
    "outputId": "6e09ad9f-eea2-4051-8a44-eb848903de2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.9372384937238494\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.9192546583850931\n",
      "Calibration = 0.01798383533875625\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_test_6, y_test_6, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kY9oSQpfZih9",
    "outputId": "fc2c91ce-105f-46a4-e12f-ff83be5113ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.06787330316742081\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.0663265306122449\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.001546772555175907\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.9416342412451362\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.8968253968253969\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.04480884441973931\n"
     ]
    }
   ],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_test_6, y_test_6, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bg7TQtL8Zjau",
    "outputId": "0337c4f8-cdf2-464a-cb5a-107dd4ebff04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in False Negative Rates\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 0 | race = African American, recid = 1) =  0.058365758754863814\n",
      "P_c(recid.hat = 0 | race = Caucasian, recid = 1) =  0.10317460317460317\n",
      "D_FNR = 0.044808844419739355\n"
     ]
    }
   ],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test_6, y_test_6, baseline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufz75N4HZnn1"
   },
   "source": [
    "### 2.3.2 Local Massaging Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUsfKimNZpbG",
    "outputId": "de634d24-9fef-45ee-cea2-cef6917b69fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "lm_pred = clf.predict(X_test_6[X_ALL])\n",
    "clf.score(X_test_6[X_ALL], y_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdfWQRTUZtqE",
    "outputId": "0bbafe47-0f5b-42bd-eb10-0eba17fe0969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       417\n",
      "           1       0.92      0.90      0.91       383\n",
      "\n",
      "    accuracy                           0.92       800\n",
      "   macro avg       0.92      0.91      0.91       800\n",
      "weighted avg       0.92      0.92      0.91       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_6, clf.predict(X_test_6[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giW3yI-cZuxb",
    "outputId": "92853e21-0a1e-474c-dce0-858328c9ab11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.49581589958158995\n",
      "P_c(recid = 1 | race = Caucasian) = 0.422360248447205\n",
      "Parity = 0.07345565113438496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07345565113438496"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_test_6, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8s31BMGZvxW",
    "outputId": "900a9c16-2ede-49b4-b4f7-c9d6255ac197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.9163179916317992\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.9130434782608695\n",
      "Calibration = 0.0032745133709296548\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_test_6, y_test_6, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L883FMWNZwxh",
    "outputId": "7183f7ac-617c-4a5a-9342-ba39171cdffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.04524886877828054\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.09693877551020408\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.051689906731923536\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.8832684824902723\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.9285714285714286\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.04530294608115626\n"
     ]
    }
   ],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_test_6, y_test_6, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXY61_2JZxjf",
    "outputId": "9be2a84c-3bb0-4d25-b02d-9e7fe2dafd40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in False Negative Rates\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 0 | race = African American, recid = 1) =  0.11673151750972763\n",
      "P_c(recid.hat = 0 | race = Caucasian, recid = 1) =  0.07142857142857142\n",
      "D_FNR = 0.045302946081156203\n"
     ]
    }
   ],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test_6, y_test_6, lm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYw2TywgZzG4"
   },
   "source": [
    "### 2.3.3 Local Preferential Sampling Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0sUBuy_Z2sg",
    "outputId": "ad104db8-2137-48b9-d26a-35fc099a4aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of recomp_X_train: (4130, 15)\n",
      "size of recomp_Y_train: (4130,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9275"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomp_X_train = recomp_train[X_ALL]\n",
    "recomp_Y_train = recomp_train[Y]\n",
    "print(\"size of recomp_X_train:\", recomp_X_train.shape)\n",
    "print(\"size of recomp_Y_train:\", recomp_Y_train.shape)\n",
    "\n",
    "clf_LPS = LogisticRegression(random_state=0).fit(recomp_X_train, recomp_Y_train)\n",
    "LPS_pred = clf_LPS.predict(X_test_6[X_ALL])\n",
    "clf_LPS.score(X_test_6[X_ALL], y_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gu6dnCuvZ4Sk",
    "outputId": "2a741ac7-a575-4bd3-aef4-3e5036c44dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       417\n",
      "           1       0.92      0.93      0.92       383\n",
      "\n",
      "    accuracy                           0.93       800\n",
      "   macro avg       0.93      0.93      0.93       800\n",
      "weighted avg       0.93      0.93      0.93       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_6, clf_LPS.predict(X_test_6[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbIMi67NZ5FZ",
    "outputId": "55babe84-3f5b-4b4b-e22e-64d7741a4e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.5418410041841004\n",
      "P_c(recid = 1 | race = Caucasian) = 0.391304347826087\n",
      "Parity = 0.15053665635801344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15053665635801344"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARITY(X_test_6, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inaKEk0zZ6Fk",
    "outputId": "f202817d-05b9-4958-a61c-1991c7f21cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.9330543933054394\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.9192546583850931\n",
      "Calibration = 0.013799734920346252\n"
     ]
    }
   ],
   "source": [
    "CALIBRATION(X_test_6, y_test_6, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKObEORIZ64L",
    "outputId": "c4e7b4f1-d229-45bd-8819-02c3b848d402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.07692307692307693\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.0663265306122449\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.010596546310832025\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.9416342412451362\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.8968253968253969\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.04480884441973931\n"
     ]
    }
   ],
   "source": [
    "EQUALITY_OF_ODDS(X_test_6, y_test_6, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkDqUZsUZ7sZ",
    "outputId": "b35756b1-7566-42e0-bec6-aedaa1ab6a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in False Negative Rates\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 0 | race = African American, recid = 1) =  0.058365758754863814\n",
      "P_c(recid.hat = 0 | race = Caucasian, recid = 1) =  0.10317460317460317\n",
      "D_FNR = 0.044808844419739355\n"
     ]
    }
   ],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test_6, y_test_6, LPS_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlqL8IREQx6t"
   },
   "source": [
    "# 3. Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment (DM and DM-sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFkhz8MEIQiC"
   },
   "source": [
    "## 3.1 Baseline Model And Evaluation (Using Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Q1yehARm7H2Z"
   },
   "outputs": [],
   "source": [
    "def base_nn_model(X_in,y_in,X_val,y_val):\n",
    "    feature = Input(X_in.shape[1],)\n",
    "    y = Dense(2,\"softmax\")(feature)\n",
    "    model = Model(feature,y)\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(0.001)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metric = [tf.keras.metrics.BinaryAccuracy()]\n",
    "    #,tf.keras.metrics.FalsePositives()，tf.keras.metrics.FalseNegatives()\n",
    "    model.compile(optimizer=adam, loss=loss, metrics=metric)\n",
    "    model.fit(X_in,tf.one_hot(y_in,2),epochs=10,batch_size=10,validation_data=(X_val,tf.one_hot(y_val,2)))\n",
    "    return model\n",
    "\n",
    "def evaluation(model,X,y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
    "    y_pred_AA, y_test_AA = y_pred[test_idx_AA], y[test_idx_AA]\n",
    "    y_pred_C, y_test_C = y_pred[test_idx_C], y[test_idx_C]\n",
    "    acc = model.evaluate(X, tf.one_hot(y,2))[1]\n",
    "    FPR_all = sum(y_pred[y==0]==1)/len(y[y==0])\n",
    "    FNR_all = sum(y_pred[y==1]==0)/len(y[y==1])\n",
    "    FPR_AA = sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
    "    FNR_AA = sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
    "    FPR_C = sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
    "    FNR_C = sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
    "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
    "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
    "    print('\\n',\"#\"*80)\n",
    "    print('The accuracy of baseline model NN is: %3f.'%(acc))\n",
    "    print('The False Positive Rate for overall population is: %3f.'%FPR_all)\n",
    "    print('The False Negative Rate for overall population is: %3f.'%FNR_all)\n",
    "    print(\"Specifically:\")\n",
    "    print('Parity Check: The rate of positive estimate for African American and Caucasian are %3f and %3f, and D_par=%3f.'%(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
    "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are %3f and %3f, and D_cal=%3f.'%(acc_AA,acc_C,acc_AA-acc_C))\n",
    "    print('The False Positive Rate for African American and Caucasian are %3f and %3f, and D_FPR=%3f.'%(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
    "    print('The False Negative Rate for African American and Caucasian are %3f and %3f, and D_FNR=%3f.'%(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
    "    print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7C6kS-eRCs5",
    "outputId": "d17b7435-bc14-4162-8816-3cd38b828099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.6104 - binary_accuracy: 0.7247 - val_loss: 0.4960 - val_binary_accuracy: 0.8037\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 0.4546 - binary_accuracy: 0.8264 - val_loss: 0.4063 - val_binary_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 1s 3ms/step - loss: 0.3818 - binary_accuracy: 0.8680 - val_loss: 0.3522 - val_binary_accuracy: 0.8813\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3353 - binary_accuracy: 0.8881 - val_loss: 0.3146 - val_binary_accuracy: 0.8925\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3025 - binary_accuracy: 0.8998 - val_loss: 0.2871 - val_binary_accuracy: 0.8988\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2780 - binary_accuracy: 0.9075 - val_loss: 0.2665 - val_binary_accuracy: 0.9075\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2594 - binary_accuracy: 0.9133 - val_loss: 0.2505 - val_binary_accuracy: 0.9087\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2451 - binary_accuracy: 0.9160 - val_loss: 0.2380 - val_binary_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2337 - binary_accuracy: 0.9179 - val_loss: 0.2275 - val_binary_accuracy: 0.9112\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2246 - binary_accuracy: 0.9201 - val_loss: 0.2192 - val_binary_accuracy: 0.9150\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2076 - binary_accuracy: 0.9300\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.930000.\n",
      "The False Positive Rate for overall population is: 0.083933.\n",
      "The False Negative Rate for overall population is: 0.054830.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.562762 and 0.397516, and D_par=0.165246.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.928870 and 0.931677, and D_cal=-0.002807.\n",
      "The False Positive Rate for African American and Caucasian are 0.104072 and 0.061224, and D_FPR=0.042848.\n",
      "The False Negative Rate for African American and Caucasian are 0.042802 and 0.079365, and D_FNR=-0.036564.\n",
      "\n",
      " ################################################################################\n",
      "Epoch 1/10\n",
      "413/413 [==============================] - 2s 3ms/step - loss: 0.6205 - binary_accuracy: 0.7332 - val_loss: 0.5167 - val_binary_accuracy: 0.7975\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.4597 - binary_accuracy: 0.8312 - val_loss: 0.4146 - val_binary_accuracy: 0.8525\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3842 - binary_accuracy: 0.8678 - val_loss: 0.3559 - val_binary_accuracy: 0.8725\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3368 - binary_accuracy: 0.8874 - val_loss: 0.3165 - val_binary_accuracy: 0.8975\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3038 - binary_accuracy: 0.8990 - val_loss: 0.2883 - val_binary_accuracy: 0.9050\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2796 - binary_accuracy: 0.9097 - val_loss: 0.2673 - val_binary_accuracy: 0.9125\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2613 - binary_accuracy: 0.9148 - val_loss: 0.2512 - val_binary_accuracy: 0.9162\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2470 - binary_accuracy: 0.9165 - val_loss: 0.2383 - val_binary_accuracy: 0.9162\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2356 - binary_accuracy: 0.9167 - val_loss: 0.2278 - val_binary_accuracy: 0.9175\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2264 - binary_accuracy: 0.9196 - val_loss: 0.2195 - val_binary_accuracy: 0.9212\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2076 - binary_accuracy: 0.9275\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.927500.\n",
      "The False Positive Rate for overall population is: 0.079137.\n",
      "The False Negative Rate for overall population is: 0.065274.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.554393 and 0.391304, and D_par=0.163089.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.924686 and 0.931677, and D_cal=-0.006991.\n",
      "The False Positive Rate for African American and Caucasian are 0.099548 and 0.056122, and D_FPR=0.043425.\n",
      "The False Negative Rate for African American and Caucasian are 0.054475 and 0.087302, and D_FNR=-0.032827.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#If we don't drop 'race' in the X_train and X_test:\n",
    "NN1 = base_nn_model(X_train,y_train,X_val,y_val)\n",
    "evaluation(NN1,X_test,y_test)\n",
    "#If we drop 'race' in the X_train and X_test:\n",
    "NN2 = base_nn_model(np.delete(X_train,0,1),y_train,np.delete(X_val,0,1),y_val)\n",
    "evaluation(NN2,np.delete(X_test,1,1),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXWOqulX9WSV"
   },
   "source": [
    "## 3.2 Baseline Model with customized constraint and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJz_2QB2NvLn",
    "outputId": "34a9556b-ae4c-4d22-abd2-02e1e04fb33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0365635229448459 0.04284790839412689\n"
     ]
    }
   ],
   "source": [
    "def DFR(model,X,y,type):\n",
    "    '''\n",
    "    type: str in ['dfnr','dfpr','both']\n",
    "    '''\n",
    "    if type!='dfnr' and type!='dfpr' and type!='both':\n",
    "        return None\n",
    "    size = len(y)\n",
    "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
    "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
    "    y_pred_AA, y_AA = y_pred[idx_AA], y[idx_AA]\n",
    "    y_pred_C, y_C = y_pred[idx_C], y[idx_C] \n",
    "    FPR_AA = sum(y_pred_AA[y_AA==0]==1)/len(y_AA[y_AA==0])\n",
    "    FNR_AA = sum(y_pred_AA[y_AA==1]==0)/len(y_AA[y_AA==1])\n",
    "    FPR_C = sum(y_pred_C[y_C==0]==1)/len(y_C[y_C==0])\n",
    "    FNR_C = sum(y_pred_C[y_C==1]==0)/len(y_C[y_C==1])\n",
    "    dfnr = FNR_AA-FNR_C\n",
    "    dfpr = FPR_AA-FPR_C\n",
    "    if type=='dfnr':\n",
    "        return dfnr\n",
    "    elif type=='dfpr':\n",
    "        return dfpr\n",
    "    else:\n",
    "        return dfnr,dfpr\n",
    "dfnr,dfpr = DFR(NN1,X_test,y_test,'both')\n",
    "print(dfnr,dfpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wqkxAj9pDcbY"
   },
   "outputs": [],
   "source": [
    "def new_training_groups(model, X_train, y_train):\n",
    "    '''\n",
    "    X       n*d\n",
    "    model.predict(X)    n*2\n",
    "    y       n,\n",
    "    delta   c,\n",
    "    dfr   \"dfpr\",\"dfnr\",\"both\"\n",
    "    '''\n",
    "    \n",
    "    #split training sets according to sensitive variable\n",
    "    X_train_AA = X_train[np.array(X_train[:,1] == 0.0)]\n",
    "    y_train_AA = y_train[np.array(X_train[:,1] == 0.0)]\n",
    "    X_train_C = X_train[np.array(X_train[:,1] == 1.0)]\n",
    "    y_train_C = y_train[np.array(X_train[:,1] == 1.0)]\n",
    "    #get the ones with wrong prediction in discriminated group\n",
    "    dn,dp = DFR(model,X_train,y_train,type=\"both\")\n",
    "    if dp>0: d = 0\n",
    "    else: d = 1\n",
    "\n",
    "    if d == 0:\n",
    "        #take penalized trainers\n",
    "        y_pred_AA = np.argmax(model.predict(X_train_AA),axis = 1)\n",
    "        y_diff_AA = y_train_AA-y_pred_AA\n",
    "        X_train_penalized = X_train_AA[y_diff_AA != 0.0]\n",
    "        y_train_penalized = y_train_AA[y_diff_AA != 0.0]\n",
    "        # safe trainers\n",
    "        X_train_clean = X_train_AA[y_diff_AA == 0.0]\n",
    "        y_train_clean = y_train_AA[y_diff_AA == 0.0]\n",
    "        #make new\n",
    "        X_train_clean = np.concatenate((X_train_clean,X_train_C),axis=0)\n",
    "        y_train_clean = np.concatenate((y_train_clean,y_train_C),axis=0)\n",
    "    \n",
    "    else:\n",
    "        #reverse the steps above for train set 1\n",
    "        y_pred_C = np.argmax(model.predict(X_train_C),axis = 1)\n",
    "        y_diff_C = y_train_C-y_pred_C\n",
    "        X_train_penalized = X_train_C[y_diff_C != 0.0]\n",
    "        y_train_penalized = y_train_C[y_diff_C != 0.0]\n",
    "        # safe trainers\n",
    "        X_train_clean = X_train_C[y_diff_C == 0.0]\n",
    "        y_train_clean = y_train_C[y_diff_C == 0.0]\n",
    "        #make new\n",
    "        X_train_clean = np.concatenate((X_train_clean,X_train_AA),axis=0)\n",
    "        y_train_clean = np.concatenate((y_train_clean,y_train_AA),axis=0)\n",
    "    \n",
    "    #X_train_penalized = tf.convert_to_tensor(X_train_penalized, dtype=tf.float32)\n",
    "    #X_train_safe = tf.convert_to_tensor(X_train_safe, dtype=tf.float32)\n",
    "    return X_train_clean , y_train_clean , X_train_penalized , y_train_penalized, dn, dp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge3I04qX2yJl"
   },
   "source": [
    "**Note: When few features are used in the model, the model and its prediction are unstable. Even though features are added, it sometimes ends the loop when D(fpr)>0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUALnfmSLCwE",
    "outputId": "f5b24409-68e3-4c8c-fcff-8f6348a73abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.6254 - binary_accuracy: 0.7535 - val_loss: 0.5199 - val_binary_accuracy: 0.8275\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.4576 - binary_accuracy: 0.8661 - val_loss: 0.4080 - val_binary_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3746 - binary_accuracy: 0.8864 - val_loss: 0.3449 - val_binary_accuracy: 0.8963\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.3237 - binary_accuracy: 0.9015 - val_loss: 0.3037 - val_binary_accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2893 - binary_accuracy: 0.9102 - val_loss: 0.2746 - val_binary_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2650 - binary_accuracy: 0.9157 - val_loss: 0.2539 - val_binary_accuracy: 0.9125\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2472 - binary_accuracy: 0.9182 - val_loss: 0.2384 - val_binary_accuracy: 0.9137\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2339 - binary_accuracy: 0.9206 - val_loss: 0.2265 - val_binary_accuracy: 0.9162\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2236 - binary_accuracy: 0.9208 - val_loss: 0.2173 - val_binary_accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.2156 - binary_accuracy: 0.9232 - val_loss: 0.2102 - val_binary_accuracy: 0.9162\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 1s 3ms/step - loss: 0.9071 - binary_accuracy: 0.4160 - val_loss: 0.8304 - val_binary_accuracy: 0.3862\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.7811 - binary_accuracy: 0.3630 - val_loss: 0.7392 - val_binary_accuracy: 0.3475\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.7104 - binary_accuracy: 0.3397 - val_loss: 0.6851 - val_binary_accuracy: 0.3250\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.6661 - binary_accuracy: 0.3143 - val_loss: 0.6485 - val_binary_accuracy: 0.2850\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.6341 - binary_accuracy: 0.2833 - val_loss: 0.6203 - val_binary_accuracy: 0.2488\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.6083 - binary_accuracy: 0.2470 - val_loss: 0.5966 - val_binary_accuracy: 0.2188\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5861 - binary_accuracy: 0.2136 - val_loss: 0.5758 - val_binary_accuracy: 0.1975\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5664 - binary_accuracy: 0.1966 - val_loss: 0.5571 - val_binary_accuracy: 0.1875\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5486 - binary_accuracy: 0.1801 - val_loss: 0.5402 - val_binary_accuracy: 0.1713\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5324 - binary_accuracy: 0.1695 - val_loss: 0.5245 - val_binary_accuracy: 0.1562\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 1s 3ms/step - loss: 1.7477 - binary_accuracy: 0.2481 - val_loss: 1.3499 - val_binary_accuracy: 0.3738\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 1.1777 - binary_accuracy: 0.4649 - val_loss: 1.0434 - val_binary_accuracy: 0.5213\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.9578 - binary_accuracy: 0.5772 - val_loss: 0.8824 - val_binary_accuracy: 0.6025\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.8256 - binary_accuracy: 0.6380 - val_loss: 0.7730 - val_binary_accuracy: 0.6413\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.7302 - binary_accuracy: 0.6726 - val_loss: 0.6896 - val_binary_accuracy: 0.6837\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.6554 - binary_accuracy: 0.7002 - val_loss: 0.6224 - val_binary_accuracy: 0.7125\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5941 - binary_accuracy: 0.7262 - val_loss: 0.5666 - val_binary_accuracy: 0.7325\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5427 - binary_accuracy: 0.7453 - val_loss: 0.5193 - val_binary_accuracy: 0.7550\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.4987 - binary_accuracy: 0.7656 - val_loss: 0.4785 - val_binary_accuracy: 0.7713\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.4607 - binary_accuracy: 0.7816 - val_loss: 0.4431 - val_binary_accuracy: 0.7825\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 1s 3ms/step - loss: 1.4945 - binary_accuracy: 0.8306 - val_loss: 1.0354 - val_binary_accuracy: 0.8587\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.8704 - binary_accuracy: 0.8119 - val_loss: 0.7487 - val_binary_accuracy: 0.7375\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.6773 - binary_accuracy: 0.6806 - val_loss: 0.6167 - val_binary_accuracy: 0.6288\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5740 - binary_accuracy: 0.6034 - val_loss: 0.5355 - val_binary_accuracy: 0.5838\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5059 - binary_accuracy: 0.5538 - val_loss: 0.4783 - val_binary_accuracy: 0.5437\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.4560 - binary_accuracy: 0.5211 - val_loss: 0.4349 - val_binary_accuracy: 0.5225\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.4172 - binary_accuracy: 0.4990 - val_loss: 0.4002 - val_binary_accuracy: 0.5075\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.3856 - binary_accuracy: 0.4831 - val_loss: 0.3715 - val_binary_accuracy: 0.4938\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.3592 - binary_accuracy: 0.4676 - val_loss: 0.3472 - val_binary_accuracy: 0.4850\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.3366 - binary_accuracy: 0.4547 - val_loss: 0.3262 - val_binary_accuracy: 0.4787\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 1s 3ms/step - loss: 1.9943 - binary_accuracy: 0.5830 - val_loss: 1.0849 - val_binary_accuracy: 0.7212\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.8227 - binary_accuracy: 0.7588 - val_loss: 0.6399 - val_binary_accuracy: 0.7912\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.5459 - binary_accuracy: 0.7966 - val_loss: 0.4707 - val_binary_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.4228 - binary_accuracy: 0.8259 - val_loss: 0.3815 - val_binary_accuracy: 0.8338\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.3520 - binary_accuracy: 0.8329 - val_loss: 0.3253 - val_binary_accuracy: 0.8562\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.3049 - binary_accuracy: 0.8395 - val_loss: 0.2861 - val_binary_accuracy: 0.8562\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.2710 - binary_accuracy: 0.8407 - val_loss: 0.2567 - val_binary_accuracy: 0.8587\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.2450 - binary_accuracy: 0.8390 - val_loss: 0.2338 - val_binary_accuracy: 0.8550\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.2244 - binary_accuracy: 0.8378 - val_loss: 0.2153 - val_binary_accuracy: 0.8512\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 2ms/step - loss: 0.2075 - binary_accuracy: 0.8361 - val_loss: 0.1999 - val_binary_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "## initialization\n",
    "np.random.seed(7777) \n",
    "model =  base_nn_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "#initialized C and delta\n",
    "C = 1\n",
    "delta = 0.2\n",
    "\n",
    "# new training groups\n",
    "X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(model, X_train, y_train)\n",
    "feature = Input(X_train.shape[1],)\n",
    "y = Dense(2,\"softmax\")(feature)\n",
    "mod_loop = Model(feature,y)\n",
    "adam = tf.keras.optimizers.Adam(0.001)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "def penal_loss(y_true,y_pred):\n",
    "  return loss(tf.one_hot(y_tp,2), mod_loop(X_tp))\n",
    "\n",
    "def clean_loss(y_true,y_pred):  \n",
    "  return loss(tf.one_hot(y_ts,2), mod_loop(X_ts))\n",
    "\n",
    "count = 0\n",
    "# start while loop\n",
    "while (count==0 or count%2==1 or abs(dp)>0.05) and count<20: \n",
    "    # or examine dn, here count%2==1 used to control accuracy in case the accuracy is below 0.5 and one of the overall fpr/fnr will be close to 1\n",
    "    C = C+delta\n",
    "    #print('Count:%d'%count)\n",
    "    mod_loop.compile(optimizer=adam,loss=[penal_loss, clean_loss],loss_weights=[C,1],metrics=metric)\n",
    "    mod_loop.fit(X_train, tf.one_hot(y_train,2), epochs=10, validation_data=(X_val,tf.one_hot(y_val,2)))\n",
    "    X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(mod_loop, X_train, y_train)\n",
    "    #dp = DFR(model_in_loop,X_test,y_test,'dfpr')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WStZGDfzefQk",
    "outputId": "d6613a4c-2a80-4b2a-efc0-08aedf0b875d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.06858748687542465, 0.07519161510758149)\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1999 - binary_accuracy: 0.8375\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.837500.\n",
      "The False Positive Rate for overall population is: 0.055156.\n",
      "The False Negative Rate for overall population is: 0.279373.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.441423 and 0.273292, and D_par=0.168131.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.820084 and 0.863354, and D_cal=-0.043270.\n",
      "The False Positive Rate for African American and Caucasian are 0.090498 and 0.015306, and D_FPR=0.075192.\n",
      "The False Negative Rate for African American and Caucasian are 0.256809 and 0.325397, and D_FNR=-0.068587.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "print(DFR(mod_loop,X_test,y_test,type='both'))\n",
    "evaluation(mod_loop,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5dLHriKnrKIr"
   },
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################### IF NEED TO MODIFY CODES, USE THIS CHUNK TO RUN ABOVE OR RUN BELOW ###########################\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxdmajarNgms"
   },
   "source": [
    "## 3.3 Implementation of $DM_{sen}$ & $DM$ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkEdvRXvThHt"
   },
   "source": [
    "### 3.3.1 Implementation on $DM_{sen}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idcynX6VBaSu"
   },
   "source": [
    "We first implement $DM_{sen}$ as we won't do anything to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQIJDMSy164k",
    "outputId": "052034f9-270f-433a-82c5-451a962e8a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################ \n",
      "                       Split up Train-Test sets  \n",
      " ################################################################################ \n",
      "\n",
      " X_train size:  (4584, 16) ,      y_train size:  (4584,) \n",
      " X_test size:  (1146, 16) ,       y_test size:  (1146,)\n",
      " X_train_AA size:  (2745, 16) ,   X_train_C size:  (1839, 16) \n",
      " X_test_AA size:  (688, 16) ,     X_test_C size:  (458, 16) \n",
      " Ratio: 1.4926590538336053 1.502183406113537\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#Use 4:1 as the ratio of train:test\n",
    "y = data.two_year_recid\n",
    "y = np.asarray(y).astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "train_size = len(y_train)\n",
    "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
    "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
    "test_size = len(y_test)\n",
    "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
    "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
    "\n",
    "X_train1 = np.hstack((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train))\n",
    "X_train1_AA = X_train1[train_idx_AA]\n",
    "X_train1_C = X_train1[train_idx_C]\n",
    "y_train_AA = y_train[train_idx_AA]\n",
    "y_train_C = y_train[train_idx_C]\n",
    "X_test1 = np.hstack((np.ones(X_test.shape[0]).reshape(X_test.shape[0],1), X_test))\n",
    "X_test1_AA = X_test1[test_idx_AA]\n",
    "X_test1_C = X_test1[test_idx_C]\n",
    "y_test_AA = y_test[test_idx_AA]\n",
    "y_test_C = y_test[test_idx_C]\n",
    "\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train size: \", X_train1.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_test size: \", X_test1.shape, ',       y_test size: ',y_test.shape)\n",
    "print(\" X_train_AA size: \", X_train1[train_idx_AA].shape, \",   X_train_C size: \", X_train1[train_idx_C].shape, '\\n',\n",
    "      \"X_test_AA size: \", X_test1[test_idx_AA].shape, ',     X_test_C size: ',X_test1[test_idx_C].shape, '\\n',\n",
    "      \"Ratio:\", X_train1[train_idx_AA].shape[0]/X_train1[train_idx_C].shape[0],X_test1[test_idx_AA].shape[0]/X_test1[test_idx_C].shape[0])\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raHMVMyr0Gsg"
   },
   "source": [
    "(Note: From the paper, loss function is modified in logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3a7d_AwRmOFo"
   },
   "outputs": [],
   "source": [
    "#pip install dccp #when running in jupyter notebook, delete this chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "o4W8A43KqgwG"
   },
   "outputs": [],
   "source": [
    "###PLEASE DON'T MOVE THIS CHUNK TO 1.1 IMPORT ESSENTIAL PACKAGES\n",
    "import dccp\n",
    "import cvxpy as cvx\n",
    "from cvxpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O9LwGH3lvpz",
    "outputId": "9a26a499-ff6d-4787-a03d-428dfa00cfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def lossfunc(X,theta,y_true):\n",
    "    # This function returns the log loss.\n",
    "    y_true= 2*y_true - 1 #{0,1}->{-1,1}\n",
    "    log_loss = sum(logistic(multiply(-y_true, X*theta)))\n",
    "    return log_loss\n",
    "\n",
    "\n",
    "np.random.seed(5243)\n",
    "theta = cvx.Variable(X_train1.shape[1])\n",
    "theta.value = np.random.rand(theta.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.005, 1.5, 1 \n",
    "Prob1 = cvx.Problem(Minimize(lossfunc(X_train1,theta,y_train)),[]) # No constraints\n",
    "             \n",
    "      \n",
    "print(dccp.is_dccp(Prob1))\n",
    "#print(theta.value)\n",
    "#[0.5591043  0.9994264  0.57031546 0.43833912 0.08453454 0.05043884\n",
    "# 0.91119515 0.16423428 0.3034639  0.41950956 0.85237613 0.4244003\n",
    "# 0.96147514 0.26277008 0.02849745 0.61075812]\n",
    "result = Prob1.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True) #Here changes the theta.value, result avoids the output\n",
    "#print(theta.value)\n",
    "#[-2.02760707  0.12980288  0.12303316 -0.27410271  1.71950929  4.75786739\n",
    "# -0.27591511  0.02399848  0.45356176  0.05861524  0.04602311  0.27193313\n",
    "# -0.04650322  0.15181987 -0.45112505 -2.24001541]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "tPl_LD06m2BN"
   },
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    #y:{-1,1}->{0,1}\n",
    "    d = np.dot(X,theta)\n",
    "    y_pred = (np.sign(d) + 1)/2\n",
    "    return y_pred\n",
    "\n",
    "theta_star = theta.value\n",
    "y_pred = predict(X_test1, theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpNPZC-3nYhS",
    "outputId": "d84573c8-edee-4614-f3f1-f2c6d34ce74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model LR is: 0.924084.\n",
      "The False Positive Rate for overall population is: 0.084956.\n",
      "The False Negative Rate for overall population is: 0.067126.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.423729 and 0.538462, and D_par=-0.114733.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.932203 and 0.921978, and D_cal=0.010225.\n",
      "The False Positive Rate for African American and Caucasian are 0.077465 and 0.087470, and D_FPR=-0.010006.\n",
      "The False Negative Rate for African American and Caucasian are 0.053191 and 0.069815, and D_FNR=-0.016624.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "def evaluation_DM(X,y,y_pred):\n",
    "    size = X.shape[0]\n",
    "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
    "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
    "    y_pred_AA, y_test_AA = y_pred[idx_AA], y[idx_AA]\n",
    "    y_pred_C, y_test_C = y_pred[idx_C], y[idx_C]\n",
    "    FPR_all = np.sum(y_pred[y==0]==1)/len(y[y==0])\n",
    "    FNR_all = np.sum(y_pred[y==1]==0)/len(y[y==1])\n",
    "    FPR_AA = np.sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
    "    FNR_AA = np.sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
    "    FPR_C = np.sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
    "    FNR_C = np.sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
    "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
    "    acc = np.sum(y_pred == y)/len(y)\n",
    "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
    "    print('\\n',\"#\"*80)\n",
    "    print('The accuracy of baseline model LR is: %3f.'%(acc))\n",
    "    print('The False Positive Rate for overall population is: %3f.'%FPR_all)\n",
    "    print('The False Negative Rate for overall population is: %3f.'%FNR_all)\n",
    "    print(\"Specifically:\")\n",
    "    print('Parity Check: The rate of positive estimate for African American and Caucasian are %3f and %3f, and D_par=%3f.'%(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
    "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are %3f and %3f, and D_cal=%3f.'%(acc_AA,acc_C,acc_AA-acc_C))\n",
    "    print('The False Positive Rate for African American and Caucasian are %3f and %3f, and D_FPR=%3f.'%(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
    "    print('The False Negative Rate for African American and Caucasian are %3f and %3f, and D_FNR=%3f.'%(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
    "    print('\\n',\"#\"*80)\n",
    "#print(y_pred.shape)\n",
    "evaluation_DM(X_test1,y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIjSM7mcsOY5"
   },
   "source": [
    "If we do not put constaints on the loss function, the accuracy of the logistic regression model is around $92\\%$, while the FPR, FNR are around 0.05. \n",
    "\n",
    "Then, we put the constraint in the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_9JaBPrsNhr",
    "outputId": "8aab5574-99c7-42fa-dd9c-8631d0509852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584 2745 1839\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Constaints on loss function\n",
    "np.random.seed(5243)\n",
    "theta1 = cvx.Variable(X_train1.shape[1])\n",
    "theta1.value = np.random.rand(theta.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
    "\n",
    "def g_theta(y,X,theta):\n",
    "    y = 2*y - 1\n",
    "    d = matmul(X,theta)\n",
    "    y_d = multiply(y,d)\n",
    "    return minimum(np.zeros_like(y_d),y_d)\n",
    "\n",
    "c = 0.05\n",
    "N0 = X_train1_AA.shape[0]\n",
    "N1 = X_train1_C.shape[0]\n",
    "N = X_train1.shape[0]\n",
    "print(N,N0,N1)\n",
    "\n",
    "\n",
    "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1,theta1,y_train)),\n",
    "                 [N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) <= c + N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)), \n",
    "                  N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) >= N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)) - c]) # With constraints\n",
    "print(dccp.is_dccp(Prob2))\n",
    "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)\n",
    "#g_theta(y_train,X_train1, theta.value).value.shape\n",
    "#constraint()\n",
    "#X_train.\n",
    "#pd.DataFrame(X_train)\n",
    "#pd.DataFrame(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1g7epK30aT2",
    "outputId": "16c26fdb-43f6-4442-e75e-38e7c03d677b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model LR is: 0.924084.\n",
      "The False Positive Rate for overall population is: 0.088496.\n",
      "The False Negative Rate for overall population is: 0.063683.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.427966 and 0.541758, and D_par=-0.113792.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.927966 and 0.923077, and D_cal=0.004889.\n",
      "The False Positive Rate for African American and Caucasian are 0.084507 and 0.089835, and D_FPR=-0.005327.\n",
      "The False Negative Rate for African American and Caucasian are 0.053191 and 0.065708, and D_FNR=-0.012517.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = predict(X_test1, theta1.value)\n",
    "evaluation_DM(X_test1,y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFSvGNJHRban"
   },
   "source": [
    "From above result, we may see the $DM_{sen}$ algorithm slightly drops the $D_{FPR}$ to around -0.005, which is very close to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rSczquXTyts"
   },
   "source": [
    "### 3.3.2 implementation on $DM$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdzBsm2yewlI",
    "outputId": "865e07b9-cfe4-4117-9ddb-1f91a89d20a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################ \n",
      "                       Split up Train-Test sets  \n",
      " ################################################################################ \n",
      "\n",
      " X_train1_sen size:  (4584, 15) ,      y_train size:  (4584,) \n",
      " X_test1_sen size:  (1146, 15) ,       y_test size:  (1146,)\n",
      " X_train1_AA_sen size:  (2745, 15) ,   X_train1_C_sen size:  (1839, 15) \n",
      " X_test1_AA_sen size:  (688, 15) ,     X_test1_C_sen size:  (458, 15) \n",
      " Ratio: 1.4926590538336053 1.502183406113537\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "X_train1_sen = np.delete(X_train1,2,1)\n",
    "X_train1_AA_sen = X_train1_sen[train_idx_AA]\n",
    "X_train1_C_sen = X_train1_sen[train_idx_C]\n",
    "X_test1_sen = np.delete(X_test1,2,1)\n",
    "X_test1_AA_sen = X_test1_sen[test_idx_AA]\n",
    "X_test1_C_sen = X_test1_sen[test_idx_C]\n",
    "\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train1_sen size: \", X_train1_sen.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_test1_sen size: \", X_test1_sen.shape, ',       y_test size: ',y_test.shape)\n",
    "print(\" X_train1_AA_sen size: \", X_train1_AA_sen.shape, \",   X_train1_C_sen size: \", X_train1_C_sen.shape, '\\n',\n",
    "      \"X_test1_AA_sen size: \", X_test1_AA_sen.shape, ',     X_test1_C_sen size: ',X_test1_C_sen.shape, '\\n',\n",
    "      \"Ratio:\", X_train1_AA_sen.shape[0]/X_train1_C_sen.shape[0],X_test1_AA_sen.shape[0]/X_test1_C_sen.shape[0])\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4JivYlwa3Dr",
    "outputId": "e26b7434-f58d-4942-a763-e6d2d6379072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584 2745 1839\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Constaints on loss function\n",
    "np.random.seed(5243)\n",
    "theta2 = cvx.Variable(X_train1_sen.shape[1])\n",
    "theta2.value = np.random.rand(theta2.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
    "c = 0.05\n",
    "N0 = X_train1_AA_sen.shape[0]\n",
    "N1 = X_train1_C_sen.shape[0]\n",
    "N = X_train1_sen.shape[0]\n",
    "print(N,N0,N1)\n",
    "\n",
    "\n",
    "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1_sen,theta2,y_train)),\n",
    "                 [N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) <= c + N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)), \n",
    "                  N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) >= N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)) - c]) # With constraints\n",
    "print(dccp.is_dccp(Prob2))\n",
    "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0sU93opf_Pr",
    "outputId": "3c4c9fe8-ac15-4f86-e8fc-d2e9b087f738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model LR is: 0.925829.\n",
      "The False Positive Rate for overall population is: 0.084956.\n",
      "The False Negative Rate for overall population is: 0.063683.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.423729 and 0.540659, and D_par=-0.116931.\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.932203 and 0.924176, and D_cal=0.008028.\n",
      "The False Positive Rate for African American and Caucasian are 0.077465 and 0.087470, and D_FPR=-0.010006.\n",
      "The False Negative Rate for African American and Caucasian are 0.053191 and 0.065708, and D_FNR=-0.012517.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = predict(X_test1_sen, theta2.value)\n",
    "evaluation_DM(X_test1_sen,y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEvRcabGJbl4",
    "outputId": "4658260a-fda6-42c4-858b-5bb47fe8edfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time of overall algorithm is: 416.239410s.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('The running time of overall algorithm is: %3fs.'%(end-start)) # around 100 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJzteiF3gNRP"
   },
   "source": [
    "We may see $DM$ algorithm drops $D_{FNR}$. But from the above results, it's hard for us to see which one is perfect and how $DM_{sen}$ violates the disparate treatment. Overall speaking, this algorithm has an impact on controlling the difference in FPR and FNR, but the effect deserves further study as when few features are in the model, both two algorithms seem to have no effect on controlling our target."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main_2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
