{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4b5532",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "\n",
    "Implementating, evaluating and comparing on  algorithms: Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment (DM and DM-sen) and Handling Conditional Discrimination (LM and LPS)\n",
    "\n",
    "Team members: \n",
    "\n",
    "- Sarah Kurihara\n",
    "\n",
    "- Varchasvi Vedula\n",
    "\n",
    "- Wenhui Fang\n",
    "\n",
    "- Krista Zhang\n",
    "\n",
    "- Sharon Meng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9553c10d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efba2fb",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14070a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import time,sys\n",
    "import zipfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "import scipy.stats as ss\n",
    "import dccp\n",
    "import cvxpy as cvx\n",
    "from cvxpy import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0a6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10067a12",
   "metadata": {},
   "source": [
    "### Load Dataset and Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227f5880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sex  age race  juv_fel_count  decile_score  juv_misd_count  juv_other_count  \\\n",
      "1   1   34    0              0             3               0                0   \n",
      "2   1   24    0              0             4               0                1   \n",
      "6   1   41    1              0             6               0                0   \n",
      "8   0   39    1              0             1               0                0   \n",
      "9   1   21    1              0             3               0                0   \n",
      "\n",
      "   priors_count  c_days_from_compas c_charge_degree  is_violent_recid  event  \\\n",
      "1             0            0.693147               0                 1      1   \n",
      "2             4            0.693147               0                 0      0   \n",
      "6            14            0.693147               0                 0      1   \n",
      "8             0            0.693147               1                 0      0   \n",
      "9             1            5.733341               0                 1      1   \n",
      "\n",
      "   two_year_recid       los   custody     lasts  \n",
      "1               1  5.488938  5.484797  5.017280  \n",
      "2               1  3.295837  0.000000  4.158883  \n",
      "6               1  5.023881  6.070738  3.583519  \n",
      "8               0  4.262680  4.290459  6.614726  \n",
      "9               1  3.178054  3.218876  6.061457  \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "data = pd.read_csv('../data/compas-scores-two-years.csv')\n",
    "#print(data.shape)\n",
    "# filter out groups other than African-American and Caucasian and set them as 0-1\n",
    "data = data[(data['race']=='African-American') | (data['race']=='Caucasian')]\n",
    "data['race'].loc[data['race']=='Caucasian'] = 1\n",
    "data['race'].loc[data['race']=='African-American'] = 0\n",
    "#print(data.shape)\n",
    "\n",
    "nan = (data.isnull().sum()/len(data))\n",
    "nan = nan[nan > 0.15].sort_values()\n",
    "nan_var = list(nan.index)\n",
    "data = data.drop(columns=nan_var)\n",
    "\n",
    "data['c_jail_in'] = pd.to_datetime(data['c_jail_in'])\n",
    "data['c_jail_out'] = pd.to_datetime(data['c_jail_out'])\n",
    "data['los'] = np.log((data['c_jail_out']-data['c_jail_in']).astype('timedelta64[h]')+1)#use log hours\n",
    "data['in_custody'] = pd.to_datetime(data['in_custody'])\n",
    "data['out_custody'] = pd.to_datetime(data['out_custody'])\n",
    "data['custody'] = np.log((data['out_custody']-data['in_custody']).astype('timedelta64[h]')+1)\n",
    "data['lasts'] = np.log(data['end']-data['start']+1)\n",
    "data['c_days_from_compas'] = np.log(data['c_days_from_compas']+1)\n",
    "\n",
    "#filter out useless variables including high correlation and string type\n",
    "useless_var = ['id','name','first','last','compas_screening_date','dob','age_cat','days_b_screening_arrest',\n",
    "               'c_jail_in','c_jail_out','c_case_number','c_charge_desc','is_recid',\n",
    "               'type_of_assessment','screening_date','v_type_of_assessment',\n",
    "               'v_screening_date','in_custody','out_custody','score_text','v_score_text',\n",
    "               'decile_score.1','v_decile_score','priors_count.1','start','end']\n",
    "data = data.drop(columns=useless_var)\n",
    "data = data[data['los']!=float('-inf')]\n",
    "data = data[data['custody']!=float('-inf')]\n",
    "data = data[data['lasts']!=float('-inf')]\n",
    "\n",
    "#one hot encoding on several features:sex,age_cat,c_charge_degree\n",
    "data['sex'].loc[data['sex']=='Male']= 1\n",
    "data['sex'].loc[data['sex']=='Female']= 0\n",
    "data['c_charge_degree'].loc[data['c_charge_degree']=='M']= 1\n",
    "data['c_charge_degree'].loc[data['c_charge_degree']=='F']= 0\n",
    "#data.to_csv('./data/compas_preproc.csv',index=False,header=True)\n",
    "del nan_var, useless_var\n",
    "\n",
    "#data = data[['age','race','sex','decile_score','priors_count','los','c_charge_degree','two_year_recid']]\n",
    "data = data.dropna()#6150*23->5730*16\n",
    "#print(data.shape)\n",
    "print(data.head(5))\n",
    "\n",
    "X = data.drop(columns='two_year_recid')\n",
    "features = list(X.columns)\n",
    "\n",
    "X.index = range(data.shape[0])\n",
    "#As age, priors_count, los are continuous variables, we can scale them\n",
    "X_cont = X[['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts']]\n",
    "X_cate = X[['sex', 'race', 'c_charge_degree', 'is_violent_recid', 'event']]\n",
    "X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts'])\n",
    "#X_cont = X[['age', 'decile_score', 'priors_count', 'los']]\n",
    "#X_cate = X[['sex', 'race', 'c_charge_degree']]\n",
    "#X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'decile_score', 'priors_count', 'los'])\n",
    "\n",
    "X_df = pd.concat([X_cate,X_cont],axis=1)\n",
    "#X['decile_score'] = X['decile_score']/10\n",
    "y_df = data[\"two_year_recid\"]\n",
    "# convert class label 0 to -1 so as to add sign in distance\n",
    "#y[y==0] = -1\n",
    "features = list(X.columns)\n",
    "\n",
    "X = np.asarray(X_df).astype('float32')\n",
    "y = np.asarray(y_df).astype('float32')\n",
    "\n",
    "del X_cate,X_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5fa36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ################################################################################ \n",
      "                       Split up Train-Validation-Test sets  \n",
      " ################################################################################ \n",
      "\n",
      " X_train size:  (4130, 15) ,      y_train size:  (4130,) \n",
      " X_validation size:  (800, 15) ,  y_validation size:  (800,) \n",
      " X_test size:  (800, 15) ,        y_test size:  (800,)\n",
      " X_train_AA size:  (2472, 15) ,   X_train_C size:  (1658, 15) \n",
      " X_test_AA size:  (478, 15) ,     X_test_C size:  (322, 15) \n",
      " Ratio: 1.490952955367913 1.484472049689441\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#Use 5:1:1 as the ratio of train:val:test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=800, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=800,random_state=3)\n",
    "\n",
    "# For A6\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=800, random_state=3)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df,y_train_df,test_size=800,random_state=3)\n",
    "\n",
    "train_size = len(y_train)\n",
    "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
    "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
    "test_size = len(y_test)\n",
    "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
    "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
    "\n",
    "# For A6\n",
    "X_train_df.race = abs(X_train_df.race-1)\n",
    "X_test_df.race = abs(X_test_df.race-1)\n",
    "X_val_df.race = abs(X_val_df.race-1)\n",
    "X_train_df.race = abs(X_train_df.c_charge_degree-1)\n",
    "X_test_df.race = abs(X_test_df.c_charge_degree-1)\n",
    "X_val_df.race = abs(X_val_df.c_charge_degree-1)\n",
    "train = pd.concat([X_train_df.reset_index(drop=True), y_train_df.reset_index(drop=True)], axis = 1)\n",
    "test = pd.concat([X_test_df.reset_index(drop=True), y_test_df.reset_index(drop=True)], axis = 1)\n",
    "\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Validation-Test sets \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train size: \", X_train.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_validation size: \", X_val.shape, \",  y_validation size: \", y_val.shape, '\\n',\n",
    "      \"X_test size: \", X_test.shape, ',        y_test size: ',y_test.shape)\n",
    "print(\" X_train_AA size: \", X_train[train_idx_AA].shape, \",   X_train_C size: \", X_train[train_idx_C].shape, '\\n',\n",
    "      \"X_test_AA size: \", X_test[test_idx_AA].shape, ',     X_test_C size: ',X_test[test_idx_C].shape, '\\n',\n",
    "      \"Ratio:\", X_train[train_idx_AA].shape[0]/X_train[train_idx_C].shape[0],X_test[test_idx_AA].shape[0]/X_test[test_idx_C].shape[0])\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550ba68",
   "metadata": {},
   "source": [
    "# 2. A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411ca9",
   "metadata": {
    "id": "tFkhz8MEIQiC"
   },
   "source": [
    "## 2.1 Baseline Model And Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6c322d",
   "metadata": {
    "id": "Q1yehARm7H2Z"
   },
   "outputs": [],
   "source": [
    "#If use Neural Network as base model\n",
    "def base_nn_model(X_in,y_in,X_val,y_val):\n",
    "    feature = Input(X_in.shape[1],)\n",
    "    y = Dense(2,\"softmax\")(feature)\n",
    "    model = Model(feature,y)\n",
    "    \n",
    "    adam = tf.keras.optimizers.Adam(0.001)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metric = [tf.keras.metrics.BinaryAccuracy()]\n",
    "    #,tf.keras.metrics.FalsePositives()ï¼Œtf.keras.metrics.FalseNegatives()\n",
    "    model.compile(optimizer=adam, loss=loss, metrics=metric)\n",
    "    model.fit(X_in,tf.one_hot(y_in,2),epochs=10,batch_size=10,validation_data=(X_val,tf.one_hot(y_val,2)))\n",
    "    return model\n",
    "\n",
    "def evaluation(model,X,y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
    "    y_pred_AA, y_test_AA = y_pred[test_idx_AA], y[test_idx_AA]\n",
    "    y_pred_C, y_test_C = y_pred[test_idx_C], y[test_idx_C]\n",
    "    acc = model.evaluate(X, tf.one_hot(y,2))[1]\n",
    "    FPR_all = np.sum(y_pred[y==0]==1)/len(y[y==0])\n",
    "    FNR_all = np.sum(y_pred[y==1]==0)/len(y[y==1])\n",
    "    FPR_AA = np.sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
    "    FNR_AA = np.sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
    "    FPR_C = np.sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
    "    FNR_C = np.sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
    "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
    "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
    "    print('\\n',\"#\"*80)\n",
    "    print('The accuracy of baseline model NN is: {:.3f}.'.format(acc))\n",
    "    print('The False Positive Rate for overall population is: {:.3f}.'.format(FPR_all))\n",
    "    print('The False Negative Rate for overall population is:{:.3f}.'.format(FNR_all))\n",
    "    print(\"Specifically:\")\n",
    "    print('Parity Check: The rate of positive estimate for African American and Caucasian are {:.3f} and {:.3f}, and D_par={:.3f}'.format(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
    "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are {:.3f} and {:.3f}, and D_cal={:.3f}.'.format(acc_AA,acc_C,acc_AA-acc_C))\n",
    "    print('The False Positive Rate for African American and Caucasian are {:.3f} and {:.3f}, and D_FPR={:.3f}.'.format(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
    "    print('The False Negative Rate for African American and Caucasian are {:.3f} and {:.3f}, and D_FNR={:.3f}.'.format(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
    "    print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d90e29c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7C6kS-eRCs5",
    "outputId": "0a820a96-eefb-4c2c-bd8d-8d4b2bf4f7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "413/413 [==============================] - 0s 742us/step - loss: 0.5671 - binary_accuracy: 0.7588 - val_loss: 0.4870 - val_binary_accuracy: 0.8238\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 0s 566us/step - loss: 0.4429 - binary_accuracy: 0.8368 - val_loss: 0.4044 - val_binary_accuracy: 0.8512\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 0s 582us/step - loss: 0.3773 - binary_accuracy: 0.8676 - val_loss: 0.3533 - val_binary_accuracy: 0.8725\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 0s 546us/step - loss: 0.3343 - binary_accuracy: 0.8823 - val_loss: 0.3177 - val_binary_accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 0s 574us/step - loss: 0.3036 - binary_accuracy: 0.8944 - val_loss: 0.2920 - val_binary_accuracy: 0.8938\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 0s 551us/step - loss: 0.2806 - binary_accuracy: 0.9041 - val_loss: 0.2722 - val_binary_accuracy: 0.9025\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 0s 558us/step - loss: 0.2628 - binary_accuracy: 0.9087 - val_loss: 0.2563 - val_binary_accuracy: 0.9112\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 0s 563us/step - loss: 0.2485 - binary_accuracy: 0.9116 - val_loss: 0.2431 - val_binary_accuracy: 0.9112\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 0s 575us/step - loss: 0.2370 - binary_accuracy: 0.9150 - val_loss: 0.2320 - val_binary_accuracy: 0.9112\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 0s 585us/step - loss: 0.2274 - binary_accuracy: 0.9160 - val_loss: 0.2230 - val_binary_accuracy: 0.9125\n",
      "25/25 [==============================] - 0s 881us/step - loss: 0.2074 - binary_accuracy: 0.9262\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.926.\n",
      "The False Positive Rate for overall population is: 0.089.\n",
      "The False Negative Rate for overall population is:0.057.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.561 and 0.404, and D_par=0.157\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.927 and 0.925, and D_cal=0.001.\n",
      "The False Positive Rate for African American and Caucasian are 0.104 and 0.071, and D_FPR=0.033.\n",
      "The False Negative Rate for African American and Caucasian are 0.047 and 0.079, and D_FNR=-0.033.\n",
      "\n",
      " ################################################################################\n",
      "Epoch 1/10\n",
      "413/413 [==============================] - 0s 698us/step - loss: 0.6243 - binary_accuracy: 0.7668 - val_loss: 0.5167 - val_binary_accuracy: 0.8338\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 0s 582us/step - loss: 0.4584 - binary_accuracy: 0.8593 - val_loss: 0.4148 - val_binary_accuracy: 0.8662\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 0s 573us/step - loss: 0.3836 - binary_accuracy: 0.8794 - val_loss: 0.3577 - val_binary_accuracy: 0.8863\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 0s 554us/step - loss: 0.3371 - binary_accuracy: 0.8874 - val_loss: 0.3187 - val_binary_accuracy: 0.8988\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 0s 561us/step - loss: 0.3042 - binary_accuracy: 0.8993 - val_loss: 0.2905 - val_binary_accuracy: 0.9112\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 0s 586us/step - loss: 0.2798 - binary_accuracy: 0.9082 - val_loss: 0.2687 - val_binary_accuracy: 0.9100\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 0s 745us/step - loss: 0.2612 - binary_accuracy: 0.9119 - val_loss: 0.2519 - val_binary_accuracy: 0.9137\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.2466 - binary_accuracy: 0.9128 - val_loss: 0.2385 - val_binary_accuracy: 0.9150\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 0s 585us/step - loss: 0.2349 - binary_accuracy: 0.9157 - val_loss: 0.2275 - val_binary_accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 0s 601us/step - loss: 0.2255 - binary_accuracy: 0.9174 - val_loss: 0.2189 - val_binary_accuracy: 0.9187\n",
      "25/25 [==============================] - 0s 938us/step - loss: 0.2068 - binary_accuracy: 0.9325\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.933.\n",
      "The False Positive Rate for overall population is: 0.079.\n",
      "The False Negative Rate for overall population is:0.055.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.559 and 0.398, and D_par=0.161\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.929 and 0.938, and D_cal=-0.009.\n",
      "The False Positive Rate for African American and Caucasian are 0.100 and 0.056, and D_FPR=0.043.\n",
      "The False Negative Rate for African American and Caucasian are 0.047 and 0.071, and D_FNR=-0.025.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#If we don't drop 'race' in the X_train and X_test:\n",
    "NN1 = base_nn_model(X_train,y_train,X_val,y_val)\n",
    "evaluation(NN1,X_test,y_test)\n",
    "#If we drop 'race' in the X_train and X_test:\n",
    "NN2 = base_nn_model(np.delete(X_train,0,1),y_train,np.delete(X_val,0,1),y_val)\n",
    "evaluation(NN2,np.delete(X_test,1,1),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bc9a8",
   "metadata": {
    "id": "EXWOqulX9WSV"
   },
   "source": [
    "Then we custom constraint and loss to the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b305b064",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJz_2QB2NvLn",
    "outputId": "af626719-84b7-4d4f-e4fb-e53300de600e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03267247236118831 0.03264382676147383\n"
     ]
    }
   ],
   "source": [
    "def DFR(model,X,y,type):\n",
    "    '''\n",
    "    type: str in ['dfnr','dfpr','both']\n",
    "    '''\n",
    "    if type!='dfnr' and type!='dfpr' and type!='both':\n",
    "        return None\n",
    "    size = len(y)\n",
    "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
    "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
    "    y_pred_AA, y_AA = y_pred[idx_AA], y[idx_AA]\n",
    "    y_pred_C, y_C = y_pred[idx_C], y[idx_C] \n",
    "    FPR_AA =np.sum(y_pred_AA[y_AA==0]==1)/len(y_AA[y_AA==0])\n",
    "    FNR_AA = np.sum(y_pred_AA[y_AA==1]==0)/len(y_AA[y_AA==1])\n",
    "    FPR_C = np.sum(y_pred_C[y_C==0]==1)/len(y_C[y_C==0])\n",
    "    FNR_C = np.sum(y_pred_C[y_C==1]==0)/len(y_C[y_C==1])\n",
    "    dfnr = FNR_AA-FNR_C\n",
    "    dfpr = FPR_AA-FPR_C\n",
    "    if type=='dfnr':\n",
    "        return dfnr\n",
    "    elif type=='dfpr':\n",
    "        return dfpr\n",
    "    else:\n",
    "        return dfnr,dfpr\n",
    "dfnr,dfpr = DFR(NN1,X_test,y_test,'both')\n",
    "print(dfnr,dfpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38089198",
   "metadata": {
    "id": "wqkxAj9pDcbY"
   },
   "outputs": [],
   "source": [
    "def new_training_groups(model, X_train, y_train):\n",
    "    '''\n",
    "    X       n*d\n",
    "    model.predict(X)    n*2\n",
    "    y       n,\n",
    "    delta   c,\n",
    "    dfr   \"dfpr\",\"dfnr\",\"both\"\n",
    "    '''\n",
    "    \n",
    "    #split training sets according to sensitive variable\n",
    "    X_train_AA = X_train[np.array(X_train[:,1] == 0.0)]\n",
    "    y_train_AA = y_train[np.array(X_train[:,1] == 0.0)]\n",
    "    X_train_C = X_train[np.array(X_train[:,1] == 1.0)]\n",
    "    y_train_C = y_train[np.array(X_train[:,1] == 1.0)]\n",
    "    #get the ones with wrong prediction in discriminated group\n",
    "    dn,dp = DFR(model,X_train,y_train,type=\"both\")\n",
    "    if dp > 0: d = 0\n",
    "    else: d = 1\n",
    "\n",
    "    if d == 0:\n",
    "        #take penalized trainers\n",
    "        y_pred_AA = np.argmax(model.predict(X_train_AA),axis = 1)\n",
    "        y_diff_AA = y_train_AA-y_pred_AA\n",
    "        X_train_penalized = X_train_AA[y_diff_AA != 0.0]\n",
    "        y_train_penalized = y_train_AA[y_diff_AA != 0.0]\n",
    "        # safe trainers\n",
    "        X_train_clean = X_train_AA[y_diff_AA == 0.0]\n",
    "        y_train_clean = y_train_AA[y_diff_AA == 0.0]\n",
    "        #make new\n",
    "        X_train_clean = np.concatenate((X_train_clean,X_train_C),axis=0)\n",
    "        y_train_clean = np.concatenate((y_train_clean,y_train_C),axis=0)\n",
    "    \n",
    "    else:\n",
    "        #reverse the steps above for train set 1\n",
    "        y_pred_C = np.argmax(model.predict(X_train_C),axis = 1)\n",
    "        y_diff_C = y_train_C-y_pred_C\n",
    "        X_train_penalized = X_train_C[y_diff_C != 0.0]\n",
    "        y_train_penalized = y_train_C[y_diff_C != 0.0]\n",
    "        # safe trainers\n",
    "        X_train_clean = X_train_C[y_diff_C == 0.0]\n",
    "        y_train_clean = y_train_C[y_diff_C == 0.0]\n",
    "        #make new\n",
    "        X_train_clean = np.concatenate((X_train_clean,X_train_AA),axis=0)\n",
    "        y_train_clean = np.concatenate((y_train_clean,y_train_AA),axis=0)\n",
    "    \n",
    "    #X_train_penalized = tf.convert_to_tensor(X_train_penalized, dtype=tf.float32)\n",
    "    #X_train_safe = tf.convert_to_tensor(X_train_safe, dtype=tf.float32)\n",
    "    return X_train_clean , y_train_clean , X_train_penalized , y_train_penalized, dn, dp.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3118e0",
   "metadata": {
    "id": "ge3I04qX2yJl"
   },
   "source": [
    "Here we use Neural network as the base model, the result is unlikely to be reimplemented, but similar result will come out.\n",
    "\n",
    "**Note: When few features are used in the model, the model and its prediction are unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f0ad023",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUALnfmSLCwE",
    "outputId": "1a41b687-adf0-41c1-e8cf-3f3d72bd690b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "413/413 [==============================] - 0s 693us/step - loss: 0.7227 - binary_accuracy: 0.6024 - val_loss: 0.5511 - val_binary_accuracy: 0.7788\n",
      "Epoch 2/10\n",
      "413/413 [==============================] - 0s 592us/step - loss: 0.5017 - binary_accuracy: 0.8000 - val_loss: 0.4316 - val_binary_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "413/413 [==============================] - 0s 549us/step - loss: 0.4099 - binary_accuracy: 0.8467 - val_loss: 0.3694 - val_binary_accuracy: 0.8675\n",
      "Epoch 4/10\n",
      "413/413 [==============================] - 0s 552us/step - loss: 0.3558 - binary_accuracy: 0.8700 - val_loss: 0.3284 - val_binary_accuracy: 0.8875\n",
      "Epoch 5/10\n",
      "413/413 [==============================] - 0s 551us/step - loss: 0.3187 - binary_accuracy: 0.8867 - val_loss: 0.2987 - val_binary_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "413/413 [==============================] - 0s 613us/step - loss: 0.2914 - binary_accuracy: 0.8969 - val_loss: 0.2763 - val_binary_accuracy: 0.9050\n",
      "Epoch 7/10\n",
      "413/413 [==============================] - 0s 607us/step - loss: 0.2706 - binary_accuracy: 0.9036 - val_loss: 0.2587 - val_binary_accuracy: 0.9050\n",
      "Epoch 8/10\n",
      "413/413 [==============================] - 0s 576us/step - loss: 0.2543 - binary_accuracy: 0.9109 - val_loss: 0.2449 - val_binary_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "413/413 [==============================] - 0s 577us/step - loss: 0.2414 - binary_accuracy: 0.9157 - val_loss: 0.2334 - val_binary_accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "413/413 [==============================] - 0s 604us/step - loss: 0.2311 - binary_accuracy: 0.9184 - val_loss: 0.2244 - val_binary_accuracy: 0.9125\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.0150 - binary_accuracy: 0.4584 - val_loss: 0.9195 - val_binary_accuracy: 0.3750\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 721us/step - loss: 0.8523 - binary_accuracy: 0.2804 - val_loss: 0.7927 - val_binary_accuracy: 0.2525\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 692us/step - loss: 0.7495 - binary_accuracy: 0.2017 - val_loss: 0.7109 - val_binary_accuracy: 0.2037\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 734us/step - loss: 0.6821 - binary_accuracy: 0.1780 - val_loss: 0.6559 - val_binary_accuracy: 0.1863\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 698us/step - loss: 0.6356 - binary_accuracy: 0.1678 - val_loss: 0.6168 - val_binary_accuracy: 0.1713\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 719us/step - loss: 0.6017 - binary_accuracy: 0.1644 - val_loss: 0.5875 - val_binary_accuracy: 0.1713\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 836us/step - loss: 0.5756 - binary_accuracy: 0.1625 - val_loss: 0.5643 - val_binary_accuracy: 0.1688\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 735us/step - loss: 0.5545 - binary_accuracy: 0.1586 - val_loss: 0.5451 - val_binary_accuracy: 0.1600\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 715us/step - loss: 0.5368 - binary_accuracy: 0.1530 - val_loss: 0.5286 - val_binary_accuracy: 0.1538\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 771us/step - loss: 0.5213 - binary_accuracy: 0.1465 - val_loss: 0.5141 - val_binary_accuracy: 0.1513\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.8421 - binary_accuracy: 0.2306 - val_loss: 1.4211 - val_binary_accuracy: 0.3700\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 820us/step - loss: 1.2311 - binary_accuracy: 0.4644 - val_loss: 1.0808 - val_binary_accuracy: 0.5100\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 932us/step - loss: 0.9849 - binary_accuracy: 0.5729 - val_loss: 0.9011 - val_binary_accuracy: 0.5900\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 853us/step - loss: 0.8390 - binary_accuracy: 0.6385 - val_loss: 0.7818 - val_binary_accuracy: 0.6475\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 844us/step - loss: 0.7360 - binary_accuracy: 0.6828 - val_loss: 0.6926 - val_binary_accuracy: 0.6888\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 799us/step - loss: 0.6563 - binary_accuracy: 0.7150 - val_loss: 0.6214 - val_binary_accuracy: 0.7275\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 832us/step - loss: 0.5917 - binary_accuracy: 0.7453 - val_loss: 0.5628 - val_binary_accuracy: 0.7462\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 775us/step - loss: 0.5378 - binary_accuracy: 0.7671 - val_loss: 0.5134 - val_binary_accuracy: 0.7638\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 764us/step - loss: 0.4920 - binary_accuracy: 0.7850 - val_loss: 0.4711 - val_binary_accuracy: 0.7800\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 767us/step - loss: 0.4527 - binary_accuracy: 0.8000 - val_loss: 0.4346 - val_binary_accuracy: 0.8012\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.4025 - binary_accuracy: 0.8517 - val_loss: 0.9445 - val_binary_accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 774us/step - loss: 0.7835 - binary_accuracy: 0.8409 - val_loss: 0.6642 - val_binary_accuracy: 0.7713\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 782us/step - loss: 0.5951 - binary_accuracy: 0.7063 - val_loss: 0.5371 - val_binary_accuracy: 0.6500\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 709us/step - loss: 0.4971 - binary_accuracy: 0.6155 - val_loss: 0.4614 - val_binary_accuracy: 0.5838\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 713us/step - loss: 0.4346 - binary_accuracy: 0.5593 - val_loss: 0.4098 - val_binary_accuracy: 0.5525\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 738us/step - loss: 0.3902 - binary_accuracy: 0.5237 - val_loss: 0.3717 - val_binary_accuracy: 0.5350\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 717us/step - loss: 0.3564 - binary_accuracy: 0.5039 - val_loss: 0.3419 - val_binary_accuracy: 0.5075\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 728us/step - loss: 0.3295 - binary_accuracy: 0.4857 - val_loss: 0.3177 - val_binary_accuracy: 0.4950\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 792us/step - loss: 0.3074 - binary_accuracy: 0.4734 - val_loss: 0.2975 - val_binary_accuracy: 0.4863\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 755us/step - loss: 0.2888 - binary_accuracy: 0.4627 - val_loss: 0.2802 - val_binary_accuracy: 0.4800\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.9788 - binary_accuracy: 0.5992 - val_loss: 1.0765 - val_binary_accuracy: 0.7487\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 0.8311 - binary_accuracy: 0.7671 - val_loss: 0.6563 - val_binary_accuracy: 0.8075\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 890us/step - loss: 0.5617 - binary_accuracy: 0.8136 - val_loss: 0.4852 - val_binary_accuracy: 0.8487\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 0.4364 - binary_accuracy: 0.8462 - val_loss: 0.3944 - val_binary_accuracy: 0.8687\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 995us/step - loss: 0.3643 - binary_accuracy: 0.8605 - val_loss: 0.3373 - val_binary_accuracy: 0.8763\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 850us/step - loss: 0.3167 - binary_accuracy: 0.8656 - val_loss: 0.2976 - val_binary_accuracy: 0.8775\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 835us/step - loss: 0.2824 - binary_accuracy: 0.8649 - val_loss: 0.2681 - val_binary_accuracy: 0.8863\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 751us/step - loss: 0.2563 - binary_accuracy: 0.8639 - val_loss: 0.2451 - val_binary_accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 782us/step - loss: 0.2356 - binary_accuracy: 0.8630 - val_loss: 0.2265 - val_binary_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 889us/step - loss: 0.2187 - binary_accuracy: 0.8620 - val_loss: 0.2111 - val_binary_accuracy: 0.8725\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.8851 - binary_accuracy: 0.8945 - val_loss: 1.3941 - val_binary_accuracy: 0.9212\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 705us/step - loss: 1.2245 - binary_accuracy: 0.9107 - val_loss: 1.0975 - val_binary_accuracy: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 688us/step - loss: 1.0210 - binary_accuracy: 0.8864 - val_loss: 0.9553 - val_binary_accuracy: 0.8687\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 687us/step - loss: 0.9081 - binary_accuracy: 0.8363 - val_loss: 0.8654 - val_binary_accuracy: 0.7987\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 678us/step - loss: 0.8324 - binary_accuracy: 0.7354 - val_loss: 0.8017 - val_binary_accuracy: 0.6725\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 672us/step - loss: 0.7769 - binary_accuracy: 0.6424 - val_loss: 0.7534 - val_binary_accuracy: 0.6237\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 677us/step - loss: 0.7338 - binary_accuracy: 0.5969 - val_loss: 0.7150 - val_binary_accuracy: 0.5925\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 737us/step - loss: 0.6989 - binary_accuracy: 0.5646 - val_loss: 0.6833 - val_binary_accuracy: 0.5713\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 726us/step - loss: 0.6698 - binary_accuracy: 0.5404 - val_loss: 0.6565 - val_binary_accuracy: 0.5362\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 690us/step - loss: 0.6447 - binary_accuracy: 0.5201 - val_loss: 0.6331 - val_binary_accuracy: 0.5150\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 1.9298 - binary_accuracy: 0.6899 - val_loss: 0.8961 - val_binary_accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 753us/step - loss: 0.6775 - binary_accuracy: 0.8852 - val_loss: 0.5330 - val_binary_accuracy: 0.9125\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 733us/step - loss: 0.4614 - binary_accuracy: 0.8884 - val_loss: 0.4044 - val_binary_accuracy: 0.8963\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 750us/step - loss: 0.3692 - binary_accuracy: 0.8816 - val_loss: 0.3393 - val_binary_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 812us/step - loss: 0.3185 - binary_accuracy: 0.8748 - val_loss: 0.2999 - val_binary_accuracy: 0.8863\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 789us/step - loss: 0.2857 - binary_accuracy: 0.8671 - val_loss: 0.2724 - val_binary_accuracy: 0.8712\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 755us/step - loss: 0.2617 - binary_accuracy: 0.8598 - val_loss: 0.2516 - val_binary_accuracy: 0.8612\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 739us/step - loss: 0.2430 - binary_accuracy: 0.8559 - val_loss: 0.2348 - val_binary_accuracy: 0.8562\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 777us/step - loss: 0.2277 - binary_accuracy: 0.8499 - val_loss: 0.2208 - val_binary_accuracy: 0.8525\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 768us/step - loss: 0.2148 - binary_accuracy: 0.8433 - val_loss: 0.2089 - val_binary_accuracy: 0.8487\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 2.4034 - binary_accuracy: 0.8753 - val_loss: 1.6268 - val_binary_accuracy: 0.9162\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 738us/step - loss: 1.3638 - binary_accuracy: 0.8901 - val_loss: 1.1738 - val_binary_accuracy: 0.9025\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 706us/step - loss: 1.0683 - binary_accuracy: 0.8775 - val_loss: 0.9809 - val_binary_accuracy: 0.8775\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 698us/step - loss: 0.9221 - binary_accuracy: 0.8448 - val_loss: 0.8701 - val_binary_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 700us/step - loss: 0.8313 - binary_accuracy: 0.8034 - val_loss: 0.7957 - val_binary_accuracy: 0.7650\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 769us/step - loss: 0.7674 - binary_accuracy: 0.7327 - val_loss: 0.7407 - val_binary_accuracy: 0.6787\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 685us/step - loss: 0.7186 - binary_accuracy: 0.6579 - val_loss: 0.6973 - val_binary_accuracy: 0.6162\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 712us/step - loss: 0.6792 - binary_accuracy: 0.6039 - val_loss: 0.6616 - val_binary_accuracy: 0.5713\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 769us/step - loss: 0.6462 - binary_accuracy: 0.5571 - val_loss: 0.6311 - val_binary_accuracy: 0.5263\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 771us/step - loss: 0.6176 - binary_accuracy: 0.5206 - val_loss: 0.6044 - val_binary_accuracy: 0.5025\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.0115 - binary_accuracy: 0.6704 - val_loss: 1.5360 - val_binary_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 784us/step - loss: 1.1204 - binary_accuracy: 0.8930 - val_loss: 0.8548 - val_binary_accuracy: 0.9225\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 767us/step - loss: 0.7305 - binary_accuracy: 0.9056 - val_loss: 0.6324 - val_binary_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 743us/step - loss: 0.5709 - binary_accuracy: 0.8988 - val_loss: 0.5178 - val_binary_accuracy: 0.9237\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 704us/step - loss: 0.4800 - binary_accuracy: 0.8949 - val_loss: 0.4459 - val_binary_accuracy: 0.9112\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 784us/step - loss: 0.4198 - binary_accuracy: 0.8860 - val_loss: 0.3956 - val_binary_accuracy: 0.8950\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 749us/step - loss: 0.3763 - binary_accuracy: 0.8814 - val_loss: 0.3580 - val_binary_accuracy: 0.8863\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 737us/step - loss: 0.3430 - binary_accuracy: 0.8746 - val_loss: 0.3286 - val_binary_accuracy: 0.8788\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 799us/step - loss: 0.3165 - binary_accuracy: 0.8661 - val_loss: 0.3048 - val_binary_accuracy: 0.8725\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 738us/step - loss: 0.2948 - binary_accuracy: 0.8610 - val_loss: 0.2850 - val_binary_accuracy: 0.8700\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.0957 - binary_accuracy: 0.8728 - val_loss: 2.1997 - val_binary_accuracy: 0.9200\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 711us/step - loss: 1.8564 - binary_accuracy: 0.8947 - val_loss: 1.6028 - val_binary_accuracy: 0.9038\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 723us/step - loss: 1.4569 - binary_accuracy: 0.8814 - val_loss: 1.3346 - val_binary_accuracy: 0.8825\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 698us/step - loss: 1.2510 - binary_accuracy: 0.8533 - val_loss: 1.1767 - val_binary_accuracy: 0.8350\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 729us/step - loss: 1.1210 - binary_accuracy: 0.8191 - val_loss: 1.0698 - val_binary_accuracy: 0.7850\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 782us/step - loss: 1.0292 - binary_accuracy: 0.7588 - val_loss: 0.9910 - val_binary_accuracy: 0.7125\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 709us/step - loss: 0.9594 - binary_accuracy: 0.6901 - val_loss: 0.9293 - val_binary_accuracy: 0.6450\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 672us/step - loss: 0.9037 - binary_accuracy: 0.6208 - val_loss: 0.8789 - val_binary_accuracy: 0.5925\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 706us/step - loss: 0.8574 - binary_accuracy: 0.5719 - val_loss: 0.8364 - val_binary_accuracy: 0.5450\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 731us/step - loss: 0.8179 - binary_accuracy: 0.5254 - val_loss: 0.7996 - val_binary_accuracy: 0.5138\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.3271 - binary_accuracy: 0.6710 - val_loss: 1.7362 - val_binary_accuracy: 0.8675\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 865us/step - loss: 1.2773 - binary_accuracy: 0.8898 - val_loss: 0.9805 - val_binary_accuracy: 0.9175\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 817us/step - loss: 0.8388 - binary_accuracy: 0.8993 - val_loss: 0.7265 - val_binary_accuracy: 0.9187\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 768us/step - loss: 0.6556 - binary_accuracy: 0.8932 - val_loss: 0.5945 - val_binary_accuracy: 0.9112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 935us/step - loss: 0.5508 - binary_accuracy: 0.8903 - val_loss: 0.5115 - val_binary_accuracy: 0.9025\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 742us/step - loss: 0.4814 - binary_accuracy: 0.8821 - val_loss: 0.4536 - val_binary_accuracy: 0.8938\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 766us/step - loss: 0.4314 - binary_accuracy: 0.8758 - val_loss: 0.4105 - val_binary_accuracy: 0.8863\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 756us/step - loss: 0.3932 - binary_accuracy: 0.8731 - val_loss: 0.3768 - val_binary_accuracy: 0.8763\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 777us/step - loss: 0.3629 - binary_accuracy: 0.8678 - val_loss: 0.3496 - val_binary_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 766us/step - loss: 0.3381 - binary_accuracy: 0.8613 - val_loss: 0.3270 - val_binary_accuracy: 0.8675\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.5330 - binary_accuracy: 0.8722 - val_loss: 2.5245 - val_binary_accuracy: 0.9137\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 691us/step - loss: 2.1370 - binary_accuracy: 0.8845 - val_loss: 1.8498 - val_binary_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 683us/step - loss: 1.6848 - binary_accuracy: 0.8768 - val_loss: 1.5468 - val_binary_accuracy: 0.8737\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 666us/step - loss: 1.4527 - binary_accuracy: 0.8506 - val_loss: 1.3693 - val_binary_accuracy: 0.8388\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 674us/step - loss: 1.3069 - binary_accuracy: 0.8194 - val_loss: 1.2495 - val_binary_accuracy: 0.7887\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 667us/step - loss: 1.2040 - binary_accuracy: 0.7671 - val_loss: 1.1611 - val_binary_accuracy: 0.7262\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 651us/step - loss: 1.1257 - binary_accuracy: 0.7053 - val_loss: 1.0918 - val_binary_accuracy: 0.6612\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 637us/step - loss: 1.0629 - binary_accuracy: 0.6460 - val_loss: 1.0350 - val_binary_accuracy: 0.6175\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 650us/step - loss: 1.0107 - binary_accuracy: 0.5896 - val_loss: 0.9869 - val_binary_accuracy: 0.5725\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 673us/step - loss: 0.9658 - binary_accuracy: 0.5455 - val_loss: 0.9451 - val_binary_accuracy: 0.5263\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.7385 - binary_accuracy: 0.6773 - val_loss: 1.9528 - val_binary_accuracy: 0.8763\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 730us/step - loss: 1.4491 - binary_accuracy: 0.8998 - val_loss: 1.1206 - val_binary_accuracy: 0.9212\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 703us/step - loss: 0.9623 - binary_accuracy: 0.9070 - val_loss: 0.8366 - val_binary_accuracy: 0.9212\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 711us/step - loss: 0.7571 - binary_accuracy: 0.9012 - val_loss: 0.6886 - val_binary_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 676us/step - loss: 0.6396 - binary_accuracy: 0.8964 - val_loss: 0.5956 - val_binary_accuracy: 0.9087\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 734us/step - loss: 0.5618 - binary_accuracy: 0.8872 - val_loss: 0.5306 - val_binary_accuracy: 0.9038\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 688us/step - loss: 0.5057 - binary_accuracy: 0.8811 - val_loss: 0.4822 - val_binary_accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 692us/step - loss: 0.4629 - binary_accuracy: 0.8726 - val_loss: 0.4444 - val_binary_accuracy: 0.8813\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 700us/step - loss: 0.4289 - binary_accuracy: 0.8666 - val_loss: 0.4139 - val_binary_accuracy: 0.8737\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 707us/step - loss: 0.4010 - binary_accuracy: 0.8625 - val_loss: 0.3885 - val_binary_accuracy: 0.8775\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 3.9201 - binary_accuracy: 0.8706 - val_loss: 2.8001 - val_binary_accuracy: 0.9075\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 668us/step - loss: 2.3822 - binary_accuracy: 0.8785 - val_loss: 2.0714 - val_binary_accuracy: 0.8900\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 690us/step - loss: 1.8918 - binary_accuracy: 0.8598 - val_loss: 1.7416 - val_binary_accuracy: 0.8562\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 654us/step - loss: 1.6390 - binary_accuracy: 0.8421 - val_loss: 1.5481 - val_binary_accuracy: 0.8238\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 690us/step - loss: 1.4800 - binary_accuracy: 0.8029 - val_loss: 1.4174 - val_binary_accuracy: 0.7875\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 639us/step - loss: 1.3678 - binary_accuracy: 0.7656 - val_loss: 1.3210 - val_binary_accuracy: 0.7387\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 661us/step - loss: 1.2824 - binary_accuracy: 0.7225 - val_loss: 1.2455 - val_binary_accuracy: 0.6825\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 668us/step - loss: 1.2141 - binary_accuracy: 0.6646 - val_loss: 1.1836 - val_binary_accuracy: 0.6250\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 676us/step - loss: 1.1572 - binary_accuracy: 0.6123 - val_loss: 1.1313 - val_binary_accuracy: 0.5825\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 645us/step - loss: 1.1085 - binary_accuracy: 0.5644 - val_loss: 1.0859 - val_binary_accuracy: 0.5500\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 4.0687 - binary_accuracy: 0.6972 - val_loss: 2.0713 - val_binary_accuracy: 0.8788\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 735us/step - loss: 1.5580 - binary_accuracy: 0.9029 - val_loss: 1.2178 - val_binary_accuracy: 0.9137\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 693us/step - loss: 1.0484 - binary_accuracy: 0.9143 - val_loss: 0.9128 - val_binary_accuracy: 0.9212\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 709us/step - loss: 0.8261 - binary_accuracy: 0.9056 - val_loss: 0.7512 - val_binary_accuracy: 0.9175\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 705us/step - loss: 0.6975 - binary_accuracy: 0.8961 - val_loss: 0.6491 - val_binary_accuracy: 0.9112\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 700us/step - loss: 0.6120 - binary_accuracy: 0.8889 - val_loss: 0.5776 - val_binary_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 748us/step - loss: 0.5501 - binary_accuracy: 0.8826 - val_loss: 0.5242 - val_binary_accuracy: 0.8938\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 694us/step - loss: 0.5029 - binary_accuracy: 0.8753 - val_loss: 0.4825 - val_binary_accuracy: 0.8863\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 720us/step - loss: 0.4653 - binary_accuracy: 0.8707 - val_loss: 0.4488 - val_binary_accuracy: 0.8788\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 715us/step - loss: 0.4346 - binary_accuracy: 0.8642 - val_loss: 0.4208 - val_binary_accuracy: 0.8750\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 4.3341 - binary_accuracy: 0.8807 - val_loss: 2.9963 - val_binary_accuracy: 0.8975\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 709us/step - loss: 2.5336 - binary_accuracy: 0.8680 - val_loss: 2.1955 - val_binary_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 679us/step - loss: 2.0038 - binary_accuracy: 0.8397 - val_loss: 1.8443 - val_binary_accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 665us/step - loss: 1.7363 - binary_accuracy: 0.8024 - val_loss: 1.6409 - val_binary_accuracy: 0.8012\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 667us/step - loss: 1.5701 - binary_accuracy: 0.7673 - val_loss: 1.5052 - val_binary_accuracy: 0.7563\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 715us/step - loss: 1.4541 - binary_accuracy: 0.7317 - val_loss: 1.4061 - val_binary_accuracy: 0.6988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 636us/step - loss: 1.3667 - binary_accuracy: 0.6971 - val_loss: 1.3291 - val_binary_accuracy: 0.6600\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 673us/step - loss: 1.2973 - binary_accuracy: 0.6586 - val_loss: 1.2666 - val_binary_accuracy: 0.6275\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 712us/step - loss: 1.2399 - binary_accuracy: 0.6109 - val_loss: 1.2139 - val_binary_accuracy: 0.5875\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 752us/step - loss: 1.1911 - binary_accuracy: 0.5719 - val_loss: 1.1686 - val_binary_accuracy: 0.5612\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 4.6383 - binary_accuracy: 0.6984 - val_loss: 2.2267 - val_binary_accuracy: 0.8600\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 719us/step - loss: 1.6483 - binary_accuracy: 0.8971 - val_loss: 1.2755 - val_binary_accuracy: 0.9137\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 725us/step - loss: 1.0963 - binary_accuracy: 0.9099 - val_loss: 0.9538 - val_binary_accuracy: 0.9150\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 934us/step - loss: 0.8639 - binary_accuracy: 0.9007 - val_loss: 0.7863 - val_binary_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 813us/step - loss: 0.7311 - binary_accuracy: 0.8937 - val_loss: 0.6814 - val_binary_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 791us/step - loss: 0.6435 - binary_accuracy: 0.8874 - val_loss: 0.6085 - val_binary_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 777us/step - loss: 0.5805 - binary_accuracy: 0.8831 - val_loss: 0.5543 - val_binary_accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 794us/step - loss: 0.5327 - binary_accuracy: 0.8777 - val_loss: 0.5120 - val_binary_accuracy: 0.8863\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 797us/step - loss: 0.4947 - binary_accuracy: 0.8714 - val_loss: 0.4780 - val_binary_accuracy: 0.8788\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 781us/step - loss: 0.4637 - binary_accuracy: 0.8661 - val_loss: 0.4499 - val_binary_accuracy: 0.8687\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 1s 4ms/step - loss: 4.7939 - binary_accuracy: 0.8779 - val_loss: 3.2050 - val_binary_accuracy: 0.9075\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 682us/step - loss: 2.6494 - binary_accuracy: 0.8734 - val_loss: 2.2478 - val_binary_accuracy: 0.8600\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 659us/step - loss: 2.0253 - binary_accuracy: 0.8298 - val_loss: 1.8420 - val_binary_accuracy: 0.8213\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 663us/step - loss: 1.7203 - binary_accuracy: 0.7787 - val_loss: 1.6138 - val_binary_accuracy: 0.7763\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 706us/step - loss: 1.5359 - binary_accuracy: 0.7380 - val_loss: 1.4652 - val_binary_accuracy: 0.7262\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 697us/step - loss: 1.4104 - binary_accuracy: 0.6988 - val_loss: 1.3593 - val_binary_accuracy: 0.6963\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 666us/step - loss: 1.3180 - binary_accuracy: 0.6678 - val_loss: 1.2790 - val_binary_accuracy: 0.6612\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 734us/step - loss: 1.2465 - binary_accuracy: 0.6390 - val_loss: 1.2153 - val_binary_accuracy: 0.6313\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 683us/step - loss: 1.1887 - binary_accuracy: 0.6119 - val_loss: 1.1630 - val_binary_accuracy: 0.6025\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 705us/step - loss: 1.1408 - binary_accuracy: 0.5787 - val_loss: 1.1191 - val_binary_accuracy: 0.5600\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 5.2757 - binary_accuracy: 0.6886 - val_loss: 2.5270 - val_binary_accuracy: 0.8413\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 807us/step - loss: 1.8533 - binary_accuracy: 0.8734 - val_loss: 1.4147 - val_binary_accuracy: 0.8988\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 705us/step - loss: 1.2035 - binary_accuracy: 0.9027 - val_loss: 1.0364 - val_binary_accuracy: 0.9125\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 705us/step - loss: 0.9316 - binary_accuracy: 0.9017 - val_loss: 0.8417 - val_binary_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 704us/step - loss: 0.7781 - binary_accuracy: 0.9002 - val_loss: 0.7211 - val_binary_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 714us/step - loss: 0.6778 - binary_accuracy: 0.8925 - val_loss: 0.6378 - val_binary_accuracy: 0.9075\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 687us/step - loss: 0.6061 - binary_accuracy: 0.8872 - val_loss: 0.5764 - val_binary_accuracy: 0.9038\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 806us/step - loss: 0.5520 - binary_accuracy: 0.8809 - val_loss: 0.5288 - val_binary_accuracy: 0.8950\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 770us/step - loss: 0.5093 - binary_accuracy: 0.8748 - val_loss: 0.4906 - val_binary_accuracy: 0.8863\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 697us/step - loss: 0.4746 - binary_accuracy: 0.8695 - val_loss: 0.4590 - val_binary_accuracy: 0.8813\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 5.1347 - binary_accuracy: 0.8890 - val_loss: 3.3615 - val_binary_accuracy: 0.9125\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 694us/step - loss: 2.7241 - binary_accuracy: 0.8855 - val_loss: 2.2582 - val_binary_accuracy: 0.8725\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 674us/step - loss: 1.9988 - binary_accuracy: 0.8303 - val_loss: 1.7859 - val_binary_accuracy: 0.8075\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 660us/step - loss: 1.6464 - binary_accuracy: 0.7697 - val_loss: 1.5253 - val_binary_accuracy: 0.7563\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 664us/step - loss: 1.4384 - binary_accuracy: 0.7186 - val_loss: 1.3602 - val_binary_accuracy: 0.7100\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 681us/step - loss: 1.3006 - binary_accuracy: 0.6770 - val_loss: 1.2456 - val_binary_accuracy: 0.6762\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 680us/step - loss: 1.2018 - binary_accuracy: 0.6431 - val_loss: 1.1606 - val_binary_accuracy: 0.6525\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 671us/step - loss: 1.1267 - binary_accuracy: 0.6177 - val_loss: 1.0944 - val_binary_accuracy: 0.6400\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 725us/step - loss: 1.0671 - binary_accuracy: 0.5985 - val_loss: 1.0407 - val_binary_accuracy: 0.6162\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 664us/step - loss: 1.0180 - binary_accuracy: 0.5780 - val_loss: 0.9959 - val_binary_accuracy: 0.5925\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 5.8412 - binary_accuracy: 0.6722 - val_loss: 2.9420 - val_binary_accuracy: 0.8112\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 0s 740us/step - loss: 2.1667 - binary_accuracy: 0.8482 - val_loss: 1.6434 - val_binary_accuracy: 0.8875\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 0s 735us/step - loss: 1.3815 - binary_accuracy: 0.8881 - val_loss: 1.1724 - val_binary_accuracy: 0.9125\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 0s 762us/step - loss: 1.0401 - binary_accuracy: 0.8971 - val_loss: 0.9263 - val_binary_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 0s 706us/step - loss: 0.8457 - binary_accuracy: 0.8949 - val_loss: 0.7734 - val_binary_accuracy: 0.9075\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 0s 718us/step - loss: 0.7186 - binary_accuracy: 0.8925 - val_loss: 0.6682 - val_binary_accuracy: 0.9050\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 0s 726us/step - loss: 0.6284 - binary_accuracy: 0.8927 - val_loss: 0.5912 - val_binary_accuracy: 0.9050\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 0s 725us/step - loss: 0.5608 - binary_accuracy: 0.8906 - val_loss: 0.5320 - val_binary_accuracy: 0.9013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "130/130 [==============================] - 0s 711us/step - loss: 0.5081 - binary_accuracy: 0.8874 - val_loss: 0.4851 - val_binary_accuracy: 0.8888\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 0s 719us/step - loss: 0.4657 - binary_accuracy: 0.8838 - val_loss: 0.4469 - val_binary_accuracy: 0.8838\n"
     ]
    }
   ],
   "source": [
    "#########################[DON'T RUN THIS CELL IF UNNECESSARY]#########################\n",
    "\n",
    "## initialization\n",
    "np.random.seed(7777) \n",
    "model =  base_nn_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "#initialized C and delta\n",
    "C = 1\n",
    "delta = 0.2\n",
    "\n",
    "# new training groups\n",
    "X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(model, X_train, y_train)\n",
    "feature = Input(X_train.shape[1],)\n",
    "y = Dense(2,\"softmax\")(feature)\n",
    "mod_loop = Model(feature,y)\n",
    "adam = tf.keras.optimizers.Adam(0.001)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "def penal_loss(y_true,y_pred):\n",
    "  return loss(tf.one_hot(y_tp,2), mod_loop(X_tp))\n",
    "\n",
    "def clean_loss(y_true,y_pred):  \n",
    "  return loss(tf.one_hot(y_ts,2), mod_loop(X_ts))\n",
    "\n",
    "count = 0\n",
    "# start while loop\n",
    "\n",
    "while (count == 0 or count % 2 == 1 or np.abs(dp) > 0.05) and count < 20: \n",
    "# for i in range(20):\n",
    "    # or examine dn, here count%2==1 used to control accuracy in case the accuracy is below 0.5 and one of the overall fpr/fnr will be close to 1\n",
    "    C = C+delta\n",
    "    #print('Count:%d'%count)\n",
    "    mod_loop.compile(optimizer=adam,loss=[penal_loss, clean_loss],loss_weights=[C,1],metrics=metric)\n",
    "    mod_loop.fit(X_train, tf.one_hot(y_train,2), epochs=10, validation_data=(X_val,tf.one_hot(y_val,2)))\n",
    "    X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(mod_loop, X_train, y_train)\n",
    "    #dp = DFR(model_in_loop,X_test,y_test,'dfpr')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04400f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e87de3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WStZGDfzefQk",
    "outputId": "90dce3ec-dab5-47b0-81c4-aee5cddb8648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.12309307640046939, 0.13161418413519255)\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.4469 - binary_accuracy: 0.9025\n",
      "\n",
      " ################################################################################\n",
      "The accuracy of baseline model NN is: 0.902.\n",
      "The False Positive Rate for overall population is: 0.146.\n",
      "The False Negative Rate for overall population is:0.044.\n",
      "Specifically:\n",
      "Parity Check: The rate of positive estimate for African American and Caucasian are 0.632 and 0.388, and D_par=0.244\n",
      "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.902 and 0.904, and D_cal=-0.002.\n",
      "The False Positive Rate for African American and Caucasian are 0.208 and 0.077, and D_FPR=0.132.\n",
      "The False Negative Rate for African American and Caucasian are 0.004 and 0.127, and D_FNR=-0.123.\n",
      "\n",
      " ################################################################################\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "print(DFR(mod_loop,X_test,y_test,type='both'))\n",
    "evaluation(mod_loop,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e556",
   "metadata": {
    "id": "5dLHriKnrKIr"
   },
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "####################### IF NEED TO MODIFY CODES, USE THIS CHUNK TO RUN ABOVE OR RUN BELOW ##########################################\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566da1d3",
   "metadata": {
    "id": "DxdmajarNgms"
   },
   "source": [
    "## Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment ($DM_{sen}$ & $DM$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd66fc6",
   "metadata": {
    "id": "idcynX6VBaSu"
   },
   "source": [
    "We first implement $DM_{sen}$ as we won't do anything to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "435a4ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQIJDMSy164k",
    "outputId": "3bc394f9-85bd-4515-ef1a-323ef15dbce0"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'AddExpression'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17752/3833001363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mX_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mX_test_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mX_val_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrace\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mX_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_charge_degree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_charge_degree\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mX_test_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_charge_degree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_charge_degree\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\atoms\\elementwise\\abs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Returns the elementwise absolute value of x.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     47\u001b[0m             )\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Convert raw values to Constants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mAtom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_to_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_from_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m             )\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Convert raw values to Constants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mAtom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_to_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_from_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py\u001b[0m in \u001b[0;36mcast_to_const\u001b[1;34m(expr)\u001b[0m\n\u001b[0;32m    505\u001b[0m                         \u001b[1;34m\"Combine Expressions using atoms such as bmat, hstack, and vstack.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                     )\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mexpr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcvxtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\expressions\\constants\\constant.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mintf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_INTF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconst_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_imag\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\cvxpy\\interface\\numpy_interface\\ndarray_interface.py\u001b[0m in \u001b[0;36mconst_to_matrix\u001b[1;34m(self, value, convert_scalars)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Return an identity matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#Use 4:1 as the ratio of train:test\n",
    "y = data.two_year_recid\n",
    "y = np.asarray(y).astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "train_size = len(y_train)\n",
    "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
    "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
    "test_size = len(y_test)\n",
    "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
    "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
    "\n",
    "\n",
    "# For A6\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.2, random_state=0)\n",
    "\n",
    "# For A6\n",
    "X_train_df.race = abs(X_train_df.race-1)\n",
    "X_test_df.race = abs(X_test_df.race-1)\n",
    "X_val_df.race = abs(X_val_df.race-1)\n",
    "X_train_df.c_charge_degree = abs(X_train_df.c_charge_degree-1)\n",
    "X_test_df.c_charge_degree = abs(X_test_df.c_charge_degree-1)\n",
    "X_val_df.c_charge_degree = abs(X_val_df.c_charge_degree-1)\n",
    "train = pd.concat([X_train_df.reset_index(drop=True), y_train_df.reset_index(drop=True)], axis = 1)\n",
    "test = pd.concat([X_test_df.reset_index(drop=True), y_test_df.reset_index(drop=True)], axis = 1)\n",
    "\n",
    "X_train1 = np.hstack((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train))\n",
    "X_train1_AA = X_train1[train_idx_AA]\n",
    "X_train1_C = X_train1[train_idx_C]\n",
    "y_train_AA = y_train[train_idx_AA]\n",
    "y_train_C = y_train[train_idx_C]\n",
    "X_test1 = np.hstack((np.ones(X_test.shape[0]).reshape(X_test.shape[0],1), X_test))\n",
    "X_test1_AA = X_test1[test_idx_AA]\n",
    "X_test1_C = X_test1[test_idx_C]\n",
    "y_test_AA = y_test[test_idx_AA]\n",
    "y_test_C = y_test[test_idx_C]\n",
    "\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train size: \", X_train1.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_test size: \", X_test1.shape, ',       y_test size: ',y_test.shape)\n",
    "print(\" X_train_AA size: \", X_train1[train_idx_AA].shape, \",   X_train_C size: \", X_train1[train_idx_C].shape, '\\n',\n",
    "      \"X_test_AA size: \", X_test1[test_idx_AA].shape, ',     X_test_C size: ',X_test1[test_idx_C].shape, '\\n',\n",
    "      \"Ratio:\", X_train1[train_idx_AA].shape[0]/X_train1[train_idx_C].shape[0],X_test1[test_idx_AA].shape[0]/X_test1[test_idx_C].shape[0])\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ad59b",
   "metadata": {
    "id": "cLJDK5Ys0M3c"
   },
   "source": [
    "### Custom Loss: Add Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e115ad8",
   "metadata": {
    "id": "raHMVMyr0Gsg"
   },
   "source": [
    "From the paper, loss function is modified in logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45efcf",
   "metadata": {
    "id": "o4W8A43KqgwG"
   },
   "outputs": [],
   "source": [
    "import dccp\n",
    "import cvxpy as cvx\n",
    "from cvxpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ae77d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O9LwGH3lvpz",
    "outputId": "7d84de9e-9219-465c-cb95-83e8bcec68df"
   },
   "outputs": [],
   "source": [
    "def lossfunc(X,theta,y_true):\n",
    "    # This function returns the log loss.\n",
    "    y_true= 2*y_true - 1 #{0,1}->{-1,1}\n",
    "    log_loss = sum(logistic(multiply(-y_true, X*theta)))\n",
    "    return log_loss\n",
    "\n",
    "\n",
    "np.random.seed(5243)\n",
    "theta = cvx.Variable(X_train1.shape[1])\n",
    "theta.value = np.random.rand(theta.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.005, 1.5, 1 \n",
    "Prob1 = cvx.Problem(Minimize(lossfunc(X_train1,theta,y_train)),[]) # No constraints\n",
    "             \n",
    "      \n",
    "print(dccp.is_dccp(Prob1))\n",
    "print(theta.value)\n",
    "#[0.5591043  0.9994264  0.57031546 0.43833912 0.08453454 0.05043884\n",
    "# 0.91119515 0.16423428 0.3034639  0.41950956 0.85237613 0.4244003\n",
    "# 0.96147514 0.26277008 0.02849745 0.61075812]\n",
    "result = Prob1.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True) #Here changes the theta.value, result avoids the output\n",
    "print(theta.value)\n",
    "#[-2.22198860e+00  1.75116119e-01  1.31373607e-01 -2.60536345e-01 1.53575511e+00  4.91247336e+00 \n",
    "# -2.60491152e-01  4.36237090e-03  4.13966295e-01  7.39736815e-02  1.01397507e-01  2.79119037e-01\n",
    "# -6.37340155e-02  3.96745359e-02  6.88413345e-02 -2.27307636e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06055b78",
   "metadata": {
    "id": "tPl_LD06m2BN"
   },
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    #y:{-1,1}->{0,1}\n",
    "    d = np.dot(X,theta)\n",
    "    y_pred = (np.sign(d) + 1)/2\n",
    "    return y_pred\n",
    "\n",
    "theta_star = theta.value\n",
    "y_pred = predict(X_test1, theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37233b36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpNPZC-3nYhS",
    "outputId": "1b8016e0-67ea-4732-eb81-033318a21537"
   },
   "outputs": [],
   "source": [
    "def evaluation_DM(X,y,y_pred):\n",
    "    size = X.shape[0]\n",
    "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
    "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
    "    y_pred_AA, y_test_AA = y_pred[idx_AA], y[idx_AA]\n",
    "    y_pred_C, y_test_C = y_pred[idx_C], y[idx_C]\n",
    "    FPR_all = np.sum(y_pred[y==0]==1)/len(y[y==0])\n",
    "    FNR_all = np.sum(y_pred[y==1]==0)/len(y[y==1])\n",
    "    FPR_AA = np.sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
    "    FNR_AA = np.sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
    "    FPR_C = np.sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
    "    FNR_C = np.sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
    "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
    "    acc = np.sum(y_pred == y)/len(y)\n",
    "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
    "    print('\\n',\"#\"*80)\n",
    "    print('The accuracy of baseline model LR is: %3f.'%(acc))\n",
    "    print('The False Positive Rate for overall population is: %3f.'%FPR_all)\n",
    "    print('The False Negative Rate for overall population is: %3f.'%FNR_all)\n",
    "    print(\"Specifically:\")\n",
    "    print('Parity Check: The rate of positive estimate for African American and Caucasian are %3f and %3f, and D_par=%3f.'%(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
    "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are %3f and %3f, and D_cal=%3f.'%(acc_AA,acc_C,acc_AA-acc_C))\n",
    "    print('The False Positive Rate for African American and Caucasian are %3f and %3f, and D_FPR=%3f.'%(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
    "    print('The False Negative Rate for African American and Caucasian are %3f and %3f, and D_FNR=%3f.'%(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
    "    print('\\n',\"#\"*80)\n",
    "#print(y_pred.shape)\n",
    "evaluation_DM(X_test1,y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab475b",
   "metadata": {
    "id": "gIjSM7mcsOY5"
   },
   "source": [
    "If we do not put constaints on the loss function, the accuracy of the logistic regression model is around $93\\%$, while the FPR, FNR is around 0.05 to 0.1. \n",
    "\n",
    "Then, we put the constraint in the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113da8c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_9JaBPrsNhr",
    "outputId": "f7ff2a43-41c4-4a01-e0cb-976fd00e4ba8"
   },
   "outputs": [],
   "source": [
    "# Constaints on loss function\n",
    "np.random.seed(5243)\n",
    "theta1 = cvx.Variable(X_train1.shape[1])\n",
    "theta1.value = np.random.rand(theta.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
    "\n",
    "def g_theta(y,X,theta):\n",
    "    y = 2*y - 1\n",
    "    d = matmul(X,theta)\n",
    "    y_d = multiply(y,d)\n",
    "    return minimum(np.zeros_like(y_d),y_d)\n",
    "\n",
    "c = 0.05\n",
    "N0 = X_train1_AA.shape[0]\n",
    "N1 = X_train1_C.shape[0]\n",
    "N = X_train1.shape[0]\n",
    "print(N,N0,N1)\n",
    "\n",
    "\n",
    "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1,theta1,y_train)),\n",
    "                 [N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) <= c + N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)), \n",
    "                  N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) >= N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)) - c]) # With constraints\n",
    "print(dccp.is_dccp(Prob2))\n",
    "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)\n",
    "#g_theta(y_train,X_train1, theta.value).value.shape\n",
    "#constraint()\n",
    "#X_train.\n",
    "#pd.DataFrame(X_train)\n",
    "#pd.DataFrame(x_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a533c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1g7epK30aT2",
    "outputId": "a1a78ae3-d68d-4c6e-db4b-2e16915000cf"
   },
   "outputs": [],
   "source": [
    "y_pred1 = predict(X_test1, theta1.value)\n",
    "evaluation_DM(X_test1,y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff637f5",
   "metadata": {
    "id": "dFSvGNJHRban"
   },
   "source": [
    "From above result, we may see the $DM_{sen}$ algorithm slightly drops the $D_{FNR}$ to around -0.015, which is very close to $D_{FPR}$. \n",
    "\n",
    "Then we implement $DM$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59db0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdzBsm2yewlI",
    "outputId": "27a37aec-3f32-4deb-e25a-2aba5e54b077"
   },
   "outputs": [],
   "source": [
    "X_train1_sen = np.delete(X_train1,2,1)\n",
    "X_train1_AA_sen = X_train1_sen[train_idx_AA]\n",
    "X_train1_C_sen = X_train1_sen[train_idx_C]\n",
    "X_test1_sen = np.delete(X_test1,2,1)\n",
    "X_test1_AA_sen = X_test1_sen[test_idx_AA]\n",
    "X_test1_C_sen = X_test1_sen[test_idx_C]\n",
    "\n",
    "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
    "print(\" X_train1_sen size: \", X_train1_sen.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
    "      \"X_test1_sen size: \", X_test1_sen.shape, ',       y_test size: ',y_test.shape)\n",
    "print(\" X_train1_AA_sen size: \", X_train1_AA_sen.shape, \",   X_train1_C_sen size: \", X_train1_C_sen.shape, '\\n',\n",
    "      \"X_test1_AA_sen size: \", X_test1_AA_sen.shape, ',     X_test1_C_sen size: ',X_test1_C_sen.shape, '\\n',\n",
    "      \"Ratio:\", X_train1_AA_sen.shape[0]/X_train1_C_sen.shape[0],X_test1_AA_sen.shape[0]/X_test1_C_sen.shape[0])\n",
    "print('\\n',\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d3b11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4JivYlwa3Dr",
    "outputId": "2ad5082a-fb14-479d-f373-1c32967d94ec"
   },
   "outputs": [],
   "source": [
    "# Constaints on loss function\n",
    "np.random.seed(5243)\n",
    "theta2 = cvx.Variable(X_train1_sen.shape[1])\n",
    "theta2.value = np.random.rand(theta2.shape[0])\n",
    "\n",
    "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
    "c = 0.05\n",
    "N0 = X_train1_AA_sen.shape[0]\n",
    "N1 = X_train1_C_sen.shape[0]\n",
    "N = X_train1_sen.shape[0]\n",
    "print(N,N0,N1)\n",
    "\n",
    "\n",
    "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1_sen,theta2,y_train)),\n",
    "                 [N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) <= c + N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)), \n",
    "                  N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) >= N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)) - c]) # With constraints\n",
    "print(dccp.is_dccp(Prob2))\n",
    "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46551595",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0sU93opf_Pr",
    "outputId": "5d39a29e-c77e-4019-bd4b-41f089a0d396"
   },
   "outputs": [],
   "source": [
    "y_pred2 = predict(X_test1_sen, theta2.value)\n",
    "evaluation_DM(X_test1_sen,y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fcca76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEvRcabGJbl4",
    "outputId": "641e24af-b610-4605-a86d-b6175c1213d6"
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('The running time of overall algorithm is: %3fs.'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc1b171",
   "metadata": {
    "id": "PJzteiF3gNRP"
   },
   "source": [
    "We may see $DM$ algorithm drops $D_{FNR}$. but from the above results, it's hard for us to see which one is perfect and how $DM_{sen}$ violates the disparate treatment. Overall speaking, this algorithm has an impact on controlling the difference in FPR and FNR, but the effect deserves further study as when few features are in the model, both two algorithms seem to have no effect on controlling our target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70fcfb",
   "metadata": {},
   "source": [
    "# 3. A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2233387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ALL = all predictors, S = sensitive attributes, E = explanatory attribute, Y = response\n",
    "S = \"race\"\n",
    "E = \"c_charge_degree\"\n",
    "Y = \"two_year_recid\"\n",
    "\n",
    "X_ALL = ['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', \n",
    "         'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts',\n",
    "         'sex', 'race', 'c_charge_degree', 'is_violent_recid', 'event']\n",
    "\n",
    "X_train = train[X_ALL]\n",
    "y_train = train[Y]\n",
    "\n",
    "X_test = test[X_ALL]\n",
    "y_test = test[Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98865c8",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Model\n",
    "\n",
    "We use a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66df1c",
   "metadata": {},
   "source": [
    "## 3.2 A6 Algorithms\n",
    "\n",
    "A6 uses the following setup:\n",
    "\n",
    "- There is a sensitive attribute S. We want to avoid discrimination between different values of S. In our case, S is race, which takes values in {African American (1), Caucasian (0)}\n",
    "- There is an explanatory variable E, which is correlated with S. Hence, simply removing S won't fix bias. In our case, we assume it is c_charge_degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c38e0",
   "metadata": {},
   "source": [
    "### 3.2.1 Create functions used in pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of partitions: 1 for each unique value of e\n",
    "\n",
    "# X is the full dataset (in our case, train)\n",
    "# e is the \n",
    "def PARTITION(X):\n",
    "    partitions = list()\n",
    "    \n",
    "    for e_i in np.unique(X[E]):\n",
    "        partitions.append(X[X[E]==e_i])\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta function returns the number of observations (i.e. people) who are incorrectly classified \n",
    "# based on theoretical probabilities of reciding, calculated as the average rate of reciding\n",
    "# for each explanatory varaible (in our case, type of crime comittied, c_charge_degree)\n",
    "\n",
    "def DELTA(X, X_ei, s_i):\n",
    "    \n",
    "    # Gi is the number of observations for each race\n",
    "    # Don't we need to pass S as a parameter for this function?\n",
    "    Gi = sum(X_ei[S] == s_i)\n",
    "    \n",
    "    # X_ei_si is the dataset that contains the observations for each race\n",
    "    X_ei_si = X_ei[X_ei[S] == s_i]\n",
    "    \n",
    "    # P_denom is the number of people in group \n",
    "    # P_num is number of observations who recid\n",
    "    P_denom = X_ei_si.shape[0]\n",
    "    P_num = sum(X_ei_si[Y] == 1)\n",
    "    \n",
    "    # P is the probability of reciding for one race\n",
    "    # It is calculated by taking number of people who recid in each group \n",
    "    # dividied by total number of people in that group\n",
    "    P = P_num/P_denom\n",
    "    \n",
    "    # All other observations (for the other group)\n",
    "    X_ei_not_si = X_ei[X_ei[S] != s_i]\n",
    "    \n",
    "    # The probability of reciding for the other group (same calculation as above)\n",
    "    Ps_2 = sum(X_ei_not_si[Y] == 1)/X_ei_not_si.shape[0]\n",
    "    \n",
    "    # Ps is P*, which is the theoretical true probability of reciding\n",
    "    # Calculated by the average \n",
    "    Ps = (P+Ps_2)/2\n",
    "    \n",
    "    # Calcualte the number of incorrectly classified people\n",
    "    d = int(round(Gi * abs(P - Ps)))\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6a210",
   "metadata": {},
   "source": [
    "### 3.2.2 Local Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled_X_ei = list()\n",
    "\n",
    "for X_ei in PARTITION(train):\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    \n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    delta_afam = DELTA(train, X_ei, 1)\n",
    "    afam_predicted_1_index = [afam_index[v] for v in np.squeeze(np.where(ranker_model.predict(afam[X_ALL]) == 1))]\n",
    "    afam_predicted_1_index_Y1 = [i for (i,v) in zip(afam_predicted_1_index, X_ei.iloc[afam_predicted_1_index][Y]) if v==1]\n",
    "    afam_predicted_1 = X_ei.iloc[afam_predicted_1_index_Y1]\n",
    "    \n",
    "    afam_ranks = (ss.rankdata(ranker_model.decision_function(afam_predicted_1[X_ALL]))-1).astype(int)\n",
    "    afam_tochange = [i for (i, v) in zip(list(range(len(afam_ranks))), afam_ranks < delta_afam) if v]\n",
    "    afam_tochange_idx = [afam_predicted_1_index_Y1[v] for v in afam_tochange]\n",
    "    \n",
    "    cauca_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    cauca = X_ei[X_ei[S] == 0].copy()\n",
    "    delta_cauca = DELTA(train, X_ei, 0)\n",
    "    cauca_predicted_0_index = [cauca_index[v] for v in np.squeeze(np.where(ranker_model.predict(cauca[X_ALL]) == 0))]\n",
    "    cauca_predicted_0_index_Y0 = [i for (i,v) in zip(cauca_predicted_0_index, X_ei.iloc[cauca_predicted_0_index][Y]) if v==0]\n",
    "    cauca_predicted_0 = X_ei.iloc[cauca_predicted_0_index_Y0]\n",
    "    \n",
    "    cauca_ranks = (ss.rankdata(-ranker_model.decision_function(cauca_predicted_0[X_ALL]))-1).astype(int)\n",
    "    cauca_tochange = [i for (i, v) in zip(list(range(len(cauca_ranks))), cauca_ranks < delta_cauca) if v]\n",
    "    cauca_tochange_idx = [cauca_predicted_0_index_Y0[v] for v in cauca_tochange]\n",
    "    \n",
    "    for i in afam_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 0\n",
    "    for i in cauca_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 1\n",
    "    \n",
    "    relabeled_X_ei.append(X_ei_copy)\n",
    "    \n",
    "    print(\"DELTA(African American) = \", delta_afam, \"African Americans changed from 1 to 0\")\n",
    "    print(\"DELTA(Caucasian) = \", delta_cauca, \"Caucasians changed from 0 to 1\")\n",
    "    \n",
    "local_massaging = pd.concat(relabeled_X_ei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_X_train = local_massaging[X_ALL]\n",
    "lm_Y_train = local_massaging[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28214248",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "clf.score(X_test[X_ALL], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total number changed values should be sum of all DELTAs shown above\n",
    "res = [1 for i, j in zip(train.sort_index()[\"two_year_recid\"], pd.DataFrame(lm_Y_train).sort_index()[\"two_year_recid\"]) if i != j]\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293334c",
   "metadata": {},
   "source": [
    "### 3.2.3 Local Preferential Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7ba01",
   "metadata": {},
   "source": [
    "In this algorithm, we take in a dataset (train) and return a modified dataset of same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "recomp_train = pd.DataFrame()\n",
    "\n",
    "# for each partition (explanatory variable)\n",
    "for X_ei in PARTITION(train):\n",
    "    \n",
    "    print(\"start partition\")\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    print(\"X_ei shape:\", X_ei_copy.shape)\n",
    "    \n",
    "    # learn a ranker Hi : Xi -> Yi\n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    # Calculate half delta (AA: S_i = 1, AA: S_i = 0)\n",
    "    half_delta_afam = DELTA(train, X_ei, 1) // 2\n",
    "    half_delta_cauc = DELTA(train, X_ei, 0) // 2\n",
    "    print(\"Half Delta(AA):\", half_delta_afam)\n",
    "    print(\"Half Delta(Cauc):\", half_delta_cauc)\n",
    "    \n",
    "    # store indicies\n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    c_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    print(\"Total AAs:\", len(afam_index))\n",
    "    print(\"Total Cs:\", len(c_index))\n",
    "    \n",
    "    # get subset of data to work with\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    c = X_ei[X_ei[S] == 0].copy()\n",
    "    print(\"afam dataset shape:\", afam.shape)\n",
    "    print(\"c dataset shape:\", c.shape)\n",
    "    \n",
    "    # rank AA\n",
    "    afam.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(afam[X_ALL]), columns = ['rank'])\n",
    "    afam_with_rank = pd.concat([afam, rank], axis=1)\n",
    "    \n",
    "    # rank C\n",
    "    c.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(c[X_ALL]), columns = ['rank'])\n",
    "    c_with_rank = pd.concat([c, rank], axis=1)\n",
    "    \n",
    "    # sort values, reset indices\n",
    "    afam_with_rank = afam_with_rank.sort_values(['rank'])\n",
    "    afam_with_rank.reset_index(drop = True, inplace = True)\n",
    "    c_with_rank = c_with_rank.sort_values(['rank'])\n",
    "    c_with_rank.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ######## Modify AA data - find rows to delete/duplicate; decision boundary is 0 #####\n",
    "    recid = sum(afam_with_rank['rank'] > 0)\n",
    "    no_recid = sum(afam_with_rank['rank'] < 0)    \n",
    "    total = len(afam_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    # compas = compas[compas['days_b_screening_arrest'] >= -30]\n",
    "    cleaned_recid = afam_with_rank[afam_with_rank['rank'] > 0]\n",
    "    cleaned_no_recid = afam_with_rank[afam_with_rank['rank'] < 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    N = half_delta_afam\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:N], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:N])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_AA = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final A:\", total_AA.shape)\n",
    "    \n",
    "    ########## Modify C data ############\n",
    "    # Find rows to delete/duplicate; decision boundary is 0; opposite code as above\n",
    "    recid = sum(c_with_rank['rank'] < 0)\n",
    "    no_recid = sum(c_with_rank['rank'] > 0)    \n",
    "    total = len(c_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    cleaned_recid = c_with_rank[c_with_rank['rank'] < 0]\n",
    "    cleaned_no_recid = c_with_rank[c_with_rank['rank'] > 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    M = half_delta_cauc\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:M], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:M])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_C = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final C:\", total_C.shape)\n",
    "    print(\"end partition\")\n",
    "    \n",
    "    # combine both datasets\n",
    "    recomp_train = recomp_train.append(total_AA)\n",
    "    recomp_train = recomp_train.append(total_C)\n",
    "    recomp_train = recomp_train.drop('rank', axis=1)\n",
    "    \n",
    "    print(\"size of train:\", train.shape)\n",
    "    print(\"size of recomp:\", recomp_train.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af1693",
   "metadata": {},
   "source": [
    "# 3.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f78872",
   "metadata": {},
   "source": [
    "Notation: P_c stands for probability based on the classifier's predictions.\n",
    "\n",
    "## Metrics Used:\n",
    "\n",
    "### Parity or D_all\n",
    "\n",
    "Parity is defined as the difference is positive prediction rates in the two race groups. Paper 6 also calls this D_all, which stands for all discrimination. Fairness calls for Parity being close to 0.\n",
    "\n",
    "<br>\n",
    "<center>Parity = |P_c(recid = 1 | race = African American) - P_c(recid = 1 | race = Caucasian)</center>\n",
    "    \n",
    "### Calibration\n",
    "\n",
    "Calibration is defined as the difference in accuracies between the two race groups. Fairness calls for Calibration being close to 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Calibration = |P_c(recid predicted correctly | race = African American) - P_c(recid predicted correctly | race = Caucasian)</center>\n",
    "\n",
    "### Equality of Odds\n",
    "\n",
    "Equality of odds is achieved when the difference in positive prediction rates is equal for the two race groups. Fairness calls for the following value to be close to 0 for both y in {0,1}.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>D_Odds = P_c(recid.hat = 1 | race = African American, recid = y) - P_c(recid.hat = 1 | race = Caucasian, recid = y)</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1df946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X must include the sensitive feature\n",
    "def PARITY(X, Y_PRED):\n",
    "    s = X[S]\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    num_afam = sum(Y_PRED[X[S] == 1])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    num_cauca = sum(Y_PRED[X[S] == 0])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid = 1 | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid = 1 | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    parity = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Parity =\", parity)\n",
    "    \n",
    "    return(parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X must include S\n",
    "def CALIBRATION(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    Y_TRUE_afam = Y_TRUE[X[S] == 1]\n",
    "    num_afam = sum([1 for (i, v) in zip(Y_TRUE_afam, Y_PRED[X[S]==1]) if i == v])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    Y_TRUE_cauca = Y_TRUE[X[S] == 0]\n",
    "    num_cauca = sum([1 for (i, v) in zip(Y_TRUE_cauca, Y_PRED[X[S]==0]) if i == v])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid predicted correctly | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid predicted correctly | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    calibration = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Calibration =\", calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EQUALITY_OF_ODDS(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    # S = afam, Y = 0\n",
    "    X_afam_0 = X[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    Y_PRED_afam_0 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    num_afam_0 = sum([1 for i in Y_PRED_afam_0 if i == 1])\n",
    "    denom_afam_0 = X_afam_0.shape[0]\n",
    "    P_afam_0 = num_afam_0/denom_afam_0\n",
    "    \n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 1])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 0\n",
    "    X_cauca_0 = X[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    Y_PRED_cauca_0 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    num_cauca_0 = sum([1 for i in Y_PRED_cauca_0 if i == 1])\n",
    "    denom_cauca_0 = X_cauca_0.shape[0]\n",
    "    P_cauca_0 = num_cauca_0/denom_cauca_0\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 1])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"For recid = 0:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 0) = \", P_afam_0)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 0) = \", P_cauca_0)\n",
    "    print(\"Difference in odds of true recid = 0 is  = D_FPR =\", abs(P_afam_0 - P_cauca_0))\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    print(\"Difference in odds of true recid = 1 is = D_TPR =\", abs(P_afam_1 - P_cauca_1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_FNR(X, Y_TRUE, Y_PRED):\n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 0])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 0])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"Difference in False Negative Rates\")\n",
    "\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 0 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 0 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    \n",
    "    print(\"D_FNR =\", abs(P_afam_1 - P_cauca_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d7ba6",
   "metadata": {},
   "source": [
    "## 3.3.1 Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "baseline_pred = clf.predict(X_test[X_ALL])\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e632e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054313be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_test, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd68571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_test, y_test, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_test, y_test, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe55fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test, y_test, baseline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf18db8",
   "metadata": {},
   "source": [
    "## 3.3.2 Local Massaging Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783512f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "lm_pred = clf.predict(X_test[X_ALL])\n",
    "clf.score(X_test[X_ALL], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579430c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7655f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_test, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b010cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_test, y_test, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e35c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_test, y_test, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test, y_test, lm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8fe55",
   "metadata": {},
   "source": [
    "## 3.3.3 Local Preferential Sampling Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1751ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "recomp_X_train = recomp_train[X_ALL]\n",
    "recomp_Y_train = recomp_train[Y]\n",
    "print(\"size of recomp_X_train:\", recomp_X_train.shape)\n",
    "print(\"size of recomp_Y_train:\", recomp_Y_train.shape)\n",
    "\n",
    "clf_LPS = LogisticRegression(random_state=0).fit(recomp_X_train, recomp_Y_train)\n",
    "LPS_pred = clf_LPS.predict(X_test[X_ALL])\n",
    "clf_LPS.score(X_test[X_ALL], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f891f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf_LPS.predict(X_test[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARITY(X_test, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION(X_test, y_test, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "EQUALITY_OF_ODDS(X_test, y_test, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45970451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_test, y_test, LPS_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
