{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5243 Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Alogorithm 4 implementation"
      ],
      "metadata": {
        "id": "KveoZlUNJijZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import essential packages"
      ],
      "metadata": {
        "id": "tR5_702OVJAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "import time,sys\n",
        "import zipfile\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VV6K-AxuVIbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Database and Data Preprocessing\n"
      ],
      "metadata": {
        "id": "6m94DWsuCzcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdN6QSFk9pAR",
        "outputId": "e456346c-f84c-4ce8-db25-03a951696427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "# if using local data, this code chunk should be commented\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start=time.time()\n",
        "data = pd.read_csv('drive/MyDrive/5243Project4/compas-scores-two-years.csv')\n",
        "#print(data.shape)\n",
        "# filter out groups other than African-American and Caucasian and set them as 0-1\n",
        "data = data[(data['race']=='African-American') | (data['race']=='Caucasian')]\n",
        "data['race'].loc[data['race']=='Caucasian'] = 1\n",
        "data['race'].loc[data['race']=='African-American'] = 0\n",
        "#print(data.shape)\n",
        "\n",
        "nan = (data.isnull().sum()/len(data))\n",
        "nan = nan[nan > 0.15].sort_values()\n",
        "nan_var = list(nan.index)\n",
        "data = data.drop(columns=nan_var)\n",
        "\n",
        "data['c_jail_in'] = pd.to_datetime(data['c_jail_in'])\n",
        "data['c_jail_out'] = pd.to_datetime(data['c_jail_out'])\n",
        "data['los'] = np.log((data['c_jail_out']-data['c_jail_in']).astype('timedelta64[h]')+1)#use log hours\n",
        "data['in_custody'] = pd.to_datetime(data['in_custody'])\n",
        "data['out_custody'] = pd.to_datetime(data['out_custody'])\n",
        "data['custody'] = np.log((data['out_custody']-data['in_custody']).astype('timedelta64[h]')+1)\n",
        "data['lasts'] = np.log(data['end']-data['start']+1)\n",
        "data['c_days_from_compas'] = np.log(data['c_days_from_compas']+1)\n",
        "\n",
        "#filter out useless variables including high correlation and string type\n",
        "useless_var = ['id','name','first','last','compas_screening_date','dob','age_cat','days_b_screening_arrest',\n",
        "               'c_jail_in','c_jail_out','c_case_number','c_charge_desc','is_recid',\n",
        "               'type_of_assessment','screening_date','v_type_of_assessment',\n",
        "               'v_screening_date','in_custody','out_custody','score_text','v_score_text',\n",
        "               'decile_score.1','v_decile_score','priors_count.1','start','end']\n",
        "data = data.drop(columns=useless_var)\n",
        "data = data[data['los']!=float('-inf')]\n",
        "data = data[data['custody']!=float('-inf')]\n",
        "data = data[data['lasts']!=float('-inf')]\n",
        "\n",
        "#one hot encoding on several features:sex,age_cat,c_charge_degree\n",
        "data['sex'].loc[data['sex']=='Male']= 1\n",
        "data['sex'].loc[data['sex']=='Female']= 0\n",
        "data['c_charge_degree'].loc[data['c_charge_degree']=='M']= 1\n",
        "data['c_charge_degree'].loc[data['c_charge_degree']=='F']= 0\n",
        "#data.to_csv('./data/compas_preproc.csv',index=False,header=True)\n",
        "del nan_var, useless_var\n",
        "\n",
        "#data = data[['age','race','sex','decile_score','priors_count','los','c_charge_degree','two_year_recid']]\n",
        "data = data.dropna()#6150*23->5730*16\n",
        "#print(data.shape)\n",
        "print(data.head(5))\n",
        "\n",
        "X = data.drop(columns='two_year_recid')\n",
        "features = list(X.columns)\n",
        "\n",
        "X.index = range(data.shape[0])\n",
        "#As age, priors_count, los are continuous variables, we can scale them\n",
        "X_cont = X[['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts']]\n",
        "X_cate = X[['sex', 'race', 'c_charge_degree', 'is_violent_recid', 'event']]\n",
        "X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_days_from_compas', 'los', 'custody', 'lasts'])\n",
        "#X_cont = X[['age', 'decile_score', 'priors_count', 'los']]\n",
        "#X_cate = X[['sex', 'race', 'c_charge_degree']]\n",
        "#X_cont = pd.DataFrame(StandardScaler().fit_transform(X_cont),columns=['age', 'decile_score', 'priors_count', 'los'])\n",
        "\n",
        "X = pd.concat([X_cate,X_cont],axis=1)\n",
        "#X['decile_score'] = X['decile_score']/10\n",
        "y = data.two_year_recid\n",
        "# convert class label 0 to -1 so as to add sign in distance\n",
        "#y[y==0] = -1\n",
        "features = list(X.columns)\n",
        "\n",
        "X = np.asarray(X).astype('float32')\n",
        "y = np.asarray(y).astype('float32')\n",
        "\n",
        "del X_cate,X_cont"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ2bTdt5T3EM",
        "outputId": "af0a3f81-b375-4ef9-9c30-049a6402cec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  sex  age race  juv_fel_count  decile_score  juv_misd_count  juv_other_count  \\\n",
            "1   1   34    0              0             3               0                0   \n",
            "2   1   24    0              0             4               0                1   \n",
            "6   1   41    1              0             6               0                0   \n",
            "8   0   39    1              0             1               0                0   \n",
            "9   1   21    1              0             3               0                0   \n",
            "\n",
            "   priors_count  c_days_from_compas c_charge_degree  is_violent_recid  event  \\\n",
            "1             0            0.693147               0                 1      1   \n",
            "2             4            0.693147               0                 0      0   \n",
            "6            14            0.693147               0                 0      1   \n",
            "8             0            0.693147               1                 0      0   \n",
            "9             1            5.733341               0                 1      1   \n",
            "\n",
            "   two_year_recid       los   custody     lasts  \n",
            "1               1  5.488938  5.484797  5.017280  \n",
            "2               1  3.295837  0.000000  4.158883  \n",
            "6               1  5.023881  6.070738  3.583519  \n",
            "8               0  4.262680  4.290459  6.614726  \n",
            "9               1  3.178054  3.218876  6.061457  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting"
      ],
      "metadata": {
        "id": "2Hrp1SZPK9zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use 5:1:1 as the ratio of train:val:test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=800, random_state=3)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=800,random_state=3)\n",
        "train_size = len(y_train)\n",
        "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
        "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
        "test_size = len(y_test)\n",
        "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
        "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
        "\n",
        "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Validation-Test sets \",'\\n',\"#\"*80,'\\n')\n",
        "print(\" X_train size: \", X_train.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
        "      \"X_validation size: \", X_val.shape, \",  y_validation size: \", y_val.shape, '\\n',\n",
        "      \"X_test size: \", X_test.shape, ',        y_test size: ',y_test.shape)\n",
        "print(\" X_train_AA size: \", X_train[train_idx_AA].shape, \",   X_train_C size: \", X_train[train_idx_C].shape, '\\n',\n",
        "      \"X_test_AA size: \", X_test[test_idx_AA].shape, ',     X_test_C size: ',X_test[test_idx_C].shape, '\\n',\n",
        "      \"Ratio:\", X_train[train_idx_AA].shape[0]/X_train[train_idx_C].shape[0],X_test[test_idx_AA].shape[0]/X_test[test_idx_C].shape[0])\n",
        "print('\\n',\"#\"*80)"
      ],
      "metadata": {
        "id": "6YTcWq5U9-K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07960407-9af4-4b66-d810-5fa19679c1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################ \n",
            "                       Split up Train-Validation-Test sets  \n",
            " ################################################################################ \n",
            "\n",
            " X_train size:  (4130, 15) ,      y_train size:  (4130,) \n",
            " X_validation size:  (800, 15) ,  y_validation size:  (800,) \n",
            " X_test size:  (800, 15) ,        y_test size:  (800,)\n",
            " X_train_AA size:  (2472, 15) ,   X_train_C size:  (1658, 15) \n",
            " X_test_AA size:  (478, 15) ,     X_test_C size:  (322, 15) \n",
            " Ratio: 1.490952955367913 1.484472049689441\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model And Evaluation"
      ],
      "metadata": {
        "id": "tFkhz8MEIQiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If use Neural Network as base model\n",
        "def base_nn_model(X_in,y_in,X_val,y_val):\n",
        "    feature = Input(X_in.shape[1],)\n",
        "    y = Dense(2,\"softmax\")(feature)\n",
        "    model = Model(feature,y)\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(0.001)\n",
        "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metric = [tf.keras.metrics.BinaryAccuracy()]\n",
        "    #,tf.keras.metrics.FalsePositives()，tf.keras.metrics.FalseNegatives()\n",
        "    model.compile(optimizer=adam, loss=loss, metrics=metric)\n",
        "    model.fit(X_in,tf.one_hot(y_in,2),epochs=10,batch_size=10,validation_data=(X_val,tf.one_hot(y_val,2)))\n",
        "    return model\n",
        "\n",
        "def evaluation(model,X,y):\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
        "    y_pred_AA, y_test_AA = y_pred[test_idx_AA], y[test_idx_AA]\n",
        "    y_pred_C, y_test_C = y_pred[test_idx_C], y[test_idx_C]\n",
        "    acc = model.evaluate(X, tf.one_hot(y,2))[1]\n",
        "    FPR_all = sum(y_pred[y==0]==1)/len(y[y==0])\n",
        "    FNR_all = sum(y_pred[y==1]==0)/len(y[y==1])\n",
        "    FPR_AA = sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
        "    FNR_AA = sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
        "    FPR_C = sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
        "    FNR_C = sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
        "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
        "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
        "    print('\\n',\"#\"*80)\n",
        "    print('The accuracy of baseline model NN is: %3f.'%(acc))\n",
        "    print('The False Positive Rate for overall population is: %3f.'%FPR_all)\n",
        "    print('The False Negative Rate for overall population is: %3f.'%FNR_all)\n",
        "    print(\"Specifically:\")\n",
        "    print('Parity Check: The rate of positive estimate for African American and Caucasian are %3f and %3f, and D_par=%3f.'%(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
        "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are %3f and %3f, and D_cal=%3f.'%(acc_AA,acc_C,acc_AA-acc_C))\n",
        "    print('The False Positive Rate for African American and Caucasian are %3f and %3f, and D_FPR=%3f.'%(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
        "    print('The False Negative Rate for African American and Caucasian are %3f and %3f, and D_FNR=%3f.'%(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
        "    print('\\n',\"#\"*80)"
      ],
      "metadata": {
        "id": "Q1yehARm7H2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If we don't drop 'race' in the X_train and X_test:\n",
        "NN1 = base_nn_model(X_train,y_train,X_val,y_val)\n",
        "evaluation(NN1,X_test,y_test)\n",
        "#If we drop 'race' in the X_train and X_test:\n",
        "NN2 = base_nn_model(np.delete(X_train,0,1),y_train,np.delete(X_val,0,1),y_val)\n",
        "evaluation(NN2,np.delete(X_test,1,1),y_test)"
      ],
      "metadata": {
        "id": "w7C6kS-eRCs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a820a96-eefb-4c2c-bd8d-8d4b2bf4f7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.7808 - binary_accuracy: 0.5751 - val_loss: 0.6257 - val_binary_accuracy: 0.7262\n",
            "Epoch 2/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.5308 - binary_accuracy: 0.7847 - val_loss: 0.4755 - val_binary_accuracy: 0.8163\n",
            "Epoch 3/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.4260 - binary_accuracy: 0.8438 - val_loss: 0.3964 - val_binary_accuracy: 0.8612\n",
            "Epoch 4/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3647 - binary_accuracy: 0.8763 - val_loss: 0.3440 - val_binary_accuracy: 0.8712\n",
            "Epoch 5/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3229 - binary_accuracy: 0.8952 - val_loss: 0.3069 - val_binary_accuracy: 0.8938\n",
            "Epoch 6/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2927 - binary_accuracy: 0.9029 - val_loss: 0.2800 - val_binary_accuracy: 0.9038\n",
            "Epoch 7/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2703 - binary_accuracy: 0.9114 - val_loss: 0.2599 - val_binary_accuracy: 0.9150\n",
            "Epoch 8/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2533 - binary_accuracy: 0.9177 - val_loss: 0.2444 - val_binary_accuracy: 0.9162\n",
            "Epoch 9/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2400 - binary_accuracy: 0.9196 - val_loss: 0.2324 - val_binary_accuracy: 0.9137\n",
            "Epoch 10/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2294 - binary_accuracy: 0.9208 - val_loss: 0.2227 - val_binary_accuracy: 0.9125\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2114 - binary_accuracy: 0.9275\n",
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model NN is: 0.927500.\n",
            "The False Positive Rate for overall population is: 0.079137.\n",
            "The False Negative Rate for overall population is: 0.065274.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.562762 and 0.378882, and D_par=0.183880.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.928870 and 0.925466, and D_cal=0.003404.\n",
            "The False Positive Rate for African American and Caucasian are 0.104072 and 0.051020, and D_FPR=0.053052.\n",
            "The False Negative Rate for African American and Caucasian are 0.042802 and 0.111111, and D_FNR=-0.068310.\n",
            "\n",
            " ################################################################################\n",
            "Epoch 1/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.7099 - binary_accuracy: 0.6201 - val_loss: 0.5844 - val_binary_accuracy: 0.7350\n",
            "Epoch 2/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.5165 - binary_accuracy: 0.7809 - val_loss: 0.4558 - val_binary_accuracy: 0.8288\n",
            "Epoch 3/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.4196 - binary_accuracy: 0.8472 - val_loss: 0.3830 - val_binary_accuracy: 0.8687\n",
            "Epoch 4/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3604 - binary_accuracy: 0.8780 - val_loss: 0.3351 - val_binary_accuracy: 0.8900\n",
            "Epoch 5/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3199 - binary_accuracy: 0.8961 - val_loss: 0.3014 - val_binary_accuracy: 0.9062\n",
            "Epoch 6/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2906 - binary_accuracy: 0.9061 - val_loss: 0.2765 - val_binary_accuracy: 0.9125\n",
            "Epoch 7/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2687 - binary_accuracy: 0.9123 - val_loss: 0.2575 - val_binary_accuracy: 0.9150\n",
            "Epoch 8/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2520 - binary_accuracy: 0.9153 - val_loss: 0.2426 - val_binary_accuracy: 0.9162\n",
            "Epoch 9/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2389 - binary_accuracy: 0.9174 - val_loss: 0.2307 - val_binary_accuracy: 0.9175\n",
            "Epoch 10/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2286 - binary_accuracy: 0.9189 - val_loss: 0.2215 - val_binary_accuracy: 0.9187\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 0.2101 - binary_accuracy: 0.9287\n",
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model NN is: 0.928750.\n",
            "The False Positive Rate for overall population is: 0.083933.\n",
            "The False Negative Rate for overall population is: 0.057441.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.558577 and 0.400621, and D_par=0.157956.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.928870 and 0.928571, and D_cal=0.000299.\n",
            "The False Positive Rate for African American and Caucasian are 0.099548 and 0.066327, and D_FPR=0.033221.\n",
            "The False Negative Rate for African American and Caucasian are 0.046693 and 0.079365, and D_FNR=-0.032672.\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we custom constraint and loss to the base model:"
      ],
      "metadata": {
        "id": "EXWOqulX9WSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DFR(model,X,y,type):\n",
        "    '''\n",
        "    type: str in ['dfnr','dfpr','both']\n",
        "    '''\n",
        "    if type!='dfnr' and type!='dfpr' and type!='both':\n",
        "        return None\n",
        "    size = len(y)\n",
        "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
        "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(np.round(y_pred), axis=1)\n",
        "    y_pred_AA, y_AA = y_pred[idx_AA], y[idx_AA]\n",
        "    y_pred_C, y_C = y_pred[idx_C], y[idx_C] \n",
        "    FPR_AA = sum(y_pred_AA[y_AA==0]==1)/len(y_AA[y_AA==0])\n",
        "    FNR_AA = sum(y_pred_AA[y_AA==1]==0)/len(y_AA[y_AA==1])\n",
        "    FPR_C = sum(y_pred_C[y_C==0]==1)/len(y_C[y_C==0])\n",
        "    FNR_C = sum(y_pred_C[y_C==1]==0)/len(y_C[y_C==1])\n",
        "    dfnr = FNR_AA-FNR_C\n",
        "    dfpr = FPR_AA-FPR_C\n",
        "    if type=='dfnr':\n",
        "        return dfnr\n",
        "    elif type=='dfpr':\n",
        "        return dfpr\n",
        "    else:\n",
        "        return dfnr,dfpr\n",
        "dfnr,dfpr = DFR(NN1,X_test,y_test,'both')\n",
        "print(dfnr,dfpr)"
      ],
      "metadata": {
        "id": "UJz_2QB2NvLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af626719-84b7-4d4f-e4fb-e53300de600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.06830955469087764 0.05305199002677995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def new_training_groups(model, X_train, y_train):\n",
        "    '''\n",
        "    X       n*d\n",
        "    model.predict(X)    n*2\n",
        "    y       n,\n",
        "    delta   c,\n",
        "    dfr   \"dfpr\",\"dfnr\",\"both\"\n",
        "    '''\n",
        "    \n",
        "    #split training sets according to sensitive variable\n",
        "    X_train_AA = X_train[np.array(X_train[:,1] == 0.0)]\n",
        "    y_train_AA = y_train[np.array(X_train[:,1] == 0.0)]\n",
        "    X_train_C = X_train[np.array(X_train[:,1] == 1.0)]\n",
        "    y_train_C = y_train[np.array(X_train[:,1] == 1.0)]\n",
        "    #get the ones with wrong prediction in discriminated group\n",
        "    dn,dp = DFR(model,X_train,y_train,type=\"both\")\n",
        "    if dp>0: d = 0\n",
        "    else: d = 1\n",
        "\n",
        "    if d == 0:\n",
        "        #take penalized trainers\n",
        "        y_pred_AA = np.argmax(model.predict(X_train_AA),axis = 1)\n",
        "        y_diff_AA = y_train_AA-y_pred_AA\n",
        "        X_train_penalized = X_train_AA[y_diff_AA != 0.0]\n",
        "        y_train_penalized = y_train_AA[y_diff_AA != 0.0]\n",
        "        # safe trainers\n",
        "        X_train_clean = X_train_AA[y_diff_AA == 0.0]\n",
        "        y_train_clean = y_train_AA[y_diff_AA == 0.0]\n",
        "        #make new\n",
        "        X_train_clean = np.concatenate((X_train_clean,X_train_C),axis=0)\n",
        "        y_train_clean = np.concatenate((y_train_clean,y_train_C),axis=0)\n",
        "    \n",
        "    else:\n",
        "        #reverse the steps above for train set 1\n",
        "        y_pred_C = np.argmax(model.predict(X_train_C),axis = 1)\n",
        "        y_diff_C = y_train_C-y_pred_C\n",
        "        X_train_penalized = X_train_C[y_diff_C != 0.0]\n",
        "        y_train_penalized = y_train_C[y_diff_C != 0.0]\n",
        "        # safe trainers\n",
        "        X_train_clean = X_train_C[y_diff_C == 0.0]\n",
        "        y_train_clean = y_train_C[y_diff_C == 0.0]\n",
        "        #make new\n",
        "        X_train_clean = np.concatenate((X_train_clean,X_train_AA),axis=0)\n",
        "        y_train_clean = np.concatenate((y_train_clean,y_train_AA),axis=0)\n",
        "    \n",
        "    #X_train_penalized = tf.convert_to_tensor(X_train_penalized, dtype=tf.float32)\n",
        "    #X_train_safe = tf.convert_to_tensor(X_train_safe, dtype=tf.float32)\n",
        "    return X_train_clean , y_train_clean , X_train_penalized , y_train_penalized, dn, dp\n"
      ],
      "metadata": {
        "id": "wqkxAj9pDcbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use Neural network as the base model, the result is unlikely to be reimplemented, but similar result will come out.\n",
        "\n",
        "**Note: When few features are used in the model, the model and its prediction are unstable."
      ],
      "metadata": {
        "id": "ge3I04qX2yJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################[DON'T RUN THIS CELL IF UNNECESSARY]#########################\n",
        "\n",
        "## initialization\n",
        "np.random.seed(7777) \n",
        "model =  base_nn_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "#initialized C and delta\n",
        "C = 1\n",
        "delta = 0.2\n",
        "iter = 20\n",
        "\n",
        "# new training groups\n",
        "X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(model, X_train, y_train)\n",
        "feature = Input(X_train.shape[1],)\n",
        "y = Dense(2,\"softmax\")(feature)\n",
        "mod_loop = Model(feature,y)\n",
        "adam = tf.keras.optimizers.Adam(0.001)\n",
        "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metric = [tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "def penal_loss(y_true,y_pred):\n",
        "  return loss(tf.one_hot(y_tp,2), mod_loop(X_tp))\n",
        "\n",
        "def clean_loss(y_true,y_pred):  \n",
        "  return loss(tf.one_hot(y_ts,2), mod_loop(X_ts))\n",
        "\n",
        "count = 0\n",
        "# start while loop\n",
        "while (count==0 or count%2==1 or abs(dp)>0.05) and count<iter: \n",
        "    # or examine dn, here count%2==1 used to control accuracy in case the accuracy is below 0.5 and one of the overall fpr/fnr will be close to 1\n",
        "    C = C+delta\n",
        "    #print('Count:%d'%count)\n",
        "    mod_loop.compile(optimizer=adam,loss=[penal_loss, clean_loss],loss_weights=[C,1],metrics=metric)\n",
        "    mod_loop.fit(X_train, tf.one_hot(y_train,2), epochs=10, validation_data=(X_val,tf.one_hot(y_val,2)))\n",
        "    X_ts, y_ts, X_tp, y_tp, dn, dp = new_training_groups(mod_loop, X_train, y_train)\n",
        "    #dp = DFR(model_in_loop,X_test,y_test,'dfpr')\n",
        "    count+=1"
      ],
      "metadata": {
        "id": "OUALnfmSLCwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a41b687-adf0-41c1-e8cf-3f3d72bd690b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.5546 - binary_accuracy: 0.7646 - val_loss: 0.4471 - val_binary_accuracy: 0.8512\n",
            "Epoch 2/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.4055 - binary_accuracy: 0.8697 - val_loss: 0.3590 - val_binary_accuracy: 0.8838\n",
            "Epoch 3/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3405 - binary_accuracy: 0.8925 - val_loss: 0.3135 - val_binary_accuracy: 0.8988\n",
            "Epoch 4/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.3025 - binary_accuracy: 0.9031 - val_loss: 0.2843 - val_binary_accuracy: 0.9062\n",
            "Epoch 5/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2767 - binary_accuracy: 0.9094 - val_loss: 0.2631 - val_binary_accuracy: 0.9125\n",
            "Epoch 6/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2576 - binary_accuracy: 0.9123 - val_loss: 0.2471 - val_binary_accuracy: 0.9137\n",
            "Epoch 7/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2431 - binary_accuracy: 0.9153 - val_loss: 0.2345 - val_binary_accuracy: 0.9125\n",
            "Epoch 8/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2317 - binary_accuracy: 0.9172 - val_loss: 0.2245 - val_binary_accuracy: 0.9150\n",
            "Epoch 9/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2226 - binary_accuracy: 0.9189 - val_loss: 0.2161 - val_binary_accuracy: 0.9150\n",
            "Epoch 10/10\n",
            "413/413 [==============================] - 1s 2ms/step - loss: 0.2154 - binary_accuracy: 0.9196 - val_loss: 0.2098 - val_binary_accuracy: 0.9175\n",
            "Epoch 1/10\n",
            "130/130 [==============================] - 1s 3ms/step - loss: 0.8857 - binary_accuracy: 0.3225 - val_loss: 0.7991 - val_binary_accuracy: 0.2738\n",
            "Epoch 2/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.7445 - binary_accuracy: 0.2671 - val_loss: 0.6992 - val_binary_accuracy: 0.2537\n",
            "Epoch 3/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.6701 - binary_accuracy: 0.2470 - val_loss: 0.6453 - val_binary_accuracy: 0.2463\n",
            "Epoch 4/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.6283 - binary_accuracy: 0.2387 - val_loss: 0.6133 - val_binary_accuracy: 0.2475\n",
            "Epoch 5/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.6019 - binary_accuracy: 0.2363 - val_loss: 0.5913 - val_binary_accuracy: 0.2387\n",
            "Epoch 6/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5826 - binary_accuracy: 0.2276 - val_loss: 0.5740 - val_binary_accuracy: 0.2262\n",
            "Epoch 7/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5665 - binary_accuracy: 0.2179 - val_loss: 0.5590 - val_binary_accuracy: 0.2188\n",
            "Epoch 8/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5522 - binary_accuracy: 0.2065 - val_loss: 0.5453 - val_binary_accuracy: 0.1925\n",
            "Epoch 9/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5389 - binary_accuracy: 0.1959 - val_loss: 0.5325 - val_binary_accuracy: 0.1850\n",
            "Epoch 10/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5264 - binary_accuracy: 0.1850 - val_loss: 0.5202 - val_binary_accuracy: 0.1737\n",
            "Epoch 1/10\n",
            "130/130 [==============================] - 1s 3ms/step - loss: 1.8510 - binary_accuracy: 0.2456 - val_loss: 1.3877 - val_binary_accuracy: 0.3625\n",
            "Epoch 2/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 1.1853 - binary_accuracy: 0.4579 - val_loss: 1.0309 - val_binary_accuracy: 0.5025\n",
            "Epoch 3/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.9379 - binary_accuracy: 0.5789 - val_loss: 0.8583 - val_binary_accuracy: 0.5875\n",
            "Epoch 4/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.8009 - binary_accuracy: 0.6409 - val_loss: 0.7485 - val_binary_accuracy: 0.6425\n",
            "Epoch 5/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.7071 - binary_accuracy: 0.6785 - val_loss: 0.6679 - val_binary_accuracy: 0.6712\n",
            "Epoch 6/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.6354 - binary_accuracy: 0.7053 - val_loss: 0.6041 - val_binary_accuracy: 0.7038\n",
            "Epoch 7/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5773 - binary_accuracy: 0.7247 - val_loss: 0.5513 - val_binary_accuracy: 0.7150\n",
            "Epoch 8/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.5286 - binary_accuracy: 0.7441 - val_loss: 0.5064 - val_binary_accuracy: 0.7287\n",
            "Epoch 9/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.4868 - binary_accuracy: 0.7610 - val_loss: 0.4676 - val_binary_accuracy: 0.7500\n",
            "Epoch 10/10\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 0.4505 - binary_accuracy: 0.7751 - val_loss: 0.4336 - val_binary_accuracy: 0.7600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "print(DFR(mod_loop,X_test,y_test,type='both'))\n",
        "evaluation(mod_loop,X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WStZGDfzefQk",
        "outputId": "90dce3ec-dab5-47b0-81c4-aee5cddb8648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.04811314928046445, -0.030104349432080513)\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 0.4336 - binary_accuracy: 0.7725\n",
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model NN is: 0.772500.\n",
            "The False Positive Rate for overall population is: 0.290168.\n",
            "The False Negative Rate for overall population is: 0.159269.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.571130 and 0.527950, and D_par=0.043179.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.778243 and 0.763975, and D_cal=0.014268.\n",
            "The False Positive Rate for African American and Caucasian are 0.276018 and 0.306122, and D_FPR=-0.030104.\n",
            "The False Negative Rate for African American and Caucasian are 0.175097 and 0.126984, and D_FNR=0.048113.\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################################################################################\n",
        "####################### IF NEED TO MODIFY CODES, USE THIS CHUNK TO RUN ABOVE OR RUN BELOW ##########################################\n",
        "####################################################################################################################################"
      ],
      "metadata": {
        "id": "5dLHriKnrKIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment ($DM_{sen}$ & $DM$)"
      ],
      "metadata": {
        "id": "DxdmajarNgms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first implement $DM_{sen}$ as we won't do anything to the dataset:"
      ],
      "metadata": {
        "id": "idcynX6VBaSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use 4:1 as the ratio of train:test\n",
        "y = data.two_year_recid\n",
        "y = np.asarray(y).astype('float32')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "train_size = len(y_train)\n",
        "train_idx_AA = np.array(range(train_size))[X_train[:,1]==0.0]\n",
        "train_idx_C = np.array(range(train_size))[X_train[:,1]==1.0]\n",
        "test_size = len(y_test)\n",
        "test_idx_AA = np.array(range(test_size))[X_test[:,1]==0.0]\n",
        "test_idx_C = np.array(range(test_size))[X_test[:,1]==1.0]\n",
        "\n",
        "X_train1 = np.hstack((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train))\n",
        "X_train1_AA = X_train1[train_idx_AA]\n",
        "X_train1_C = X_train1[train_idx_C]\n",
        "y_train_AA = y_train[train_idx_AA]\n",
        "y_train_C = y_train[train_idx_C]\n",
        "X_test1 = np.hstack((np.ones(X_test.shape[0]).reshape(X_test.shape[0],1), X_test))\n",
        "X_test1_AA = X_test1[test_idx_AA]\n",
        "X_test1_C = X_test1[test_idx_C]\n",
        "y_test_AA = y_test[test_idx_AA]\n",
        "y_test_C = y_test[test_idx_C]\n",
        "\n",
        "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
        "print(\" X_train size: \", X_train1.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
        "      \"X_test size: \", X_test1.shape, ',       y_test size: ',y_test.shape)\n",
        "print(\" X_train_AA size: \", X_train1[train_idx_AA].shape, \",   X_train_C size: \", X_train1[train_idx_C].shape, '\\n',\n",
        "      \"X_test_AA size: \", X_test1[test_idx_AA].shape, ',     X_test_C size: ',X_test1[test_idx_C].shape, '\\n',\n",
        "      \"Ratio:\", X_train1[train_idx_AA].shape[0]/X_train1[train_idx_C].shape[0],X_test1[test_idx_AA].shape[0]/X_test1[test_idx_C].shape[0])\n",
        "print('\\n',\"#\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQIJDMSy164k",
        "outputId": "3bc394f9-85bd-4515-ef1a-323ef15dbce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################ \n",
            "                       Split up Train-Test sets  \n",
            " ################################################################################ \n",
            "\n",
            " X_train size:  (4584, 16) ,      y_train size:  (4584,) \n",
            " X_test size:  (1146, 16) ,       y_test size:  (1146,)\n",
            " X_train_AA size:  (2745, 16) ,   X_train_C size:  (1839, 16) \n",
            " X_test_AA size:  (688, 16) ,     X_test_C size:  (458, 16) \n",
            " Ratio: 1.4926590538336053 1.502183406113537\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Loss: Add Constraints"
      ],
      "metadata": {
        "id": "cLJDK5Ys0M3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the paper, loss function is modified in logistic regression:"
      ],
      "metadata": {
        "id": "raHMVMyr0Gsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dccp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a7d_AwRmOFo",
        "outputId": "54101aef-a953-4544-97a0-3ff0cbca7af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dccp in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: cvxpy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from dccp) (1.0.31)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (0.70.12.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (1.4.1)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (2.0.10)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (0.6.2.post0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (1.21.5)\n",
            "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy>=0.3.5->dccp) (3.2.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy>=0.3.5->dccp) (0.1.5.post0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy>=0.3.5->dccp) (0.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dccp\n",
        "import cvxpy as cvx\n",
        "from cvxpy import *"
      ],
      "metadata": {
        "id": "o4W8A43KqgwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lossfunc(X,theta,y_true):\n",
        "    # This function returns the log loss.\n",
        "    y_true= 2*y_true - 1 #{0,1}->{-1,1}\n",
        "    log_loss = sum(logistic(multiply(-y_true, X*theta)))\n",
        "    return log_loss\n",
        "\n",
        "\n",
        "np.random.seed(5243)\n",
        "theta = cvx.Variable(X_train1.shape[1])\n",
        "theta.value = np.random.rand(theta.shape[0])\n",
        "\n",
        "tau, mu, EPS = 0.005, 1.5, 1 \n",
        "Prob1 = cvx.Problem(Minimize(lossfunc(X_train1,theta,y_train)),[]) # No constraints\n",
        "             \n",
        "      \n",
        "print(dccp.is_dccp(Prob1))\n",
        "print(theta.value)\n",
        "#[0.5591043  0.9994264  0.57031546 0.43833912 0.08453454 0.05043884\n",
        "# 0.91119515 0.16423428 0.3034639  0.41950956 0.85237613 0.4244003\n",
        "# 0.96147514 0.26277008 0.02849745 0.61075812]\n",
        "result = Prob1.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True) #Here changes the theta.value, result avoids the output\n",
        "print(theta.value)\n",
        "#[-2.22198860e+00  1.75116119e-01  1.31373607e-01 -2.60536345e-01 1.53575511e+00  4.91247336e+00 \n",
        "# -2.60491152e-01  4.36237090e-03  4.13966295e-01  7.39736815e-02  1.01397507e-01  2.79119037e-01\n",
        "# -6.37340155e-02  3.96745359e-02  6.88413345e-02 -2.27307636e+00]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O9LwGH3lvpz",
        "outputId": "7d84de9e-9219-465c-cb95-83e8bcec68df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "[0.5591043  0.9994264  0.57031546 0.43833912 0.08453454 0.05043884\n",
            " 0.91119515 0.16423428 0.3034639  0.41950956 0.85237613 0.4244003\n",
            " 0.96147514 0.26277008 0.02849745 0.61075812]\n",
            "[-2.02760707  0.12980288  0.12303316 -0.27410271  1.71950929  4.75786739\n",
            " -0.27591511  0.02399848  0.45356176  0.05861524  0.04602311  0.27193313\n",
            " -0.04650322  0.15181987 -0.45112505 -2.24001541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,theta):\n",
        "    #y:{-1,1}->{0,1}\n",
        "    d = np.dot(X,theta)\n",
        "    y_pred = (np.sign(d) + 1)/2\n",
        "    return y_pred\n",
        "\n",
        "theta_star = theta.value\n",
        "y_pred = predict(X_test1, theta_star)"
      ],
      "metadata": {
        "id": "tPl_LD06m2BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_DM(X,y,y_pred):\n",
        "    size = X.shape[0]\n",
        "    idx_AA = np.array(range(size))[X[:,1]==0.0]\n",
        "    idx_C = np.array(range(size))[X[:,1]==1.0]\n",
        "    y_pred_AA, y_test_AA = y_pred[idx_AA], y[idx_AA]\n",
        "    y_pred_C, y_test_C = y_pred[idx_C], y[idx_C]\n",
        "    FPR_all = np.sum(y_pred[y==0]==1)/len(y[y==0])\n",
        "    FNR_all = np.sum(y_pred[y==1]==0)/len(y[y==1])\n",
        "    FPR_AA = np.sum(y_pred_AA[y_test_AA==0]==1)/len(y_test_AA[y_test_AA==0])\n",
        "    FNR_AA = np.sum(y_pred_AA[y_test_AA==1]==0)/len(y_test_AA[y_test_AA==1])\n",
        "    FPR_C = np.sum(y_pred_C[y_test_C==0]==1)/len(y_test_C[y_test_C==0])\n",
        "    FNR_C = np.sum(y_pred_C[y_test_C==1]==0)/len(y_test_C[y_test_C==1])\n",
        "    pred_p_AA, pred_p_C = np.mean(y_pred_AA==1), np.mean(y_pred_C==1)\n",
        "    acc = np.sum(y_pred == y)/len(y)\n",
        "    acc_AA, acc_C = np.mean(y_pred_AA == y_test_AA), np.mean(y_pred_C == y_test_C)\n",
        "    print('\\n',\"#\"*80)\n",
        "    print('The accuracy of baseline model LR is: %3f.'%(acc))\n",
        "    print('The False Positive Rate for overall population is: %3f.'%FPR_all)\n",
        "    print('The False Negative Rate for overall population is: %3f.'%FNR_all)\n",
        "    print(\"Specifically:\")\n",
        "    print('Parity Check: The rate of positive estimate for African American and Caucasian are %3f and %3f, and D_par=%3f.'%(pred_p_AA,pred_p_C,pred_p_AA-pred_p_C))\n",
        "    print('Calibration Check: The rate of correct estimate for African American and Caucasian are %3f and %3f, and D_cal=%3f.'%(acc_AA,acc_C,acc_AA-acc_C))\n",
        "    print('The False Positive Rate for African American and Caucasian are %3f and %3f, and D_FPR=%3f.'%(FPR_AA,FPR_C,FPR_AA-FPR_C))\n",
        "    print('The False Negative Rate for African American and Caucasian are %3f and %3f, and D_FNR=%3f.'%(FNR_AA,FNR_C,FNR_AA-FNR_C))\n",
        "    print('\\n',\"#\"*80)\n",
        "#print(y_pred.shape)\n",
        "evaluation_DM(X_test1,y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpNPZC-3nYhS",
        "outputId": "1b8016e0-67ea-4732-eb81-033318a21537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model LR is: 0.924084.\n",
            "The False Positive Rate for overall population is: 0.084956.\n",
            "The False Negative Rate for overall population is: 0.067126.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.423729 and 0.538462, and D_par=-0.114733.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.932203 and 0.921978, and D_cal=0.010225.\n",
            "The False Positive Rate for African American and Caucasian are 0.077465 and 0.087470, and D_FPR=-0.010006.\n",
            "The False Negative Rate for African American and Caucasian are 0.053191 and 0.069815, and D_FNR=-0.016624.\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not put constaints on the loss function, the accuracy of the logistic regression model is around $93\\%$, while the FPR, FNR is around 0.05 to 0.1. \n",
        "\n",
        "Then, we put the constraint in the following model:"
      ],
      "metadata": {
        "id": "gIjSM7mcsOY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constaints on loss function\n",
        "np.random.seed(5243)\n",
        "theta1 = cvx.Variable(X_train1.shape[1])\n",
        "theta1.value = np.random.rand(theta.shape[0])\n",
        "\n",
        "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
        "\n",
        "def g_theta(y,X,theta):\n",
        "    y = 2*y - 1\n",
        "    d = matmul(X,theta)\n",
        "    y_d = multiply(y,d)\n",
        "    return minimum(np.zeros_like(y_d),y_d)\n",
        "\n",
        "c = 0.05\n",
        "N0 = X_train1_AA.shape[0]\n",
        "N1 = X_train1_C.shape[0]\n",
        "N = X_train1.shape[0]\n",
        "print(N,N0,N1)\n",
        "\n",
        "\n",
        "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1,theta1,y_train)),\n",
        "                 [N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) <= c + N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)), \n",
        "                  N0/N*sum(g_theta(y_train_C,X_train1_C,theta1)) >= N1/N*sum(g_theta(y_train_AA, X_train1_AA,theta1)) - c]) # With constraints\n",
        "print(dccp.is_dccp(Prob2))\n",
        "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)\n",
        "#g_theta(y_train,X_train1, theta.value).value.shape\n",
        "#constraint()\n",
        "#X_train.\n",
        "#pd.DataFrame(X_train)\n",
        "#pd.DataFrame(x_train1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_9JaBPrsNhr",
        "outputId": "f7ff2a43-41c4-4a01-e0cb-976fd00e4ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4584 2745 1839\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = predict(X_test1, theta1.value)\n",
        "evaluation_DM(X_test1,y_test,y_pred1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1g7epK30aT2",
        "outputId": "a1a78ae3-d68d-4c6e-db4b-2e16915000cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model LR is: 0.924084.\n",
            "The False Positive Rate for overall population is: 0.088496.\n",
            "The False Negative Rate for overall population is: 0.063683.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.427966 and 0.541758, and D_par=-0.113792.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.927966 and 0.923077, and D_cal=0.004889.\n",
            "The False Positive Rate for African American and Caucasian are 0.084507 and 0.089835, and D_FPR=-0.005327.\n",
            "The False Negative Rate for African American and Caucasian are 0.053191 and 0.065708, and D_FNR=-0.012517.\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above result, we may see the $DM_{sen}$ algorithm slightly drops the $D_{FNR}$ to around -0.015, which is very close to $D_{FPR}$. \n",
        "\n",
        "Then we implement $DM$:"
      ],
      "metadata": {
        "id": "dFSvGNJHRban"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1_sen = np.delete(X_train1,2,1)\n",
        "X_train1_AA_sen = X_train1_sen[train_idx_AA]\n",
        "X_train1_C_sen = X_train1_sen[train_idx_C]\n",
        "X_test1_sen = np.delete(X_test1,2,1)\n",
        "X_test1_AA_sen = X_test1_sen[test_idx_AA]\n",
        "X_test1_C_sen = X_test1_sen[test_idx_C]\n",
        "\n",
        "print('\\n',\"#\"*80,'\\n',' '*20,\" Split up Train-Test sets \",'\\n',\"#\"*80,'\\n')\n",
        "print(\" X_train1_sen size: \", X_train1_sen.shape, \",      y_train size: \", y_train.shape, '\\n',\n",
        "      \"X_test1_sen size: \", X_test1_sen.shape, ',       y_test size: ',y_test.shape)\n",
        "print(\" X_train1_AA_sen size: \", X_train1_AA_sen.shape, \",   X_train1_C_sen size: \", X_train1_C_sen.shape, '\\n',\n",
        "      \"X_test1_AA_sen size: \", X_test1_AA_sen.shape, ',     X_test1_C_sen size: ',X_test1_C_sen.shape, '\\n',\n",
        "      \"Ratio:\", X_train1_AA_sen.shape[0]/X_train1_C_sen.shape[0],X_test1_AA_sen.shape[0]/X_test1_C_sen.shape[0])\n",
        "print('\\n',\"#\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdzBsm2yewlI",
        "outputId": "27a37aec-3f32-4deb-e25a-2aba5e54b077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################ \n",
            "                       Split up Train-Test sets  \n",
            " ################################################################################ \n",
            "\n",
            " X_train1_sen size:  (4584, 15) ,      y_train size:  (4584,) \n",
            " X_test1_sen size:  (1146, 15) ,       y_test size:  (1146,)\n",
            " X_train1_AA_sen size:  (2745, 15) ,   X_train1_C_sen size:  (1839, 15) \n",
            " X_test1_AA_sen size:  (688, 15) ,     X_test1_C_sen size:  (458, 15) \n",
            " Ratio: 1.4926590538336053 1.502183406113537\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constaints on loss function\n",
        "np.random.seed(5243)\n",
        "theta2 = cvx.Variable(X_train1_sen.shape[1])\n",
        "theta2.value = np.random.rand(theta2.shape[0])\n",
        "\n",
        "tau, mu, EPS = 0.5, 1.6, 1e-4 \n",
        "c = 0.05\n",
        "N0 = X_train1_AA_sen.shape[0]\n",
        "N1 = X_train1_C_sen.shape[0]\n",
        "N = X_train1_sen.shape[0]\n",
        "print(N,N0,N1)\n",
        "\n",
        "\n",
        "Prob2 = cvx.Problem(Minimize(lossfunc(X_train1_sen,theta2,y_train)),\n",
        "                 [N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) <= c + N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)), \n",
        "                  N0/N*sum(g_theta(y_train_C,X_train1_C_sen,theta2)) >= N1/N*sum(g_theta(y_train_AA,X_train1_AA_sen,theta2)) - c]) # With constraints\n",
        "print(dccp.is_dccp(Prob2))\n",
        "result1 = Prob2.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4JivYlwa3Dr",
        "outputId": "2ad5082a-fb14-479d-f373-1c32967d94ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4584 2745 1839\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = predict(X_test1_sen, theta2.value)\n",
        "evaluation_DM(X_test1_sen,y_test,y_pred2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0sU93opf_Pr",
        "outputId": "5d39a29e-c77e-4019-bd4b-41f089a0d396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ################################################################################\n",
            "The accuracy of baseline model LR is: 0.925829.\n",
            "The False Positive Rate for overall population is: 0.084956.\n",
            "The False Negative Rate for overall population is: 0.063683.\n",
            "Specifically:\n",
            "Parity Check: The rate of positive estimate for African American and Caucasian are 0.423729 and 0.540659, and D_par=-0.116931.\n",
            "Calibration Check: The rate of correct estimate for African American and Caucasian are 0.932203 and 0.924176, and D_cal=0.008028.\n",
            "The False Positive Rate for African American and Caucasian are 0.077465 and 0.087470, and D_FPR=-0.010006.\n",
            "The False Negative Rate for African American and Caucasian are 0.053191 and 0.065708, and D_FNR=-0.012517.\n",
            "\n",
            " ################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.time()\n",
        "print('The running time of overall algorithm is: %3fs.'%(end-start))"
      ],
      "metadata": {
        "id": "UEvRcabGJbl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641e24af-b610-4605-a86d-b6175c1213d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The running time of overall algorithm is: 642.106358s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may see $DM$ algorithm drops $D_{FNR}$. but from the above results, it's hard for us to see which one is perfect and how $DM_{sen}$ violates the disparate treatment. Overall speaking, this algorithm has an impact on controlling the difference in FPR and FNR, but the effect deserves further study as when few features are in the model, both two algorithms seem to have no effect on controlling our target."
      ],
      "metadata": {
        "id": "PJzteiF3gNRP"
      }
    }
  ]
}
