{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d21239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compas = pd.read_csv('../data/compas-scores-two-years.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efda004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>marsha miles</td>\n",
       "      <td>marsha</td>\n",
       "      <td>miles</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>Male</td>\n",
       "      <td>1971-08-22</td>\n",
       "      <td>44</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>2013-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>edward riddle</td>\n",
       "      <td>edward</td>\n",
       "      <td>riddle</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>Male</td>\n",
       "      <td>1974-07-23</td>\n",
       "      <td>41</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "      <td>2014-02-19</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>2014-04-18</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              name   first       last compas_screening_date   sex  \\\n",
       "0   1  miguel hernandez  miguel  hernandez            2013-08-14  Male   \n",
       "1   3       kevon dixon   kevon      dixon            2013-01-27  Male   \n",
       "2   4          ed philo      ed      philo            2013-04-14  Male   \n",
       "5   7      marsha miles  marsha      miles            2013-11-30  Male   \n",
       "6   8     edward riddle  edward     riddle            2014-02-19  Male   \n",
       "\n",
       "          dob  age          age_cat              race  ...  v_decile_score  \\\n",
       "0  1947-04-18   69  Greater than 45             Other  ...               1   \n",
       "1  1982-01-22   34          25 - 45  African-American  ...               1   \n",
       "2  1991-05-14   24     Less than 25  African-American  ...               3   \n",
       "5  1971-08-22   44          25 - 45             Other  ...               1   \n",
       "6  1974-07-23   41          25 - 45         Caucasian  ...               2   \n",
       "\n",
       "   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
       "0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n",
       "1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
       "2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
       "5           Low        2013-11-30  2013-11-30   2013-12-01               0   \n",
       "6           Low        2014-02-19  2014-03-31   2014-04-18              14   \n",
       "\n",
       "  start  end event two_year_recid  \n",
       "0     0  327     0              0  \n",
       "1     9  159     1              1  \n",
       "2     0   63     0              1  \n",
       "5     1  853     0              0  \n",
       "6     5   40     1              1  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter data for useful rows\n",
    "compas = compas[compas['days_b_screening_arrest'] >= -30]\n",
    "compas = compas[compas['days_b_screening_arrest'] <= 30]\n",
    "compas = compas[compas['is_recid'] != -1]\n",
    "compas = compas[compas['c_charge_degree'] != \"O\"]\n",
    "compas = compas[compas['score_text'] != \"N/A\"]\n",
    "compas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4601fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compas = compas[[\"sex\",\"age\",\"race\",\"decile_score\",\"priors_count\",\n",
    "                 \"c_charge_degree\",\"two_year_recid\",\"c_jail_in\", \"c_jail_out\"]]\n",
    "\n",
    "compas = compas.loc[compas.race.isin([\"Caucasian\", \"African-American\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a933ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahkurihara/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/sarahkurihara/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Convert features to categorical type\n",
    "compas_encoded = compas.copy()\n",
    "compas_encoded.sex = pd.get_dummies(compas[\"sex\"])[\"Female\"]\n",
    "compas_encoded.race = pd.get_dummies(compas[\"race\"])[\"African-American\"]\n",
    "compas_encoded.c_charge_degree = pd.get_dummies(compas[\"c_charge_degree\"])[\"F\"]\n",
    "\n",
    "# Calculate length of stay to use in model (log hours)\n",
    "compas_encoded['c_jail_in'] = pd.to_datetime(compas_encoded['c_jail_in'])\n",
    "compas_encoded['c_jail_out'] = pd.to_datetime(compas_encoded['c_jail_out'])\n",
    "compas_encoded['los'] = np.log((compas_encoded['c_jail_out']-compas_encoded['c_jail_in']).astype('timedelta64[h]'))\n",
    "\n",
    "compas_encoded.drop([\"c_jail_in\", \"c_jail_out\"], axis = 1, inplace=True)\n",
    "\n",
    "compas_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "compas_encoded.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c149a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compas_encoded.head()\n",
    "compas_encoded.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a949a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features, S = sensitive attributes, E = explanatory attribute, Y = response\n",
    "X = ['sex','age','decile_score','priors_count', 'los']\n",
    "S = \"race\"\n",
    "E = \"c_charge_degree\"\n",
    "Y = \"two_year_recid\"\n",
    "\n",
    "X_ALL = ['sex','age','decile_score','priors_count','race','c_charge_degree','los']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94fa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5:1:1 split\n",
    "np.random.seed(5243)\n",
    "train, test = train_test_split(compas_encoded, test_size=1/7)\n",
    "train, val = train_test_split(train, test_size=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67b802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3610, 8) (723, 8) (723, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513b19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[X_ALL]\n",
    "L_train = train[X]\n",
    "s_train = train[S]\n",
    "e_train = train[E]\n",
    "y_train = train[Y]\n",
    "\n",
    "X_val = val[X_ALL]\n",
    "L_val = val[X]\n",
    "s_val = val[S]\n",
    "e_val = val[E]\n",
    "y_val = val[Y]\n",
    "\n",
    "X_test = test[X_ALL]\n",
    "L_test = test[X]\n",
    "s_test = test[S]\n",
    "e_test = test[E]\n",
    "y_test = test[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61dbf162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686030428769018"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c38e0",
   "metadata": {},
   "source": [
    "## Create functions used in pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150d7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of partitions: 1 for each unique value of e\n",
    "\n",
    "# X is the full dataset (in our case, train)\n",
    "# e is the \n",
    "def PARTITION(X):\n",
    "    partitions = list()\n",
    "    \n",
    "    for e_i in np.unique(X[E]):\n",
    "        partitions.append(X[X[E]==e_i])\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc4b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta function returns the number of observations (i.e. people) who are incorrectly classified \n",
    "# based on theoretical probabilities of reciding, calculated as the average rate of reciding\n",
    "# for each explanatory varaible (in our case, type of crime comittied, c_charge_degree)\n",
    "\n",
    "def DELTA(X, X_ei, s_i):\n",
    "    \n",
    "    # Gi is the number of observations for each race\n",
    "    # Don't we need to pass S as a parameter for this function?\n",
    "    Gi = sum(X_ei[S] == s_i)\n",
    "    \n",
    "    # X_ei_si is the dataset that contains the observations for each race\n",
    "    X_ei_si = X_ei[X_ei[S] == s_i]\n",
    "    \n",
    "    # P_denom is the number of people in group \n",
    "    # P_num is number of observations who recid\n",
    "    P_denom = X_ei_si.shape[0]\n",
    "    P_num = sum(X_ei_si[Y] == 1)\n",
    "    \n",
    "    # P is the probability of reciding for one race\n",
    "    # It is calculated by taking number of people who recid in each group \n",
    "    # dividied by total number of people in that group\n",
    "    P = P_num/P_denom\n",
    "    \n",
    "    # All other observations (for the other group)\n",
    "    X_ei_not_si = X_ei[X_ei[S] != s_i]\n",
    "    \n",
    "    # The probability of reciding for the other group (same calculation as above)\n",
    "    Ps_2 = sum(X_ei_not_si[Y] == 1)/X_ei_not_si.shape[0]\n",
    "    \n",
    "    # Ps is P*, which is the theoretical true probability of reciding\n",
    "    # Calculated by the average \n",
    "    Ps = (P+Ps_2)/2\n",
    "    \n",
    "    # Calcualte the number of incorrectly classified people\n",
    "    d = int(round(Gi * abs(P - Ps)))\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6a210",
   "metadata": {},
   "source": [
    "# Local Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e1ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELTA(African American) =  44 African Americans changed from 1 to 0\n",
      "DELTA(Caucasian) =  38 Caucasians changed from 0 to 1\n",
      "DELTA(African American) =  99 African Americans changed from 1 to 0\n",
      "DELTA(Caucasian) =  57 Caucasians changed from 0 to 1\n"
     ]
    }
   ],
   "source": [
    "relabeled_X_ei = list()\n",
    "\n",
    "for X_ei in PARTITION(train):\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    \n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    delta_afam = DELTA(train, X_ei, 1)\n",
    "    afam_predicted_1_index = [afam_index[v] for v in np.squeeze(np.where(ranker_model.predict(afam[X_ALL]) == 1))]\n",
    "    afam_predicted_1_index_Y1 = [i for (i,v) in zip(afam_predicted_1_index, X_ei.iloc[afam_predicted_1_index][Y]) if v==1]\n",
    "    afam_predicted_1 = X_ei.iloc[afam_predicted_1_index_Y1]\n",
    "    \n",
    "    afam_ranks = (ss.rankdata(ranker_model.decision_function(afam_predicted_1[X_ALL]))-1).astype(int)\n",
    "    afam_tochange = [i for (i, v) in zip(list(range(len(afam_ranks))), afam_ranks < delta_afam) if v]\n",
    "    afam_tochange_idx = [afam_predicted_1_index_Y1[v] for v in afam_tochange]\n",
    "    \n",
    "    cauca_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    cauca = X_ei[X_ei[S] == 0].copy()\n",
    "    delta_cauca = DELTA(train, X_ei, 0)\n",
    "    cauca_predicted_0_index = [cauca_index[v] for v in np.squeeze(np.where(ranker_model.predict(cauca[X_ALL]) == 0))]\n",
    "    cauca_predicted_0_index_Y0 = [i for (i,v) in zip(cauca_predicted_0_index, X_ei.iloc[cauca_predicted_0_index][Y]) if v==0]\n",
    "    cauca_predicted_0 = X_ei.iloc[cauca_predicted_0_index_Y0]\n",
    "    \n",
    "    cauca_ranks = (ss.rankdata(-ranker_model.decision_function(cauca_predicted_0[X_ALL]))-1).astype(int)\n",
    "    cauca_tochange = [i for (i, v) in zip(list(range(len(cauca_ranks))), cauca_ranks < delta_cauca) if v]\n",
    "    cauca_tochange_idx = [cauca_predicted_0_index_Y0[v] for v in cauca_tochange]\n",
    "    \n",
    "    for i in afam_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 0\n",
    "    for i in cauca_tochange_idx:\n",
    "        X_ei_copy.loc[X_ei_copy.index[i], Y] = 1\n",
    "    \n",
    "    relabeled_X_ei.append(X_ei_copy)\n",
    "    \n",
    "    print(\"DELTA(African American) = \", delta_afam, \"African Americans changed from 1 to 0\")\n",
    "    print(\"DELTA(Caucasian) = \", delta_cauca, \"Caucasians changed from 0 to 1\")\n",
    "    \n",
    "local_massaging = pd.concat(relabeled_X_ei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0dd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_X_train = local_massaging[X_ALL]\n",
    "lm_Y_train = local_massaging[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28214248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708160442600276"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "clf.score(X_val[X_ALL], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1027894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number changed values should be sum of all DELTAs shown above\n",
    "res = [1 for i, j in zip(train.sort_index()[\"two_year_recid\"], pd.DataFrame(lm_Y_train).sort_index()[\"two_year_recid\"]) if i != j]\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af1693",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f78872",
   "metadata": {},
   "source": [
    "Notation: P_c stands for probability based on the classifier's predictions.\n",
    "\n",
    "### Parity or D_all\n",
    "\n",
    "Parity is defined as the difference is positive prediction rates in the two race groups. Paper 6 also calls this D_all, which stands for all discrimination. Fairness calls for Parity being close to 0.\n",
    "\n",
    "<br>\n",
    "<center>Parity = |P_c(recid = 1 | race = African American) - P_c(recid = 1 | race = Caucasian)</center>\n",
    "    \n",
    "### Calibration\n",
    "\n",
    "Calibration is defined as the difference in accuracies between the two race groups. Fairness calls for Calibration being close to 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>Calibration = |P_c(recid predicted correctly | race = African American) - P_c(recid predicted correctly | race = Caucasian)</center>\n",
    "\n",
    "### Equality of Odds\n",
    "\n",
    "Equality of odds is achieved when the difference in positive prediction rates is equal for the two race groups. Fairness calls for the following value to be close to 0 for both y in {0,1}.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>D_Odds = P_c(recid.hat = 1 | race = African American, recid = y) - P_c(recid.hat = 1 | race = Caucasian, recid = y)</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e1df946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X must include the sensitive feature\n",
    "def PARITY(X, Y_PRED):\n",
    "    s = X[S]\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    num_afam = sum(Y_PRED[X[S] == 1])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    num_cauca = sum(Y_PRED[X[S] == 0])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid = 1 | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid = 1 | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    parity = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Parity =\", parity)\n",
    "    \n",
    "    return(parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d04389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X must include S\n",
    "def CALIBRATION(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    afam = X[X[S] == 1]\n",
    "    Y_TRUE_afam = Y_TRUE[X[S] == 1]\n",
    "    num_afam = sum([1 for (i, v) in zip(Y_TRUE_afam, Y_PRED[X[S]==1]) if i == v])\n",
    "    den_afam = afam.shape[0]\n",
    "    \n",
    "    cauca = X[X[S] == 0]\n",
    "    Y_TRUE_cauca = Y_TRUE[X[S] == 0]\n",
    "    num_cauca = sum([1 for (i, v) in zip(Y_TRUE_cauca, Y_PRED[X[S]==0]) if i == v])\n",
    "    den_cauca = cauca.shape[0]\n",
    "    \n",
    "    print(\"P_c(recid predicted correctly | race = African American) =\", num_afam/den_afam)\n",
    "    print(\"P_c(recid predicted correctly | race = Caucasian) =\", num_cauca/den_cauca)\n",
    "    calibration = abs(num_afam/den_afam - num_cauca/den_cauca)\n",
    "    print(\"Calibration =\", calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dbc3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EQUALITY_OF_ODDS(X, Y_TRUE, Y_PRED):\n",
    "    \n",
    "    # S = afam, Y = 0\n",
    "    X_afam_0 = X[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    Y_PRED_afam_0 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 0)]\n",
    "    num_afam_0 = sum([1 for i in Y_PRED_afam_0 if i == 1])\n",
    "    denom_afam_0 = X_afam_0.shape[0]\n",
    "    P_afam_0 = num_afam_0/denom_afam_0\n",
    "    \n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 1])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 0\n",
    "    X_cauca_0 = X[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    Y_PRED_cauca_0 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 0)]\n",
    "    num_cauca_0 = sum([1 for i in Y_PRED_cauca_0 if i == 1])\n",
    "    denom_cauca_0 = X_cauca_0.shape[0]\n",
    "    P_cauca_0 = num_cauca_0/denom_cauca_0\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 1])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"For recid = 0:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 0) = \", P_afam_0)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 0) = \", P_cauca_0)\n",
    "    print(\"Difference in odds of true recid = 0 is  = D_FPR =\", abs(P_afam_0 - P_cauca_0))\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 1 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 1 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    print(\"Difference in odds of true recid = 1 is = D_TPR =\", abs(P_afam_1 - P_cauca_1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cee0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_FNR(X, Y_TRUE, Y_PRED):\n",
    "    # S = afam, Y = 1\n",
    "    X_afam_1 = X[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    Y_PRED_afam_1 = Y_PRED[np.logical_and(X[S]==1, Y_TRUE == 1)]\n",
    "    num_afam_1 = sum([1 for i in Y_PRED_afam_1 if i == 0])\n",
    "    denom_afam_1 = X_afam_1.shape[0]\n",
    "    P_afam_1 = num_afam_1/denom_afam_1\n",
    "    \n",
    "    # S = cauca, Y = 1\n",
    "    X_cauca_1 = X[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    Y_PRED_cauca_1 = Y_PRED[np.logical_and(X[S]==0, Y_TRUE == 1)]\n",
    "    num_cauca_1 = sum([1 for i in Y_PRED_cauca_1 if i == 0])\n",
    "    denom_cauca_1 = X_cauca_1.shape[0]\n",
    "    P_cauca_1 = num_cauca_1/denom_cauca_1\n",
    "    \n",
    "    print(\"Difference in False Negative Rates\")\n",
    "\n",
    "    print(\"For recid = 1:\\n\")\n",
    "    print(\"P_c(recid.hat = 0 | race = African American, recid = 1) = \", P_afam_1)\n",
    "    print(\"P_c(recid.hat = 0 | race = Caucasian, recid = 1) = \", P_cauca_1)\n",
    "    \n",
    "    print(\"D_FNR =\", abs(P_afam_1 - P_cauca_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d7ba6",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a73fbbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686030428769018"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "baseline_pred = clf.predict(X_val[X_ALL])\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e632e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71       377\n",
      "           1       0.69      0.62      0.65       346\n",
      "\n",
      "    accuracy                           0.69       723\n",
      "   macro avg       0.69      0.68      0.68       723\n",
      "weighted avg       0.69      0.69      0.68       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, clf.predict(X_val[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054313be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.5011286681715575\n",
      "P_c(recid = 1 | race = Caucasian) = 0.31785714285714284\n",
      "Parity = 0.1832715253144147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1832715253144147"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_val, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd68571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.6884875846501128\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.6821428571428572\n",
      "Calibration = 0.006344727507255676\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_val, y_val, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb9fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.31221719457013575\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.17307692307692307\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.13914027149321267\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.6891891891891891\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.5\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.18918918918918914\n"
     ]
    }
   ],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_val, y_val, baseline_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbe55fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in False Negative Rates\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 0 | race = African American, recid = 1) =  0.3108108108108108\n",
      "P_c(recid.hat = 0 | race = Caucasian, recid = 1) =  0.5\n",
      "D_FNR = 0.1891891891891892\n"
     ]
    }
   ],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_val, y_val, baseline_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf18db8",
   "metadata": {},
   "source": [
    "## Local Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783512f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6708160442600276"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(lm_X_train, lm_Y_train)\n",
    "lm_pred = clf.predict(X_val[X_ALL])\n",
    "clf.score(X_val[X_ALL], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5579430c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.70       377\n",
      "           1       0.68      0.60      0.63       346\n",
      "\n",
      "    accuracy                           0.67       723\n",
      "   macro avg       0.67      0.67      0.67       723\n",
      "weighted avg       0.67      0.67      0.67       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, clf.predict(X_val[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b7655f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.3905191873589165\n",
      "P_c(recid = 1 | race = Caucasian) = 0.46785714285714286\n",
      "Parity = 0.07733795549822636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07733795549822636"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parity\n",
    "\n",
    "PARITY(X_val, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26b010cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.672686230248307\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.6678571428571428\n",
      "Calibration = 0.004829087391164166\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "\n",
    "CALIBRATION(X_val, y_val, lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "730e35c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.2171945701357466\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.32051282051282054\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.10331825037707393\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.5630630630630631\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.6532258064516129\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.0901627433885498\n"
     ]
    }
   ],
   "source": [
    "# Equality of Odds\n",
    "\n",
    "EQUALITY_OF_ODDS(X_val, y_val, lm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293334c",
   "metadata": {},
   "source": [
    "## Local Preferential Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7ba01",
   "metadata": {},
   "source": [
    "In this algorithm, we take in a dataset (train) and return a modified dataset of same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed18b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start partition\n",
      "X_ei shape: (1245, 8)\n",
      "Half Delta(AA): 22\n",
      "Half Delta(Cauc): 19\n",
      "Total AAs: 664\n",
      "Total Cs: 581\n",
      "afam dataset shape: (664, 8)\n",
      "c dataset shape: (581, 8)\n",
      "N: 22\n",
      "rows in cleaned_recid before: (253, 9)\n",
      "rows in cleaned_recid after: (231, 9)\n",
      "N: 22\n",
      "rows in cleaned_no_recid before: (411, 9)\n",
      "rows in cleaned_no_recid after: (433, 9)\n",
      "size of final A: (664, 9)\n",
      "M: 19\n",
      "rows in cleaned_recid before: (480, 9)\n",
      "rows in cleaned_recid after: (461, 9)\n",
      "M: 19\n",
      "rows in cleaned_no_recid before: (101, 9)\n",
      "rows in cleaned_no_recid after: (120, 9)\n",
      "size of final C: (581, 9)\n",
      "end partition\n",
      "size of train: (3610, 8)\n",
      "size of recomp: (1245, 8)\n",
      "start partition\n",
      "X_ei shape: (2365, 8)\n",
      "Half Delta(AA): 49\n",
      "Half Delta(Cauc): 28\n",
      "Total AAs: 1504\n",
      "Total Cs: 861\n",
      "afam dataset shape: (1504, 8)\n",
      "c dataset shape: (861, 8)\n",
      "N: 49\n",
      "rows in cleaned_recid before: (897, 9)\n",
      "rows in cleaned_recid after: (848, 9)\n",
      "N: 49\n",
      "rows in cleaned_no_recid before: (607, 9)\n",
      "rows in cleaned_no_recid after: (656, 9)\n",
      "size of final A: (1504, 9)\n",
      "M: 28\n",
      "rows in cleaned_recid before: (575, 9)\n",
      "rows in cleaned_recid after: (547, 9)\n",
      "M: 28\n",
      "rows in cleaned_no_recid before: (286, 9)\n",
      "rows in cleaned_no_recid after: (314, 9)\n",
      "size of final C: (861, 9)\n",
      "end partition\n",
      "size of train: (3610, 8)\n",
      "size of recomp: (3610, 8)\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "recomp_train = pd.DataFrame()\n",
    "\n",
    "# for each partition (explanatory variable)\n",
    "for X_ei in PARTITION(train):\n",
    "    \n",
    "    print(\"start partition\")\n",
    "    X_ei_copy = X_ei.copy()\n",
    "    print(\"X_ei shape:\", X_ei_copy.shape)\n",
    "    \n",
    "    # learn a ranker Hi : Xi -> Yi\n",
    "    ranker_model = LogisticRegression(random_state=0).fit(X_ei[X_ALL], X_ei[Y])\n",
    "    \n",
    "    # Calculate half delta (AA: S_i = 1, AA: S_i = 0)\n",
    "    half_delta_afam = DELTA(train, X_ei, 1) // 2\n",
    "    half_delta_cauc = DELTA(train, X_ei, 0) // 2\n",
    "    print(\"Half Delta(AA):\", half_delta_afam)\n",
    "    print(\"Half Delta(Cauc):\", half_delta_cauc)\n",
    "    \n",
    "    # store indicies\n",
    "    afam_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 1)) if v]\n",
    "    c_index = [i for (i, v) in zip(list(range(X_ei.shape[0])), list(X_ei[S] == 0)) if v]\n",
    "    print(\"Total AAs:\", len(afam_index))\n",
    "    print(\"Total Cs:\", len(c_index))\n",
    "    \n",
    "    # get subset of data to work with\n",
    "    afam = X_ei[X_ei[S] == 1].copy()\n",
    "    c = X_ei[X_ei[S] == 0].copy()\n",
    "    print(\"afam dataset shape:\", afam.shape)\n",
    "    print(\"c dataset shape:\", c.shape)\n",
    "    \n",
    "    # rank AA\n",
    "    afam.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(afam[X_ALL]), columns = ['rank'])\n",
    "    afam_with_rank = pd.concat([afam, rank], axis=1)\n",
    "    \n",
    "    # rank C\n",
    "    c.reset_index(drop=True, inplace=True)\n",
    "    rank = pd.DataFrame(ranker_model.decision_function(c[X_ALL]), columns = ['rank'])\n",
    "    c_with_rank = pd.concat([c, rank], axis=1)\n",
    "    \n",
    "    # sort values, reset indices\n",
    "    afam_with_rank = afam_with_rank.sort_values(['rank'])\n",
    "    afam_with_rank.reset_index(drop = True, inplace = True)\n",
    "    c_with_rank = c_with_rank.sort_values(['rank'])\n",
    "    c_with_rank.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    ######## Modify AA data - find rows to delete/duplicate; decision boundary is 0 #####\n",
    "    recid = sum(afam_with_rank['rank'] > 0)\n",
    "    no_recid = sum(afam_with_rank['rank'] < 0)    \n",
    "    total = len(afam_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    # compas = compas[compas['days_b_screening_arrest'] >= -30]\n",
    "    cleaned_recid = afam_with_rank[afam_with_rank['rank'] > 0]\n",
    "    cleaned_no_recid = afam_with_rank[afam_with_rank['rank'] < 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    N = half_delta_afam\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:N], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"N:\", N)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:N])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_AA = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final A:\", total_AA.shape)\n",
    "    \n",
    "    ########## Modify C data ############\n",
    "    # Find rows to delete/duplicate; decision boundary is 0; opposite code as above\n",
    "    recid = sum(c_with_rank['rank'] < 0)\n",
    "    no_recid = sum(c_with_rank['rank'] > 0)    \n",
    "    total = len(c_with_rank)\n",
    "    \n",
    "    # make copy of recids and no_recids\n",
    "    cleaned_recid = c_with_rank[c_with_rank['rank'] < 0]\n",
    "    cleaned_no_recid = c_with_rank[c_with_rank['rank'] > 0]\n",
    "    \n",
    "    # delete first 1/2 delta values from recid\n",
    "    M = half_delta_cauc\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_recid before:\", cleaned_recid.shape)\n",
    "    cleaned_recid.drop(index=cleaned_recid.index[:M], axis=0, inplace=True)\n",
    "    print(\"rows in cleaned_recid after:\", cleaned_recid.shape)\n",
    "    \n",
    "    # flip order, then duplicate first 1/2 delta values from no_recid\n",
    "    #print(\"cleaned_no_recid before:\", cleaned_no_recid)\n",
    "    cleaned_no_recid = cleaned_no_recid.sort_values(by='rank', ascending=False)\n",
    "    #print(\"cleaned_no_recid after:\", cleaned_no_recid)\n",
    "    print(\"M:\", M)\n",
    "    print(\"rows in cleaned_no_recid before:\", cleaned_no_recid.shape)\n",
    "    cleaned_no_recid = cleaned_no_recid.append(cleaned_no_recid[0:M])\n",
    "    print(\"rows in cleaned_no_recid after:\", cleaned_no_recid.shape)\n",
    "    \n",
    "    # combine \n",
    "    total_C = pd.concat([cleaned_recid, cleaned_no_recid])\n",
    "    print(\"size of final C:\", total_C.shape)\n",
    "    print(\"end partition\")\n",
    "    \n",
    "    # combine both datasets\n",
    "    recomp_train = recomp_train.append(total_AA)\n",
    "    recomp_train = recomp_train.append(total_C)\n",
    "    recomp_train = recomp_train.drop('rank', axis=1)\n",
    "    \n",
    "    print(\"size of train:\", train.shape)\n",
    "    print(\"size of recomp:\", recomp_train.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8fe55",
   "metadata": {},
   "source": [
    "## Evaluation for LPS Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e25ca5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      sex  age  race  decile_score  priors_count  c_charge_degree  \\\n",
       "433    0   28     1             4             2                0   \n",
       "434    0   26     1             5             2                0   \n",
       "435    0   34     1             7             0                0   \n",
       "436    0   22     1             6             0                0   \n",
       "437    0   38     1             4             5                0   \n",
       "..   ...  ...   ...           ...           ...              ...   \n",
       "837    1   34     0             9            15                1   \n",
       "836    0   23     0             7            10                1   \n",
       "835    0   45     0             6            21                1   \n",
       "834    0   30     0             9             9                1   \n",
       "833    0   24     0            10             6                1   \n",
       "\n",
       "     two_year_recid       los  \n",
       "433               0  4.976734  \n",
       "434               0  3.912023  \n",
       "435               1  6.549651  \n",
       "436               0  4.248495  \n",
       "437               0  4.897840  \n",
       "..              ...       ...  \n",
       "837               1  4.356709  \n",
       "836               0  7.430707  \n",
       "835               1  3.433987  \n",
       "834               1  6.966967  \n",
       "833               0  5.153292  \n",
       "\n",
       "[3610 rows x 8 columns]>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomp_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e1751ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of recomp_X_train: (3610, 7)\n",
      "size of recomp_Y_train: (3610,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.686030428769018"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomp_X_train = recomp_train[X_ALL]\n",
    "recomp_Y_train = recomp_train[Y]\n",
    "print(\"size of recomp_X_train:\", recomp_X_train.shape)\n",
    "print(\"size of recomp_Y_train:\", recomp_Y_train.shape)\n",
    "\n",
    "clf_LPS = LogisticRegression(random_state=0).fit(recomp_X_train, recomp_Y_train)\n",
    "LPS_pred = clf_LPS.predict(X_val[X_ALL])\n",
    "clf_LPS.score(X_val[X_ALL], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48f891f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71       377\n",
      "           1       0.69      0.62      0.65       346\n",
      "\n",
      "    accuracy                           0.69       723\n",
      "   macro avg       0.69      0.68      0.68       723\n",
      "weighted avg       0.69      0.69      0.68       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, clf_LPS.predict(X_val[X_ALL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "685d30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid = 1 | race = African American) = 0.5033860045146726\n",
      "P_c(recid = 1 | race = Caucasian) = 0.3142857142857143\n",
      "Parity = 0.18910029022895836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18910029022895836"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARITY(X_val, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c744138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_c(recid predicted correctly | race = African American) = 0.6862302483069977\n",
      "P_c(recid predicted correctly | race = Caucasian) = 0.6857142857142857\n",
      "Calibration = 0.000515962592712027\n"
     ]
    }
   ],
   "source": [
    "CALIBRATION(X_val, y_val, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c085f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For recid = 0:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 0) =  0.3167420814479638\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 0) =  0.16666666666666666\n",
      "Difference in odds of true recid = 0 is  = D_FPR = 0.15007541478129713\n",
      "\n",
      "\n",
      "\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 1 | race = African American, recid = 1) =  0.6891891891891891\n",
      "P_c(recid.hat = 1 | race = Caucasian, recid = 1) =  0.5\n",
      "Difference in odds of true recid = 1 is = D_TPR = 0.18918918918918914\n"
     ]
    }
   ],
   "source": [
    "EQUALITY_OF_ODDS(X_val, y_val, LPS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45970451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in False Negative Rates\n",
      "For recid = 1:\n",
      "\n",
      "P_c(recid.hat = 0 | race = African American, recid = 1) =  0.3108108108108108\n",
      "P_c(recid.hat = 0 | race = Caucasian, recid = 1) =  0.5\n",
      "D_FNR = 0.1891891891891892\n"
     ]
    }
   ],
   "source": [
    "# D_FNR\n",
    "\n",
    "D_FNR(X_val, y_val, LPS_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
